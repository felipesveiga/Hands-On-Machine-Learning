{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dab5d-da0e-4ce3-a3e9-54b5bcb2bfcb",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Natural Language\n",
    "Processing with RNNs and\n",
    "Attention</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O capítulo será composto por duas seções. Na primeira, iremos continuar desenvolvendo nossos estudos com RNN's, mas aplicadas no âmbito de NLP. Já na segunda, iremos dar enfoque aos mecanismos de atenção.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b714a89-a6d9-46ec-8cd1-0902fc86b0d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Generating Shakespearean Text Using a\n",
    "Character RNN</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, vamos montar uma rede capaz de gerar poemas com o estilo de escrita de Shakespeare. Ela será treinada para prever o próximo caractere a ser digitado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8c045-822d-4583-8db8-691abddf4605",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Training Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Antes de tudo, vamos baixar o corpus do projeto e associá-lo a uma variável.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f79259-f3a1-40f6-a316-14af3e83cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o arquivo com textos do Shakespeare. \n",
    "from tensorflow.keras.utils import get_file\n",
    "file = \"shakespeare.txt\"\n",
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(file, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81299805-b45f-4f53-98af-86552c9ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d056133c-9b3e-4a51-aba1-76bcb96caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para numeralizarmos os textos, vamos recorrer à classe `Tokenizer`. Ao setarmos `char_level=True`, vamos associar cada caractere do corpus\n",
    "# a um número (começando por 1). \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2954dcc1-8079-4cb9-8435-49ac0c4d0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 18, 1, 3, 7, 2, 9, 2, 31]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de caracteres.\n",
    "tokenizer.texts_to_sequences(['Hi, there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad414cc6-d520-43b3-b853-081218bd5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que o objeto não considera caracteres acentuados, por padrão.\n",
    "len(tokenizer.texts_to_sequences(['Olá, como vai?'])[0]), len('Ola, como vai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f67dc2-1a5b-4db5-bcff-23a7ba75c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  e t o a']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo índices em uma string.\n",
    "tokenizer.sequences_to_texts([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045ae798-e219-4849-9149-bc02272a5571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribuindo os índices do texto à variável encoded. Vamos fazer uma subtração para que o primeiro índice seja atribuído a 0.\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f6fc4-0d74-4fc1-9ab6-d048faf5e4bb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> How to Split a Sequential Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos dividir aqui o nosso corpus no set de treino, validação e teste. O autor aproveita a situação, e revela os prós e contras das diferentes metodologias de separação do dataset. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, vamos optar por manter os primeiros fragmentos do corpus para treinamento, e o restante para validação e teste. Isso tem a assunção de que os mesmos padrões presentes em momentos anteriores do arquivo estarão contidos em momentos futuros (série estacionária).\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6e51d51-8c93-457d-87f0-cc6ba2150127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fd8bc-1caf-409b-8c3e-9950bbf4b73e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Chopping the Sequential Dataset into Multiple Windows</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Até agora, convertemos todo o texto em uma única instância. Como treinar um modelo com isso é inviável, temos que quebrar esses dados em janelas, tratando cada uma delas como uma instância.\n",
    "        </li>\n",
    "        <li>\n",
    "            A função `Dataset.window` cria datasets dentro de nosso dataset principal, cada um contendo uma porção específica de elementos. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0bf6c7f-ec42-47f5-acb7-84a530759555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sub-datasets de 101 caracteres. Quando definimos `shift=1`, escolhemos que a janela se desloque 1 caractere por vez para montar \n",
    "# cada sub-dataset [0-101, 1-102...].\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps+1\n",
    "# `drop_remainder` exclui as últimas janelas cujo tamanho acabaria menor do que `size`.\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844669c-dac7-4bdc-b93f-476ebc411257",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No entanto, devemos observar que as classes de modelo admitem apenas `tf.Tensor` como input. Caso quisermos converter cada Dataset aninhado em um tensor, podemos fazer um truque com a função `flat_map`, passando a ela uma lambda que cria batches de mesmo comprimento que `window_length`.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em tese, `flat_map` torna Datasets de aninhamento num único Dataset. Mas, como ele admite uma função que aplique alguma transformação nos Datasets aninhados antes da planificação, podemos aplicar `batch`. No final das contas, ficamos com tensores de mesmo conteúdo que os Datasets.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54b7c-4eb1-45b7-bade-3f735abf20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda w: w.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa9441f-28f9-4d03-8ace-846a4af548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos aproveitar a situação, e já criar batches com vários chunks aleatórios.\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53bb9b81-f9d0-4d3d-9360-1d6a0ef37860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o X e y da IA. Vamos programá-la para que ela receba a sentença recortada do primeiro ao penúltimo caractere,\n",
    "# e preveja a sequência do segundo ao último caractere.\n",
    "dataset = dataset.map(lambda w: (w[:, :-1], w[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed8a2c9-d2db-41d3-975f-033b6185850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos tornar o X num One-Hot Encoding.\n",
    "from tensorflow import one_hot\n",
    "\n",
    "max_id = len(tokenizer.word_index) # Quantidade de caracteres distintos.\n",
    "n = dataset.map(lambda X,y: (one_hot(X, depth=max_id), y)).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf7819b-11db-4624-931d-9d89415e75b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garantindo que o próximo batch a ser processado no treinamento já seja alocado à memória, antecipadamente.\n",
    "dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eda9c-72ff-4c0a-a80f-2061df54b1b5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Building and Training the Char-RNN Model</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos montar agora uma rede recorrente que preverá o próximo caractere de uma sequência.\n",
    "        </li>\n",
    "        <li>\n",
    "            Não montei a rede, porque demora muito para treinar.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfac29e6-c8a2-4aa5-9015-73c6c67aeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8383898-1509-480c-8c48-719a576353bc",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Generating Fake Shakespearean Text</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A geração de textos completos poderia acontecer em ciclos, em que dada uma frase, o modelo prevê o próximo caractere e o vincula ao final da frase para uma nova previsão. O problema dessa abordagem é a alta probabilidade de sempre o mesmo caractere ser previsto...\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74f1de-d8b5-4ae4-a7a5-563fda91f370",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Temperature</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma maneira de garantirmos outputs mais diversos é acrescentar algumas etapas pós-predict do modelo. Tendo as probabilidades, extraímos o seu log e as dividimos por uma `temperature`. O valor desse argumento é de escolha do desenvolvedor.\n",
    "        </li>\n",
    "        <li>\n",
    "            Nós usaremos a função `tf.random.categorical`, que escolherá o próximo token da frase, dado os logits computados.\n",
    "        </li>\n",
    "        <li>\n",
    "            Menores temperatures ($<1$) acentuam as diferenças entre os logits, enquanto valores acima de 1 tendem a planificar esse conjunto de números. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6aaa31-1ce0-4168-9ea1-51b6488718ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3588750f-7ff3-48b2-afcb-73e79a086d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7ec8a-bd5d-4305-b432-41fe09f527b9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Statefull RNN</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma RNN Stateful é aquela que preserva o seu state anterior para a próxima iteração de treinamento. No caso, isso é apenas recomendado caso a primeira instância do próximo batch seja uma continuação da última do batch anterior.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, quando fazemos batch, as instâncias não são consecutivas por posição. No caso de $\\text{batch}=32$, as primeiras instâncias dos dois primeiros batches (1 e 33) não são consecutivas.\n",
    "        </li>\n",
    "        <li>\n",
    "            A solução para isso seria criar batches unitários!\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242a2a3-8f56-4042-95c9-f471d88d1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, TimeDistributed\n",
    "\n",
    "# Deixe `stateful=True` em cada uma de suas camadas recorrentes.\n",
    "# Informe `batch_input_shape` na primeira camada da rede.\n",
    "stateful_model = Sequential([\n",
    "    GRU(128, return_sequences=True, stateful=True, dropout=.2, recurrent_dropout=.2, batch_input_shape=[batch_size, None, max_id]),\n",
    "    GRU(128, return_sequences=True, stateful=True, dropout=.2, recurrent_dropout=.2),\n",
    "    TimeDistributed(Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69209961-fc2f-4863-91b1-0020caae0708",
   "metadata": {},
   "source": [
    "<p style='color:red'> Iniciei Stateful RNN's; Montar a classe ResetStatesCallback; Pesqui </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
