{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dab5d-da0e-4ce3-a3e9-54b5bcb2bfcb",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Natural Language\n",
    "Processing with RNNs and\n",
    "Attention</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O capítulo será composto por duas seções. Na primeira, iremos continuar desenvolvendo nossos estudos com RNN's, mas aplicadas no âmbito de NLP. Já na segunda, iremos dar enfoque aos mecanismos de atenção.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b714a89-a6d9-46ec-8cd1-0902fc86b0d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Generating Shakespearean Text Using a\n",
    "Character RNN</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, vamos montar uma rede capaz de gerar poemas com o estilo de escrita de Shakespeare. Ela será treinada para prever o próximo caractere a ser digitado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8c045-822d-4583-8db8-691abddf4605",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Training Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Antes de tudo, vamos baixar o corpus do projeto e associá-lo a uma variável.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f79259-f3a1-40f6-a316-14af3e83cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o arquivo com textos do Shakespeare. \n",
    "from tensorflow.keras.utils import get_file\n",
    "file = \"shakespeare.txt\"\n",
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(file, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81299805-b45f-4f53-98af-86552c9ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d056133c-9b3e-4a51-aba1-76bcb96caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para numeralizarmos os textos, vamos recorrer à classe `Tokenizer`. Ao setarmos `char_level=True`, vamos associar cada caractere do corpus\n",
    "# a um número (começando por 1). \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2954dcc1-8079-4cb9-8435-49ac0c4d0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 18, 1, 3, 7, 2, 9, 2, 31]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de caracteres.\n",
    "tokenizer.texts_to_sequences(['Hi, there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad414cc6-d520-43b3-b853-081218bd5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que o objeto não considera caracteres acentuados, por padrão.\n",
    "len(tokenizer.texts_to_sequences(['Olá, como vai?'])[0]), len('Ola, como vai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f67dc2-1a5b-4db5-bcff-23a7ba75c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  e t o a']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo índices em uma string.\n",
    "tokenizer.sequences_to_texts([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045ae798-e219-4849-9149-bc02272a5571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribuindo os índices do texto à variável encoded. Vamos fazer uma subtração para que o primeiro índice seja atribuído a 0.\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f6fc4-0d74-4fc1-9ab6-d048faf5e4bb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> How to Split a Sequential Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos dividir aqui o nosso corpus no set de treino, validação e teste. O autor aproveita a situação, e revela os prós e contras das diferentes metodologias de separação do dataset. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, vamos optar por manter os primeiros fragmentos do corpus para treinamento, e o restante para validação e teste. Isso tem a assunção de que os mesmos padrões presentes em momentos anteriores do arquivo estarão contidos em momentos futuros (série estacionária).\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6e51d51-8c93-457d-87f0-cc6ba2150127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fd8bc-1caf-409b-8c3e-9950bbf4b73e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Chopping the Sequential Dataset into Multiple Windows</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Até agora, convertemos todo o texto em uma única instância. Como treinar um modelo com isso é inviável, temos que quebrar esses dados em janelas, tratando cada uma delas como uma instância.\n",
    "        </li>\n",
    "        <li>\n",
    "            A função `Dataset.window` cria datasets dentro de nosso dataset principal, cada um contendo uma porção específica de elementos. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0bf6c7f-ec42-47f5-acb7-84a530759555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sub-datasets de 101 caracteres. Quando definimos `shift=1`, escolhemos que a janela se desloque 1 caractere por vez para montar \n",
    "# cada sub-dataset [0-101, 1-102...].\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps+1\n",
    "# `drop_remainder` exclui as últimas janelas cujo tamanho acabaria menor do que `size`.\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844669c-dac7-4bdc-b93f-476ebc411257",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No entanto, devemos observar que as classes de modelo admitem apenas `tf.Tensor` como input. Caso quisermos converter cada Dataset aninhado em um tensor, podemos fazer um truque com a função `flat_map`, passando a ela uma lambda que cria batches de mesmo comprimento que `window_length`.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em tese, `flat_map` torna Datasets de aninhamento num único Dataset. Mas, como ele admite uma função que aplica alguma transformação nos Datasets aninhados antes da planificação, podemos aplicar `batch`. No final das contas, ficamos com tensores de mesmo conteúdo que os Datasets.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54b7c-4eb1-45b7-bade-3f735abf20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda w: w.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa9441f-28f9-4d03-8ace-846a4af548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos aproveitar a situação, e já criar batches com vários chunks aleatórios.\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53bb9b81-f9d0-4d3d-9360-1d6a0ef37860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o X e y da IA. Vamos programá-la para que ela receba a sentença recortada do primeiro ao penúltimo caractere,\n",
    "# e preveja a sequência do segundo ao último caractere.\n",
    "dataset = dataset.map(lambda w: (w[:, :-1], w[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed8a2c9-d2db-41d3-975f-033b6185850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos tornar o X num One-Hot Encoding.\n",
    "from tensorflow import one_hot\n",
    "\n",
    "max_id = len(tokenizer.word_index) # Quantidade de caracteres distintos.\n",
    "n = dataset.map(lambda X,y: (one_hot(X, depth=max_id), y)).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf7819b-11db-4624-931d-9d89415e75b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garantindo que o próximo batch a ser processado no treinamento já seja alocado à memória, antecipadamente.\n",
    "dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eda9c-72ff-4c0a-a80f-2061df54b1b5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Building and Training the Char-RNN Model</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos montar agora uma rede recorrente que preverá o próximo caractere de uma sequência.\n",
    "        </li>\n",
    "        <li>\n",
    "            Não montei a rede, porque demora muito para treinar.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfac29e6-c8a2-4aa5-9015-73c6c67aeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8383898-1509-480c-8c48-719a576353bc",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Generating Fake Shakespearean Text</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A geração de textos completos poderia acontecer em ciclos, em que dada uma frase, o modelo prevê o próximo caractere e o vincula ao final da frase para uma nova previsão. O problema dessa abordagem é a alta probabilidade de sempre o mesmo caractere ser previsto...\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74f1de-d8b5-4ae4-a7a5-563fda91f370",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Temperature</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma maneira de garantirmos outputs mais diversos é acrescentar algumas etapas pós-predict do modelo. Tendo as probabilidades, extraímos o seu log e as dividimos por uma `temperature`. O valor desse argumento é de escolha do desenvolvedor.\n",
    "        </li>\n",
    "        <li>\n",
    "            Nós usaremos a função `tf.random.categorical`, que escolherá o próximo token da frase, dado os logits computados.\n",
    "        </li>\n",
    "        <li>\n",
    "            Menores temperatures ($<1$) acentuam as diferenças entre os logits, enquanto valores acima de 1 tendem a planificar esse conjunto de números. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6aaa31-1ce0-4168-9ea1-51b6488718ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3588750f-7ff3-48b2-afcb-73e79a086d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7ec8a-bd5d-4305-b432-41fe09f527b9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stateful RNN</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma RNN Stateful é aquela que preserva o seu state anterior para a próxima iteração de treinamento. No caso, isso é apenas recomendado caso a primeira instância do próximo batch seja uma continuação da última do batch anterior.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, quando fazemos batch, as instâncias não são consecutivas por posição. No caso de $\\text{batch}=32$, as primeiras instâncias dos dois primeiros batches (1 e 33) não são consecutivas.\n",
    "        </li>\n",
    "        <li>\n",
    "            A solução para isso seria criar batches unitários!\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b6556-acc4-484b-ab4c-c8e56870de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o dataset para uma RNN Stateful\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps,\n",
    "drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id),\n",
    "Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242a2a3-8f56-4042-95c9-f471d88d1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, TimeDistributed\n",
    "\n",
    "# Deixe `stateful=True` em cada uma de suas camadas recorrentes.\n",
    "# Informe `batch_input_shape` na primeira camada da rede.\n",
    "stateful_model = Sequential([\n",
    "    GRU(128, return_sequences=True, stateful=True, dropout=.2, recurrent_dropout=.2, batch_input_shape=[batch_size, None, max_id]),\n",
    "    GRU(128, return_sequences=True, stateful=True, dropout=.2, recurrent_dropout=.2),\n",
    "    TimeDistributed(Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525701e-1c8d-4c37-9157-69ee17cd5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A única coisa que devemos nos atentar é resetar o state ao final de cada ÉPOCA.\n",
    "# Para isso, vamos ter que criar um callback especializado nisso. Lembre-se de passá-lo em uma lista no `fit do modelo.\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ResetStatesEpoch(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e74cba-5b37-4bd8-a0d5-21a10b3755e9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Sentiment Analysis</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Agora, vamos fazer um pequeno trabalho de análise de sentimentos com o IMDb. Um fato importante sobre ele é que seus textos já foram tokenizados por ordem de frequência - quanto menor o inteiro, mais frequente ele é.\n",
    "        </li>\n",
    "        <li>\n",
    "            Os textos foram pré-processados antes da tokenização (lowercase, remoção de pontuação...).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25682f25-e19b-488e-a8fb-3105c543c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Carregando as reviews.\n",
    "from tensorflow.keras.datasets.imdb import load_data\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed36a102-63fa-4e20-ab0a-d1c14693aea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que os textos já estão tokenizados.\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc021587-9371-45d9-8f9d-c0255685ec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Você pode decodificar o texto da seguinte maneira.\n",
    "from tensorflow.keras.datasets.imdb import get_word_index\n",
    "word_index =  get_word_index()\n",
    "\n",
    "# Os 3 primeiros índices fazem referência a tokens especiais (padding, start-of-sequence e unknown words). Por isso, vamos ter que fazer um\n",
    "# pequeno ajuste sobre as chaves.\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "\n",
    "# Incluindo os tokens especiais ao nosso dicionário.\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "    \n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd3bbd-5cdc-494b-8bd7-fe42a7b1f98e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Espaçamento entre Palavras</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em projetos multilíngua, temos que considerar que nem todas as línguas separam palavras com espaço (chinês, vietnamita). Isso levou pesquisadores a fazer uma série de estudos sobre técnicas de tokenização de textos, como apresentado pelo livro.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bf1bd-f33a-4923-b537-43f1abb26d43",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor decide também elaborar um pequeno projeto do IMDb com as revisões em caracteres-byte.\n",
    "        </li>\n",
    "        <li>\n",
    "            A solução vai se parecer com um CountVectorizer.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e12b46d-00eb-48ff-b004-4e7bf96ca700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-04 13:09:14.880683: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 13:09:15.443243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-04 13:09:15.443356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-04 13:09:15.533299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-04 13:09:15.728085: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 13:09:18.907226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-04 13:09:21.365651: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|                            | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|                            | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                | 0/80 [00:00<?, ? MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:02<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:02<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:02<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Size...:   6%|█▌                      | 5/80 [00:03<02:39,  2.13s/ MiB]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:03<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:04<?, ? url/s]\u001b[A\n",
      "Dl Size...:  12%|██▉                    | 10/80 [00:04<00:32,  2.16 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:04<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:04<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:04<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:05<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:05<?, ? url/s]\u001b[A\n",
      "Dl Size...:  19%|████▎                  | 15/80 [00:05<00:21,  2.97 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:05<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:05<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:05<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:06<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:06<?, ? url/s]\u001b[A\n",
      "Dl Size...:  25%|█████▊                 | 20/80 [00:06<00:16,  3.54 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:06<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:06<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:07<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:07<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:07<?, ? url/s]\u001b[A\n",
      "Dl Size...:  31%|███████▏               | 25/80 [00:07<00:14,  3.89 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:07<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:07<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:08<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:08<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:08<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:08<?, ? url/s]\u001b[A\n",
      "Dl Size...:  39%|████████▉              | 31/80 [00:08<00:11,  4.17 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:08<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:09<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:09<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:09<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:09<?, ? url/s]\u001b[A\n",
      "Dl Size...:  45%|██████████▎            | 36/80 [00:09<00:09,  4.47 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:09<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:10<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:10<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:10<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:10<?, ? url/s]\u001b[A\n",
      "Dl Size...:  51%|███████████▊           | 41/80 [00:10<00:08,  4.58 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:10<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:11<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:11<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:11<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:11<?, ? url/s]\u001b[A\n",
      "Dl Size...:  57%|█████████████▏         | 46/80 [00:11<00:07,  4.65 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:11<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:12<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:12<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:12<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:12<?, ? url/s]\u001b[A\n",
      "Dl Size...:  64%|██████████████▋        | 51/80 [00:12<00:06,  4.69 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:13<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:13<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:13<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:13<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:14<?, ? url/s]\u001b[A\n",
      "Dl Size...:  70%|████████████████       | 56/80 [00:14<00:05,  4.45 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:14<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:14<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:15<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:15<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:15<?, ? url/s]\u001b[A\n",
      "Dl Size...:  76%|█████████████████▌     | 61/80 [00:15<00:04,  4.11 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:15<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:16<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:16<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:16<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:16<?, ? url/s]\u001b[A\n",
      "Dl Size...:  82%|██████████████████▉    | 66/80 [00:16<00:03,  4.11 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:17<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:17<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:17<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:17<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:18<?, ? url/s]\u001b[A\n",
      "Dl Size...:  89%|████████████████████▍  | 71/80 [00:18<00:02,  4.08 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:18<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:18<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:18<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:19<?, ? url/s]\u001b[A\n",
      "Dl Size...:  94%|█████████████████████▌ | 75/80 [00:19<00:01,  3.96 MiB/s]\u001b[A\n",
      "Dl Size...:  94%|█████████████████████▌ | 75/80 [00:29<00:01,  3.96 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:33<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:34<?, ? url/s]\u001b[A\n",
      "Dl Size...:  96%|██████████████████████▏| 77/80 [00:34<00:03,  1.18s/ MiB]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:35<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:35<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|                            | 0/1 [00:36<?, ? url/s]\u001b[A\n",
      "Dl Completed...: 100%|████████████████████| 1/1 [00:36<00:00, 36.29s/ url]\u001b[A\n",
      "Dl Size...: 100%|███████████████████████| 80/80 [00:36<00:00,  2.20 MiB/s]\u001b[A\n",
      "Dl Completed...: 100%|████████████████████| 1/1 [00:36<00:00, 36.30s/ url]\n",
      "Generating splits...:   0%|                    | 0/3 [00:00<?, ? splits/s]\n",
      "Generating train examples...:   0%|      | 0/25000 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating train examples...:   0%| | 1/25000 [00:02<18:21:27,  2.64s/ exa\u001b[A\n",
      "Generating train examples...:   8%| | 1936/25000 [00:03<00:33, 679.03 exam\u001b[A\n",
      "Generating train examples...:  16%|▏| 3947/25000 [00:04<00:18, 1123.65 exa\u001b[A\n",
      "Generating train examples...:  24%|▏| 5940/25000 [00:05<00:13, 1404.17 exa\u001b[A\n",
      "Generating train examples...:  32%|▎| 7925/25000 [00:06<00:10, 1587.44 exa\u001b[A\n",
      "Generating train examples...:  39%|▍| 9824/25000 [00:07<00:09, 1684.26 exa\u001b[A\n",
      "Generating train examples...:  47%|▍| 11810/25000 [00:08<00:07, 1776.90 ex\u001b[A\n",
      "Generating train examples...:  55%|▌| 13807/25000 [00:09<00:06, 1843.86 ex\u001b[A\n",
      "Generating train examples...:  63%|▋| 15821/25000 [00:10<00:04, 1895.36 ex\u001b[A\n",
      "Generating train examples...:  71%|▋| 17865/25000 [00:11<00:03, 1940.18 ex\u001b[A\n",
      "Generating train examples...:  79%|▊| 19845/25000 [00:12<00:02, 1915.15 ex\u001b[A\n",
      "Generating train examples...:  87%|▊| 21814/25000 [00:13<00:01, 1930.90 ex\u001b[A\n",
      "Generating train examples...:  95%|▉| 23822/25000 [00:14<00:00, 1953.82 ex\u001b[A\n",
      "                                                                          \u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Generating splits...:  33%|████        | 1/3 [00:19<00:38, 19.23s/ splits]\u001b[A\n",
      "Generating test examples...:   0%|       | 0/25000 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating test examples...:   3%| | 762/25000 [00:01<00:31, 761.88 exampl\u001b[A\n",
      "Generating test examples...:  11%| | 2787/25000 [00:02<00:14, 1504.53 exam\u001b[A\n",
      "Generating test examples...:  19%|▏| 4802/25000 [00:03<00:11, 1737.52 exam\u001b[A\n",
      "Generating test examples...:  27%|▎| 6744/25000 [00:04<00:10, 1817.98 exam\u001b[A\n",
      "Generating test examples...:  35%|▎| 8770/25000 [00:05<00:08, 1892.69 exam\u001b[A\n",
      "Generating test examples...:  43%|▍| 10721/25000 [00:06<00:07, 1912.09 exa\u001b[A\n",
      "Generating test examples...:  51%|▌| 12706/25000 [00:07<00:06, 1935.55 exa\u001b[A\n",
      "Generating test examples...:  59%|▌| 14770/25000 [00:08<00:05, 1976.16 exa\u001b[A\n",
      "Generating test examples...:  67%|▋| 16797/25000 [00:09<00:04, 1991.83 exa\u001b[A\n",
      "Generating test examples...:  75%|▊| 18789/25000 [00:10<00:03, 1971.75 exa\u001b[A\n",
      "Generating test examples...:  83%|▊| 20762/25000 [00:11<00:02, 1938.99 exa\u001b[A\n",
      "Generating test examples...:  91%|▉| 22773/25000 [00:12<00:01, 1960.25 exa\u001b[A\n",
      "Generating test examples...:  99%|▉| 24788/25000 [00:13<00:00, 1976.48 exa\u001b[A\n",
      "                                                                          \u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Generating splits...:  67%|████████    | 2/3 [00:38<00:19, 19.14s/ splits]\u001b[A\n",
      "Generating unsupervised examples...:   0%| | 0/50000 [00:00<?, ? examples/\u001b[A\n",
      "Generating unsupervised examples...:   0%| | 1/50000 [00:04<58:45:10,  4.2\u001b[A\n",
      "Generating unsupervised examples...:   4%| | 1887/50000 [00:05<01:41, 476.\u001b[A\n",
      "Generating unsupervised examples...:   8%| | 3765/50000 [00:06<00:54, 847.\u001b[A\n",
      "Generating unsupervised examples...:  12%| | 5778/50000 [00:07<00:37, 1167\u001b[A\n",
      "Generating unsupervised examples...:  15%|▏| 7736/50000 [00:08<00:30, 1390\u001b[A\n",
      "Generating unsupervised examples...:  19%|▏| 9720/50000 [00:09<00:25, 1560\u001b[A\n",
      "Generating unsupervised examples...:  23%|▏| 11693/50000 [00:10<00:22, 168\u001b[A\n",
      "Generating unsupervised examples...:  27%|▎| 13674/50000 [00:11<00:20, 176\u001b[A\n",
      "Generating unsupervised examples...:  31%|▎| 15701/50000 [00:12<00:18, 184\u001b[A\n",
      "Generating unsupervised examples...:  35%|▎| 17654/50000 [00:13<00:17, 186\u001b[A\n",
      "Generating unsupervised examples...:  39%|▍| 19610/50000 [00:14<00:16, 189\u001b[A\n",
      "Generating unsupervised examples...:  43%|▍| 21557/50000 [00:15<00:15, 189\u001b[A\n",
      "Generating unsupervised examples...:  47%|▍| 23583/50000 [00:16<00:13, 193\u001b[A\n",
      "Generating unsupervised examples...:  51%|▌| 25543/50000 [00:17<00:12, 194\u001b[A\n",
      "Generating unsupervised examples...:  55%|▌| 27502/50000 [00:18<00:11, 191\u001b[A\n",
      "Generating unsupervised examples...:  59%|▌| 29435/50000 [00:19<00:10, 191\u001b[A\n",
      "Generating unsupervised examples...:  63%|▋| 31363/50000 [00:20<00:09, 190\u001b[A\n",
      "Generating unsupervised examples...:  67%|▋| 33276/50000 [00:21<00:09, 184\u001b[A\n",
      "Generating unsupervised examples...:  70%|▋| 35166/50000 [00:22<00:07, 185\u001b[A\n",
      "Generating unsupervised examples...:  74%|▋| 37145/50000 [00:23<00:06, 189\u001b[A\n",
      "Generating unsupervised examples...:  78%|▊| 39131/50000 [00:24<00:05, 192\u001b[A\n",
      "Generating unsupervised examples...:  82%|▊| 41057/50000 [00:25<00:04, 186\u001b[A\n",
      "Generating unsupervised examples...:  86%|▊| 42958/50000 [00:26<00:03, 187\u001b[A\n",
      "Generating unsupervised examples...:  90%|▉| 44940/50000 [00:27<00:02, 190\u001b[A\n",
      "Generating unsupervised examples...:  94%|▉| 46905/50000 [00:28<00:01, 192\u001b[A\n",
      "Generating unsupervised examples...:  98%|▉| 48832/50000 [00:29<00:00, 190\u001b[A\n",
      "                                                                          \u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "Shuffling /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.in\u001b[A\n",
      "                                                                          \u001b[A\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/veiga/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Carregando o dataset.\n",
    "from tensorflow_datasets import load\n",
    "datasets, info = load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37fd38-47dc-4868-9443-6801200bf2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pequena função de encurtamento dos textos (questão de memória) e remoção das tags HTML.\n",
    "# Também mantemos somente caracteres alfabéticos e dividimos as strings por espaçamento.\n",
    "# Por fim, acrescentamos uma tag <pad> para garantir que todos os vetores sejam de mesmo tamanho.\n",
    "from tensorflow.strings import substr, regex_replace, split\n",
    "\n",
    "def preprocessing(X_batch, y_batch):\n",
    "    X_batch = substr(X_batch, 0, 300)\n",
    "    X_batch = regex_replace(X_batch, b'<br\\\\s*/?>', b' ')\n",
    "    X_batch = regex_replace(X_batch, b'[^A-z\\']', b' ')\n",
    "    X_batch = split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b'<pad>'), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cc80c-62b6-46a8-9a65-42669224996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por questões de memória, vamos também manter apenas os 10000 tokens mais frequentes.\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "\n",
    "for x,y in datasets['train'].batch(32).map(preprocessing):\n",
    "    for review in x:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "truncated_vocab = [word for word, count in vocabulary.most_common(vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300d05e-0e54-4bb9-ac8e-cbc2523a45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando nossa tabela texto-id.\n",
    "from tensorflow import constant, int64\n",
    "from tensorflow import range as tf_range\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticVocabularyTable\n",
    "num_oov_buckets = 1000\n",
    "words = constant(truncated_vocab)\n",
    "words_id = tf_range(len(words), dtype=int64)\n",
    "vocab_init = KeyValueTensorInitializer(words, words_id)\n",
    "table = StaticVocabularyTable(vocab_init, num_oov_buckets=num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c0e34-c705-4d10-b79d-edb4fe81c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe aqui que 'tripper' obteve um ID maior do que 10000, significando que não estava presente entre \n",
    "# os dados de treinamento.\n",
    "table.lookup(constant([b'she was a day tripper one way ticket yeah'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bac3a-8a75-477f-a02e-e095ede4d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para codificação de textos.\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c86f5-1b3f-4a18-8dd3-a711d351bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets['train'].batch(32).map(preprocessing)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995e435-a5c0-42c3-95ff-45dec7d9f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "embed_size = 128\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "    input_shape=[None]),\n",
    "    GRU(128, return_sequences=True),\n",
    "    GRU(128),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6070-6f03-441c-86ee-160a139b25d5",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Embedding Layer</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Embedding Layer tem a tarefa de mapear uma palavra a um vetor. Sua ideia é conseguir encapsular a semântica do token nos componentes.\n",
    "        </li>\n",
    "        <li>\n",
    "            Basicamente, ela consiste em uma matriz (n_classes, n_dimensoes). O que devemos fazer é recortar o vetor linha com o índice da classe sendo utilizada.\n",
    "        </li>\n",
    "        <li>\n",
    "            Tendo o vetor em mãos, poderemos passá-lo como input de uma camada subsequente. Observe que nessa situação, faremos um forward pass por <strong>token</strong>. Isso fará com que a rede tenha que armazenar $\\text{text-length}\\times{\\text{batch-size}}$ deriavadas parciais, antes do backpropagation.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077f528-bd83-4bde-a94b-2abde4afbfa3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Masking</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Keras ainda conta com uma funcionalidade de negligenciamento do token de padding (&lt;pad&gt;). Basicamente, a camada de Embedding lançará, junto com o vetor, um array booleano sinalizando se a camada seguinte deverá considerá-lo ou não.\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa máscara será propagada por todo o modelo, desde que a camada receptora lance um array mantendo a dimensão de tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ad76c-952c-49ce-a3c3-8e74a940e26e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Reusing Pretrained Embeddings</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Ao invés de treinarmos nossos próprios embeddings, podemos recorrer a componentes pré-treinados open-source. Eles estão disponíveis via TensorFlow Hub.\n",
    "        </li>\n",
    "        <li>\n",
    "            Chamamos cada componente do TF-Hub como módulo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Aqui, testaremos a camada de Embedding nnlm-en-dim50. Ela medirá o vetor de cada token de sua string e retornará o embedding médio multiplicado pela raiz da quantidade de palavras do seu texto.\n",
    "    <center style='margin-top:20px'>\n",
    "        $\\text{embedding}=\\frac{\\sqrt{n}}{n}\\sum_{i=1}^{n}v_{i}$\n",
    "    </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d921e2a3-444b-4a7e-b546-977f6da86353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.src.engine.base_layer_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLayer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m     11\u001b[0m         Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m         Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m ])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/base_layer.py:757\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/version_utils.py:49\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/generic_utils.py:556\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/generic_utils.py:547\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine.base_layer_v1'"
     ]
    }
   ],
   "source": [
    "# Observe que os coeficientes dos módulos do TF-Hub vêm congelados. \n",
    "# Caso esteja interesado em fazer um fine-tuning em cima deles, defina o argumento `trainable=True`.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow_hub import KerasLayer\n",
    "from tensorflow import string\n",
    "\n",
    "model = Sequential([\n",
    "        KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                        dtype=string, input_shape=[], output_shape=[50], trainable=True),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4840957-d3a4-450f-8c74-3c9912e31d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que o TF-Hub armazenará os módulos baixados no cache da sua máquina.\n",
    "# Caso deseje que eles sejam depositados em disco, ponha o path desejado na variável de ambiente \n",
    "# 'TFHUB_CACHE_DIR'.\n",
    "from os import environ\n",
    "#environ['TFHUB_CACHE_DIR'] = '<path>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331779f-7f53-4aa3-b48c-e7b809eb551d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> An Encoder–Decoder Network for Neural Machine Translation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Nesta seção, aprendemos uma aplicação de RNN's Encoder-Decoder no cenário de tradução de textos (NMT). No paper, os autores empilham 4 LSTM's (acredito que tanto na parte encoder, quanto decoder). Ademais, eles descobriram que inverter a frase de input, antes de passá-la ao modelo, aprimora o desempenho em sequências longas!\n",
    "        </li>\n",
    "        <li> \n",
    "            Durante o treinamento, os pesquisadores buscaram fazer com que os batches tivessem sentenças de comprimento similares. Isso aprimorou a velocidade do treinamento em duas vezes!\n",
    "        </li>\n",
    "        <li>\n",
    "            A geração da sentença é feita com um Beam Search.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd0646-df4f-4bdb-b055-de0ac7aac60e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='margin-top:20px'>\n",
    "        <img src='16_sequence_to_sequence.png'>\n",
    "            <figcaption> Arquitetura proposta pelo paper \"Sequence-to-Sequence Learning\"</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf99b8-3f3a-43e8-b39d-a9c07236b724",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Beam Search</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Beam Search é uma técnica de geração de sequências. Tendo um modelo em mãos, pedimos para ele escolher os $\\beta$ estados mais prováveis para serem o primeiro componente.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em seguida, pedimos para ele computar as probabilidades de transição $s_{1}\\to{s_{2}}$, tendo como origem esses $\\beta$ estados escolhidos. Novamente, selecionamos as $\\beta$ transições mais prováveis - considerando todos os estados de origem em mãos, e não $\\beta$ por estado. \n",
    "        </li>\n",
    "        <li>\n",
    "            Repetimos esse procedimento, até que todos os $\\beta$ caminhos tenham atingido oo estado &lt;eos&gt;. Finalmente, escolhemos o caminho com a maior probabilidade!\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9fefa-1986-4e74-b0c0-b31ba54dfa37",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Outras Técnicas de Menção Válida</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Géron diz que providenciar a palavra anterior na previsão da próxima pode facilitar o treinamento da rede. Em treinamento, insira a palavra que deveria ter sido prevista e, em inferência, a que foi prevista na última iteração.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='16_previous_token.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            Computar a probabilidade de cada token do vocabulário de output pode onerar desnecessariamente a nossa máquina. Por isso, em treinamento, costuma-se medir a softmax de apenas o token da target junto com uma amostra aleatória. Em inferência, vamos ter que recorrer à softmax padrão mesmo.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24172a13-26d6-4139-a7db-b42d594a45b7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Bidirectional RNNs</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Nas arquiteturas vistas até então, vetorizamos tokens levando em conta apenas as palavras que os antecedem. Muitas vezes, porém, considerar tokens que o sucedem podem fornecer uma vetorização mais contextualizada. Para essa tarefa, foram criadas as RNN's bidirecionais. \n",
    "        </li>\n",
    "        <li> \n",
    "            Nela, cada célula da rede ganhará uma irmã-gêmea, encarregada de processar o mesmo input. No entanto, ela o lerá de trás para frente.  \n",
    "        </li>\n",
    "        <li>\n",
    "            No final, o output desse par será o concatenado dos seus dois vetores de resultado. Portanto, se as camadas lançam vetores de $n$ dimensões, o output oficial será de $2\\times{n}$.\n",
    "              <center style='margin-top:20px'>\n",
    "                <img src='bidirectional_rnn.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0a333-f14f-42c4-b057-f241dbc0dc3b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Attention Mechanisms</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            As técnicas de tradução de textos vistas até agora têm bastante dificuldade em traduzir textos compridos. No caso do modelo do \"Sequence-to-Sequence Learning\", observe que os hidden states de palavras mais afastadas do começo são \"sobrepostos\" pelos das palavras iniciais. \n",
    "        </li>\n",
    "        <li>\n",
    "            A fim de lidar com messe problema, o paper 'Neural Machine Translation By Jointly Learning To Align And Translate' propõe uma nova arquitetura de modelos de tradução, recorrendo a RNN's Bidirecionais (BiRNN).\n",
    "        </li>\n",
    "        <li>\n",
    "            Com seu uso, cada palavra receberá um hidden state próprio, levando em conta as palavras que a precedem e sucedem. Nessa situação não precisaremos mais nos preocupar com a sobreposição dos estados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65622f79-7358-4ccc-ba52-94c3ced3edde",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Additive/Concat/Bahdanau Attention</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tendo esses hidden states ($h$) em mãos, os passamos a um MLP, juntamente com o state da última previsão do decoder ($s_{t-1}$) para a computação de um score $e$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Esses scores/energias do MLP passam por uma softmax, que nos retorna valores $\\alpha$ indicando o grau de relação de cada palavra com a última previsão do decoder.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por fim, computamos um vetor $c$ com base na soma dos hidden states, ponderados pelos seus respectivos $\\alpha$'s. O decoder o receberá, juntamente com o hidden state e target da iteração anterior para a computação da previsão.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285ff4c-3b57-494e-8816-b007ed22110c",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Benefício do Mecanismo de Atenção</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            O vetor $c$ possibilita uma contextualização mais precisa ao decoder sobre quais informações da frase de origem ele deverá se <i> atentar</i> mais. Em arquiteturas anteriores, o uso de um hidden state imutável fazia o decoder considerar todo o conteúdo do texto - incluindo aquele que não era necessário - na previsão de cada token.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0f6c2-8169-4f1a-abb5-59682730b2fe",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Computações Encoder</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Sendo $T_x$ a quantidade de palavras da sentença origem, $j$ o indexador de uma palavra da sentença de origem, e $i$ o de uma outra palavra da frase sendo prevista:\n",
    "            <center style='margin-top:20px'>\n",
    "                $e_{ij}=v_{a}^{T}\\tanh{(W_a[s_{i-1};h_{j}])}$\n",
    "            </center>\n",
    "            <center style='margin-top:20px'>\n",
    "                $\\alpha_{ij}=\\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x}\\exp(e_{ik})}$\n",
    "            </center>\n",
    "            <center style='margin-top:20px'>\n",
    "                $c_i=\\displaystyle \\sum_{j=1}^{T_x}\\alpha_{ij}h_j$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df1573-e63f-40f6-b0d3-14f5f5024e50",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Computações Decoder</h4>\n",
    "<div style='font-size:20px'> \n",
    "    <center style='margin-top:20px'>\n",
    "        $s_i=f(s_{i-1},y_{i-1},c_i)$\n",
    "    </center>\n",
    "    <center style='margin-top:20px'>\n",
    "        $p(y_i|y_1,...,y_{i-1},x)=g(y_{i-1},s_i,c_i)$\n",
    "    </center>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a2a68-c79b-430b-83b3-872421f82f98",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Dot/Luong Attention</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O paper \"Effective Approaches to Attention-based Neural Machine Translation\" propõe uma computação mais simplificada das energias de cada palavra (dot-product). São apresentadas duas modalidades para essa conta.\n",
    "            <center style='margin-top:20px'>\n",
    "                $$\n",
    "                    e_{ij}=\n",
    "                        \\begin{cases}\n",
    "                            s_{i}^{T}h_{j}  &\\text{(dot)} \\\\\n",
    "                            s_{i}^{T}Wh_{j}  &\\text{(general)}\n",
    "                        \\end{cases}\n",
    "                $$\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            É importante frisar que o dot-attention recorre ao state <strong> atual</strong> do decoder na computação das energias. Além disso, o vetor de contexto $c_i$ será computado pela média dos hidden states das palavras ponderada pelos coeficientes gerados pela softmax. \n",
    "            <center style='margin-top:20px'>\n",
    "                $\\displaystyle c_i=\\frac{1}{T_x}\\sum_{i=1}^{T_x}\\alpha_{ij}h_{j}$\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            Assim como em Bahdanau, todas as palavras são levadas em conta na computação de $c_i$, por padrão. Essa abordagem é nomeada pelos autores como Atenção Global. A fim de reduzir o custo computacional dessa operação, o paper propôs a chamada Local Attention.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c75392-420e-4aa8-9493-734e0778a289",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Local Attention</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A Local Attention propõe criar um certo $c_i$, considerando apenas um intervalo $[p_i-D,p_i+D]$ da frase-fonte ($D$ é um inteiro pré-determinado).\n",
    "        </li>\n",
    "        <li>\n",
    "            A posição central $p_i$ pode ser simplesmente $i$, considerando que a ordenação das palavras nas frases de origem e target seja a mesma. Por outro lado, os autores também propõe uma definição um pouco mais sofisticada:\n",
    "            <center style='margin-top:20px'>\n",
    "                $p_i=T_x \\times{\\text{sigmoid}{(v_{p}^{T}\\tanh{(W_{p}s_{i})})}}$\n",
    "            </center>\n",
    "            <p style='font-size:10px;margin-left:15%'> \n",
    "                    $v_p$ e $W_p$ terão seus coeficientes aprendidos no decorrer do treinamento.\n",
    "                </p>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            Os coeficientes da média ponderada serão calculados da seguinte forma:\n",
    "             <center style='margin-top:20px'>\n",
    "                $\\displaystyle \\alpha_{ij}=\\frac{\\exp(e_{ij})}{\\sum_{k\\in{[p_i-D,p_i+D]}}\\exp(e_{ik})}\\times \\exp{(-\\frac{(j-p_i)^{2}}{2(\\frac{D}{2})^{2}})}$\n",
    "            <center style='margin-top:20px'>\n",
    "            </center>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e1c60-ece3-4814-9560-b7a611804db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html (Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162a3e0d-eb84-4adc-8943-f321af27efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 97dac8b] Continuar Attention is All You Need (Multi-Head Attention)\n",
      " 2 files changed, 10 insertions(+), 19 deletions(-)\n",
      "Enumerating objects: 11, done.\n",
      "Counting objects: 100% (10/10), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (6/6), done.\n",
      "Writing objects: 100% (6/6), 650 bytes | 650.00 KiB/s, done.\n",
      "Total 6 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/Hands-On-Machine-Learning.git\n",
      "   a9e8618..97dac8b  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -am 'Continuar Attention is All You Need (Multi-Head Attention)'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69209961-fc2f-4863-91b1-0020caae0708",
   "metadata": {},
   "source": [
    "<p style='color:red'>  Vi de Positional Encoding até Training; Continuar Attention is All You Need (Results)</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
