{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dab5d-da0e-4ce3-a3e9-54b5bcb2bfcb",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Natural Language\n",
    "Processing with RNNs and\n",
    "Attention</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O capítulo será composto por duas seções. Na primeira, iremos continuar desenvolvendo nossos estudos com RNN's, mas aplicadas no âmbito de NLP. Já na segunda, iremos dar enfoque aos mecanismos de atenção.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b714a89-a6d9-46ec-8cd1-0902fc86b0d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Generating Shakespearean Text Using a\n",
    "Character RNN</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, vamos montar uma rede capaz de gerar poemas com o estilo de escrita de Shakespeare. Ela será treinada para prever o próximo caractere a ser digitado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8c045-822d-4583-8db8-691abddf4605",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Training Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Antes de tudo, vamos baixar o corpus do projeto e associá-lo a uma variável.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67f79259-f3a1-40f6-a316-14af3e83cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o arquivo com textos do Shakespeare. \n",
    "from tensorflow.keras.utils import get_file\n",
    "file = \"shakespeare.txt\"\n",
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(file, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81299805-b45f-4f53-98af-86552c9ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d056133c-9b3e-4a51-aba1-76bcb96caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para numeralizarmos os textos, vamos recorrer à classe `Tokenizer`. Ao setarmos `char_level=True`, vamos associar cada caractere do corpus\n",
    "# a um número (começando por 1). \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2954dcc1-8079-4cb9-8435-49ac0c4d0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 18, 1, 3, 7, 2, 9, 2, 31]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de caracteres.\n",
    "tokenizer.texts_to_sequences(['Hi, there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad414cc6-d520-43b3-b853-081218bd5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que o objeto não considera caracteres acentuados, por padrão.\n",
    "len(tokenizer.texts_to_sequences(['Olá, como vai?'])[0]), len('Ola, como vai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f67dc2-1a5b-4db5-bcff-23a7ba75c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  e t o a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo índices em uma string.\n",
    "tokenizer.sequences_to_texts([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "045ae798-e219-4849-9149-bc02272a5571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribuindo os índices do texto à variável encoded. Vamos fazer uma subtração para que o primeiro índice seja atribuído a 0.\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f6fc4-0d74-4fc1-9ab6-d048faf5e4bb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> How to Split a Sequential Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos dividir aqui o nosso corpus no set de treino, validação e teste. O autor aproveita a situação, e revela os prós e contras das diferentes metodologias de separação do dataset. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, vamos optar por manter os primeiros fragmentos do corpus para treinamento, e o restante para validação e teste. Isso tem a assunção de que os mesmos padrões presentes em momentos anteriores do arquivo estarão contidos em momentos futuros (série estacionária).\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6e51d51-8c93-457d-87f0-cc6ba2150127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fd8bc-1caf-409b-8c3e-9950bbf4b73e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Chopping the Sequential Dataset into Multiple Windows</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Até agora, convertemos todo o texto em uma única instância. Como treinar um modelo com isso é inviável, temos que quebrar esses dados em janelas, tratando cada uma delas como uma instância.\n",
    "        </li>\n",
    "        <li>\n",
    "            A função `Dataset.window` cria datasets dentro de nosso dataset principal, cada um contendo uma porção específica de elementos. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0bf6c7f-ec42-47f5-acb7-84a530759555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sub-datasets de 101 caracteres. Quando definimos `shift=1`, escolhemos que a janela se desloque 1 caractere por vez para montar \n",
    "# cada sub-dataset [0-101, 1-102...].\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps+1\n",
    "# `drop_remainder` exclui as últimas janelas cujo tamanho acabaria menor do que `size`.\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844669c-dac7-4bdc-b93f-476ebc411257",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No entanto, devemos observar que as classes de modelo admitem apenas `tf.Tensor` como input. Caso quisermos converter cada Dataset aninhado em um tensor, podemos fazer um truque com a função `flat_map`, passando a ela uma lambda que cria batches de mesmo comprimento que `window_length`.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em tese, `flat_map` torna Datasets de aninhamento num único Dataset. Mas, como ele admite uma função que aplique alguma transformação nos Datasets aninhados antes da planificação, podemos aplicar `batch`. No final das contas, ficamos com tensores de mesmo conteúdo que os Datasets.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aec54b7c-4eb1-45b7-bade-3f735abf20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda w: w.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efa9441f-28f9-4d03-8ace-846a4af548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos aproveitar a situação, e já criar batches com vários chunks aleatórios.\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53bb9b81-f9d0-4d3d-9360-1d6a0ef37860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o X e y da IA. Vamos programá-la para que ela receba a sentença recortada do primeiro ao penúltimo caractere,\n",
    "# e preveja a sequência do segundo ao último caractere.\n",
    "dataset = dataset.map(lambda w: (w[:, :-1], w[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ed8a2c9-d2db-41d3-975f-033b6185850b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vamos tornar o X num One-Hot Encoding.\n",
    "from tensorflow import one_hot\n",
    "\n",
    "max_id = len(tokenizer.word_index) # Quantidade de caracteres distintos.\n",
    "n = dataset.map(lambda X,y: (one_hot(X, depth=max_id), y)).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7819b-11db-4624-931d-9d89415e75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantindo que o próximo batch a ser processado no treinamento já seja alocado à memória, antecipadamente.\n",
    "dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eda9c-72ff-4c0a-a80f-2061df54b1b5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Building and Training the Char-RNN Model</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos montar agora uma rede recorrente que preverá o próximo caractere de uma sequência.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "758623b3-929b-4d8a-8ff0-177d88d59ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69209961-fc2f-4863-91b1-0020caae0708",
   "metadata": {},
   "source": [
    "<p style='color:red'> Treinamento da rede dá bug (NoneType)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a300eac-9b1c-4a77-8c6d-804bfbcfbe89",
   "metadata": {},
   "source": [
    "<p style='color:red'> Terminei de ler WaveNet. Começar Cap 16</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d9056-27a1-4a03-afb7-f2486ecb4906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
