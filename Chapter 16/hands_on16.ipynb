{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dab5d-da0e-4ce3-a3e9-54b5bcb2bfcb",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Natural Language\n",
    "Processing with RNNs and\n",
    "Attention</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O capítulo será composto por duas seções. Na primeira, iremos continuar desenvolvendo nossos estudos com RNN's, mas aplicadas no âmbito de NLP. Já na segunda, iremos dar enfoque aos mecanismos de atenção.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b714a89-a6d9-46ec-8cd1-0902fc86b0d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Generating Shakespearean Text Using a\n",
    "Character RNN</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, vamos montar uma rede capaz de gerar poemas com o estilo de escrita de Shakespeare. Ela será treinada para prever o próximo caractere a ser digitado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8c045-822d-4583-8db8-691abddf4605",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Training Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Antes de tudo, vamos baixar o corpus do projeto e associá-lo a uma variável.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67f79259-f3a1-40f6-a316-14af3e83cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o arquivo com textos do Shakespeare. \n",
    "from tensorflow.keras.utils import get_file\n",
    "file = \"shakespeare.txt\"\n",
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(file, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81299805-b45f-4f53-98af-86552c9ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d056133c-9b3e-4a51-aba1-76bcb96caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para numeralizarmos os textos, vamos recorrer à classe `Tokenizer`. Ao setarmos `char_level=True`, vamos associar cada caractere do corpus\n",
    "# a um número (começando por 1). \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2954dcc1-8079-4cb9-8435-49ac0c4d0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 18, 1, 3, 7, 2, 9, 2, 31]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de caracteres.\n",
    "tokenizer.texts_to_sequences(['Hi, there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad414cc6-d520-43b3-b853-081218bd5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que o objeto não considera caracteres acentuados, por padrão.\n",
    "len(tokenizer.texts_to_sequences(['Olá, como vai?'])[0]), len('Ola, como vai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f67dc2-1a5b-4db5-bcff-23a7ba75c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  e t o a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo índices em uma string.\n",
    "tokenizer.sequences_to_texts([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "045ae798-e219-4849-9149-bc02272a5571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribuindo os índices do texto à variável tokenizer. Vamos fazer uma subtração para que o primeiro índice seja atribuído a 0.\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f6fc4-0d74-4fc1-9ab6-d048faf5e4bb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> How to Split a Sequential Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos dividir aqui o nosso corpus no set de treino, validação e teste. O autor aproveita a situação, e revela os prós e contras das diferentes metodologias de separação do dataset. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, vamos optar por manter os primeiros fragmentos do corpus para treinamento, e o restante para validação e teste. Isso tem a assunção de que os mesmos padrões presentes em momentos anteriores do arquivo estarão contidos em momentos futuros (série estacionária).\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6e51d51-8c93-457d-87f0-cc6ba2150127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fd8bc-1caf-409b-8c3e-9950bbf4b73e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Chopping the Sequential Dataset into Multiple Windows</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Até agora, convertemos todo o texto em uma única instância. Como treinar um modelo com isso é inviável, temos que quebrar esses dados em janelas, tratando cada uma delas como uma instância.\n",
    "        </li>\n",
    "        <li>\n",
    "            A função `Dataset.window` cria datasets dentro de nosso dataset principal, cada um contendo uma porção específica de elementos. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0bf6c7f-ec42-47f5-acb7-84a530759555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sub-datasets de 101 caracteres. Quando definimos `shift=1`, escolhemos que a janela se desloque 1 caractere por vez para montar \n",
    "# cada sub-dataset [0-101, 1-102...].\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps+1\n",
    "# `drop_remainder` exclui as últimas janelas cujo tamanho acabaria menor do que `size`.\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844669c-dac7-4bdc-b93f-476ebc411257",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No entanto, devemos observar que as classes de modelo admitem apenas `tf.Tensor` como input. Caso quisermos converter cada Dataset aninhado em um tensor, podemos fazer um truque com a função `flat_map`, passando a ela uma lambda que cria batches de mesmo comprimento que `window_length`.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em tese, `flat_map` torna Datasets de aninhamento num único Dataset. Mas, como ele admite uma função que aplique alguma transformação nos Datasets aninhados antes da planificação, podemos aplicar `batch` que no final das contas apenas os converta em tensores.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69209961-fc2f-4863-91b1-0020caae0708",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Creating the Training Dataset e How to Split a Sequential Dataset; Chopping the Sequential Dataset into Multiple Windows</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a300eac-9b1c-4a77-8c6d-804bfbcfbe89",
   "metadata": {},
   "source": [
    "<p style='color:red'> Terminei de ler WaveNet. Começar Cap 16</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
