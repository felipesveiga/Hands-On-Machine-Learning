{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dab5d-da0e-4ce3-a3e9-54b5bcb2bfcb",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Natural Language\n",
    "Processing with RNNs and\n",
    "Attention</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O capítulo será composto por duas seções. Na primeira, iremos continuar desenvolvendo nossos estudos com RNN's, mas aplicadas no âmbito de NLP. Já na segunda, iremos dar enfoque aos mecanismos de atenção.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b714a89-a6d9-46ec-8cd1-0902fc86b0d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Generating Shakespearean Text Using a\n",
    "Character RNN</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, vamos montar uma rede capaz de gerar poemas com o estilo de escrita de Shakespeare. Ela será treinada para prever o próximo caractere a ser digitado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8c045-822d-4583-8db8-691abddf4605",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Training Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Antes de tudo, vamos baixar o corpus do projeto e associá-lo a uma variável.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f79259-f3a1-40f6-a316-14af3e83cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o arquivo com textos do Shakespeare. \n",
    "from tensorflow.keras.utils import get_file\n",
    "file = \"shakespeare.txt\"\n",
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(file, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81299805-b45f-4f53-98af-86552c9ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d056133c-9b3e-4a51-aba1-76bcb96caa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para numeralizarmos os textos, vamos recorrer à classe `Tokenizer`. Ao setarmos `char_level=True`, vamos associar cada caractere do corpus\n",
    "# a um número (começando por 1). \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2954dcc1-8079-4cb9-8435-49ac0c4d0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 18, 1, 3, 7, 2, 9, 2, 31]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de caracteres.\n",
    "tokenizer.texts_to_sequences(['Hi, there!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad414cc6-d520-43b3-b853-081218bd5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que o objeto não considera caracteres acentuados, por padrão.\n",
    "len(tokenizer.texts_to_sequences(['Olá, como vai?'])[0]), len('Ola, como vai?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f67dc2-1a5b-4db5-bcff-23a7ba75c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  e t o a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo índices em uma string.\n",
    "tokenizer.sequences_to_texts([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ae798-e219-4849-9149-bc02272a5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuindo os índices do texto à variável tokenizer. Vamos fazer uma subtração para que o primeiro índice seja atribuído a 0.\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f6fc4-0d74-4fc1-9ab6-d048faf5e4bb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> How to Split a Sequential Dataset</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos dividir aqui o nosso corpus no set de treino, validação e teste. O autor aproveita a situação, e revela os prós e contras das diferentes metodologias de separação do dataset. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, vamos optar por manter os primeiros fragmentos do corpus para treinamento, e o restante para validação e teste. Isso tem a assunção de que os mesmos padrões presentes em momentos anteriores do arquivo estarão contidos em momentos futuros (série estacionária).\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e51d51-8c93-457d-87f0-cc6ba2150127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "train_size = encoded.shape[0] * 90 // 100\n",
    "dataset = Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69209961-fc2f-4863-91b1-0020caae0708",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Creating the Training Dataset e How to Split a Sequential Dataset; Chopping the Sequential Dataset into Multiple Windows</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a300eac-9b1c-4a77-8c6d-804bfbcfbe89",
   "metadata": {},
   "source": [
    "<p style='color:red'> Terminei de ler WaveNet. Começar Cap 16</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
