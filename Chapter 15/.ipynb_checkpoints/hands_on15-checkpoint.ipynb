{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1642882c-6867-4b6d-a98d-020ade97d8ac",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Processing Sequences Using RNNs and CNNs</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As Redes Neurais Recorrentes (RNN's) são uma modalidade de modelos de Deep Learning que demonstraram bastante sucesso em previsões de séries temporais extensas e NLP.\n",
    "        </li>\n",
    "        <li> \n",
    "            No entanto, elas apresentam dois grandes problemas: instabilidade de gradientes e uma memória de curto-prazo bastante limitada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0f8c5-1a11-48f3-9271-aaf6e1ca8766",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Recurrent Neurons and Layers</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O principal componente das RNN's são os neurônios recorrentes. Eles funcionam de maneira bastante similar às camadas dos MLP's, com o acréscimo de eles somarem o output da última iteração ao produto da função linear.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='recurrent_function.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Podemos ilustrar a passagem do output $y_{t-1}$ por um diagrama.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='rnn_unrolled.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Observe que $Y_{t-1}$ é uma função de $X_{t-1}$ e $Y_{t-2}$. Este, por sua vez, é uma função de $X_{t-2}$ e $Y_{t-3}$. Então, as iterações anteriores sempre impactarão o resultado da atual.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36856edd-11bc-476b-84c9-1bb6121f07f7",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Memory Cell</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As camadas recorrentes também são conhecidas como células de memória, por conta da sua capacidade de memorizar os outputs de iterações anteriores.\n",
    "        </li>\n",
    "        <li> \n",
    "            Aqui é distinguido também o significado de state e output de uma memory cell. O state é a função composta $h_{(t)}=f(h_{(t-1)}, x_{(t)})$ que vimos acima e ele nem sempre será o output da célula, como sugere a representação abaixo.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='cell_state_outputs.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b3508-fdba-4fda-b053-407f38e5d3c9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Input and Output Sequences</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Existem inúmeras modalidades de RNN's, sendo cada uma delas utilizada em tarefas distintas.  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce02fb-faf3-4095-baad-11c283df3b6e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sequence-to-Sequence Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Recebe uma sequência de inputs para lançar uma sequência de outputs. Bastante utilizada na previsão de séries temporais.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='seq_seq.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed4b5f-fa73-4f0b-bba6-7900af1d7236",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sequence-to-Vector Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Essa RNN propõe receber uma sequência de inputs e levar em conta apenas o output da última iteração. \n",
    "        </li> \n",
    "        <li>\n",
    "            Por exemplo, podemos abastecê-la com uma sequência de palavras e fazê-la lançar um score sentimental (0=insatisfeito, 1=muito satisfeito) \n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='seq_vec.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc819e01-16c0-43c5-9bbb-96ad7d94691b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vector-to-Sequence Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Capaz de gerar sequências de dados com base no abastecimento de um único vetor. \n",
    "        </li> \n",
    "        <li>\n",
    "            Essa modalidade pode ser usada em sistemas de descrição de fotografias.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='vec_seq.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052b691-611b-4051-943e-ceb9e14abc14",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoder-Decoder Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É uma rede híbrida que possui uma RNN sequence-to-vector cujo output alimenta outra RNN vector-to-sequence.\n",
    "        </li>\n",
    "        <li>\n",
    "            Pode ser bastante útil em sistemas de tradução de frases.\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa arquitetura inspirou o surgimento dos Transformers, modelos considerados SOTA principalmente no âmbito do NLP. \n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='encoder_decoder.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971daa21-2c74-492d-bf20-5d3d048e18bd",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Forecasting a Time Series</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>  \n",
    "            Uma série temporal consiste em uma sequência de dados ao longo de um período de tempo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa série pode envolver uma única sequência de dados (univarial), ou várias (multivarial). Por exemplo, poderíamos criar uma RNN para prever o Dividend Yield de uma companhia, ou para estimar o valor de uma série de métricas sobre a sua saúde financeira (Dívida, Lucro, etc).\n",
    "        </li>\n",
    "        <li>\n",
    "            Podemos tanto usar nossos modelos para prever valores futuros da sequência, quanto números passados, no caso de eles constarem como nulos na tabela. Esse último caso de uso leva o nome de imputação.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e7b434-80e1-4f63-9c86-e1e835495b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer uma breve demonstração do uso de RNN's em Séries Temporais.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_time_series(batch_size:int, n_steps:int)->np.array:\n",
    "    '''\n",
    "        Gera uma quantidade `batch_size` de séries temporais com `n_steps` de comprimento.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        `batch_size`: int\n",
    "            Número de séries temporais.\n",
    "        `n_steps`: int\n",
    "            Tamanho das séries.\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        Um `np.array` com os dados das séries temporais. Elas serão a soma de duas funções seno mais um noise Gaussiano.\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = .5 * np.sin((time-offsets1) * (freq1*10+10))\n",
    "    series += .2 * np.sin((time-offsets2) * (freq2*10+10))\n",
    "    series += .1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035c96e4-93a3-4b8e-b1c4-fa6b1f2be852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando 10000 séries temporais de 51 períodos.\n",
    "# Nossa target será o último valor dessas séries.\n",
    "n_steps = 50 \n",
    "series = generate_time_series(10000, n_steps+1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4f27c-04f9-4c22-826c-6089f18a25c4",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>  \n",
    "            Os dados de séries temporais costumam ser armazenados em matrizes 3-D, do formato $[\\text{n-series, n-steps, dimensionality}]$. No caso de séries multivariais, dimensionality sempre será maior do que 1.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef271b0d-8b42-476d-b47d-e8e18d03623a",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Baseline Metrics</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos criar aqui algumas baselines que representarão a menor performance esperada para o modelo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ac879-53bb-42a2-b8f5-1455d6564b84",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Naïve Approach</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Esse método consiste em avaliar a performance, caso o modelo apenas preveja o último valor de cada série de $X$ (ou o penúltimo número da série como um todo). \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f29bd78-c9c9-4952-961c-3d6f1afe4cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.014811385>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from tensorflow.math import reduce_mean\n",
    "y_pred = X_valid[:, -1]\n",
    "mean_squared_error(y_valid.flatten(), y_pred.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c602e-0e5f-4bff-b5fd-7e662b25bc51",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Simple Models</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Modelos menos complexos, como a Regressão Linear, podem também ser considerados como baseline devido à sua capacidade limitada de aprender padrões dos dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f8bac4-8574-4e22-986a-9db6ca05b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "lr = Sequential([\n",
    "    Flatten(input_shape=[50, 1]), # Vamos ter que tornar o array 2-D para que a rede o receba.\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8882ef1-a4d1-40ae-9df3-7fc0f57ecb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando esse código no Kaggle, devemos obter um MSE de cerca de 0.0017, melhor do que nossa abordagem Naïve.\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "lr.compile(optimizer='adam', loss=mean_squared_error)\n",
    "lr.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f1bdc-e149-4939-a7cb-19e8f7b18d4f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Implementing a Simple RNN</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como já mencionamos, uma camada recorrente tem funcionamento bastante parecido com o de uma Dense, com exceção da soma do state anterior $h_{(n-1)}$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para cada série temporal, passamos os dados de uma única instância para o neurônio, que computará a soma ponderada com o hidden state anterior e pasará esse produto à função de ativação. Esse resultado final ficará armazenado como o hidden state do próxima iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fc7063-fb5d-4538-b111-8599207284d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando essa pequena RNN, espera-se um MSE de aproximadamente 0.008. \n",
    "# O motivo desse modelo ser pior do que a Regressão Linear é o fato dessa ter um coeficiente por time step. Já a camada SimpleRNN\n",
    "# designa um único coeficiente à toda série."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e72db78-5914-4369-9686-f4cd3a7299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por padrão, as camadas recorrentes lançam apenas o último output. Para fazer com que ela lance os resultados da função para\n",
    "# cada time step, passe o argumento `return_sequences=True`\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "rnn = Sequential([\n",
    "    SimpleRNN(1, input_shape=[None, 1]) # Lembrando, as RNN's admitem séries de todos os tamanhos. Por isso, podemos definir a primeira\n",
    "                                    # dimensão como None.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dc936-601a-423a-9178-d932ddfda068",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', loss=mean_squared_error)\n",
    "rnn.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4719c8-e344-464e-83ca-425e3d035565",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Trend x Seasonality</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os conceitos de Trend e Seasonality são bastante discutidos no âmbito de análise de séries temporais.\n",
    "            <ul> \n",
    "                <li> \n",
    "                    <i> Trend:</i> Uma tendência consiste no crescimento ou decréscimo da variável numa janela de longo prazo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    <i> Seasonality:</i> Uma sazonalidade é uma alteração da variável por questões sazonais. Por exemplo, o consumo de açaí no Brasil tende a ser maior entre os meses de Novembro-Março por conta das temperaturas mais altas desse período de tempo. \n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696762d6-cc6b-4cc4-b260-99753d146aae",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Deep RNN's</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Podemos tentar aprimorar os resultados de nossa rede recorrente pondo mais camadas nela.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a9af6-68ec-407b-ba30-812cdcb7466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note que as camadas anteriores à de output devem retornar a sequência de suas previsões. Isso porque, caso o contrário, a próxima `SimpleRNN`\n",
    "# receberá apenas um array 2-d (lembre-se que ela espera receber um 3-D!).\n",
    "rnn2 = Sequential([\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=True),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    SimpleRNN(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e3746-a8ee-4b24-aedd-ab9ba714d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2.compile(optimizer='adam', loss=mean_squared_error)\n",
    "rnn2.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed3841-adbd-474d-aacc-2ab0a3da7b1c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor nos aconselha a utilizar uma Dense layer como camada de output. Isso oferece ganho na velocidade da convergência do modelo. \n",
    "        </li>\n",
    "        <li>\n",
    "            Nesse caso, ponha `return_sequences` da penúltima camada como False, porque camadas densas apenas admitem inputs 2-D.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7654a69-e226-4abb-9955-353fd8951c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = Sequential([\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=True),\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=False),\n",
    "    Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a3fb2-947c-40ce-ae13-4f3ea3a21656",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Forecasting Several Time Steps Ahead</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            As RNN's podem ser programadas para preverem os valores da série temporal de n steps adiante do atual.\n",
    "        </li>\n",
    "        <li>\n",
    "            Nessa situação, podemos configurar o treinamento do modelo para envolver a previsão dos valores das próximas n steps da série. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940b6d-2be2-4e00-9f78-3bf8dc6b3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando uma nova time series, agora com 10 time steps a mais.\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps+10)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10a4e2-fa47-4fd5-b36c-757d49909726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A target de cada time step de cada time series serão os próximos 10 steps.\n",
    "y = np.empty((10000, n_steps, 10))\n",
    "\n",
    "# Em cada iteração, acrescentamos a n-ésima target de cada série temporal.\n",
    "for step_ahead in range(1, 10+1):\n",
    "    y[..., step_ahead-1] = series[:, step_ahead:step_ahead+n_steps, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576683c-bd02-4e67-a8d5-87ec71ec8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:7000]\n",
    "y_valid = y[7000:9000]\n",
    "y_test = y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee568c3d-41fa-4869-83ea-9979a6b5ecff",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Para garantir que nós criemos uma RNN sequence-to-sequence, passe o argumento `return_sequences` como True. A `Dense` deverá conter 10 neurônios (um para cada time step) e estar encapsulada dentro de uma `TimeDistributed` layer. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa29ef-aa21-4bf4-a237-952ed92e4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "rnn4 = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be745303-eb5f-47b1-9b7c-c53c05f67efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loss do modelo sempre levará em conta as previsões feitas em cada time step.\n",
    "# Comom nos importamos apenas com as estimativas do último step, vamos criar uma métrica de MSE específica para isso.\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "def mse_last_step(y_true, y_pred)->float:\n",
    "    return mean_squared_error(y_true[:, -1], y_pred[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e3ebb-7298-46d4-ac83-4f617c818717",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> MC Dropout</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor nos sugere usar Monte Carlo Dropout sobre nossas camadas recorrentes. Podemos criar intervalos de confiança sobre as nossas previsões.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d7e17-5295-4e8e-9de0-390d3c736d6d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Handling Long Sequences</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O treinamento de RNN's em longas sequências de dados pode levar à desestabilização dos gradientes, ou esquecimento dos primeiros inputs da série. Felizmente, temos alguns métodos que podem aliviar esses problemas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb37644-76db-404e-9f6a-64970bac796d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Fighting the Unstable Gradients Problem</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No contexto das séries temporais, o uso de funções de ativação não-saturantes pode contribuir para a explosão dos gradientes. Pensando que há uma tendência de subida no set de treino, os coeficientes da rede vão ser forçados a assumirem valores cada vez maiores para acertarem as previsões. Por isso, recorrer a funções saturantes (como a tanh) pode ajudar a impedir esse problema.\n",
    "        </li>\n",
    "        <li>\n",
    "            A Batch Normalization também não é tão boa com RNN's. Ao invés dela, costumamos usar a Layer Normalization, que extrai as estatísticas pelo eixo de features, e não de batch.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689592f-ffce-47f6-905e-4699d5ef87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LayerNormalization, SimpleRNNCell, TimeDistributed, RNN, Layer\n",
    "from tensorflow.keras.activations import get\n",
    "from tensorflow import Tensor\n",
    "from typing import List, Tuple\n",
    "\n",
    "class LNSimpleRNNCell(Layer):\n",
    "    '''\n",
    "        Célula de memória que aplica Layer Normalization aos produtos da função linear, antes da aplicação da função de ativação.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        `units`: int\n",
    "            Número de neurônios da camada.\n",
    "        `activation`: str\n",
    "            Função de ativação da camada. Default é 'tanh'.\n",
    "        **kwargs serão passaos à camada-base da classe (keras.layers.Layer).\n",
    "    '''\n",
    "    def __init__(self, units:int, activation='tanh', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.activation = activation\n",
    "        self.simple_rnn_cell = SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, states)->Tuple[Tensor, List[Tensor]]:\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bebd85-8dcd-434e-9219-75e99b6954e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando uma pequena RNN com nossas camadas de Layer Normalization.\n",
    "ln_model = Sequential([\n",
    "    RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0975e1a-389c-43a4-b63a-3b40afdab4cb",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tackling the Short-Term Memory Problem</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            As RNN's têm o defeito de seus hidden states perderem informações dos primeiros inputs ao longo do processamento. Isso pode ser danoso em tarefas de tradução, por exemplo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Como solução, algumas células com memória de longo prazo foram desenvolvidas. Elas se provaram tão úteis que se prefere utilizá-las ao invés das células tradicionais. A LSTM é a célula mais popular do mercado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b1519-b979-4d02-9818-65e07fe9149d",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Long Short-Term Memory (LSTM) Cells</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A arquitetura das células LSTM permite que se escolha quais eventos de longo prazo descartar, quais registrar e quais considerar na geração do output.\n",
    "        </li>\n",
    "        <li> \n",
    "            Elas possuem um state de longo prazo $c_{(t)}$ e curto prazo $h_{(t)}$.\n",
    "            <center style='margin-top:20px'> \n",
    "                <h1> Arquitetura LSTM</h1>\n",
    "                <img src='lstm_diagram.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9976627-3a6e-4fa3-9c31-fc30464249a6",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Todas essas filtragens são feitas graças aos gates (representados pelas funções $f_{(t)}$, $i_{(t)}$ e $o_{(t)}$)\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f646b7-f913-46a2-a4c5-1efeb59736cc",
   "metadata": {},
   "source": [
    "<p style='color:red'> In short, an LSTM cell can learn to recognize (p.685)</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
