{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1642882c-6867-4b6d-a98d-020ade97d8ac",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Processing Sequences Using RNNs and CNNs</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As Redes Neurais Recorrentes (RNN's) são uma modalidade de modelos de Deep Learning que demonstraram bastante sucesso em previsões de séries temporais extensas e NLP.\n",
    "        </li>\n",
    "        <li> \n",
    "            No entanto, elas apresentam dois grandes problemas: instabilidade de gradientes e uma memória de curto-prazo bastante limitada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0f8c5-1a11-48f3-9271-aaf6e1ca8766",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Recurrent Neurons and Layers</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O principal componente das RNN's são os neurônios recorrentes. Eles funcionam de maneira bastante similar às camadas dos MLP's, com o acréscimo de eles somarem o output da última iteração ao produto da função linear.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='recurrent_function.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Podemos ilustrar a passagem do output $y_{t-1}$ por um diagrama.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='rnn_unrolled.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'> \n",
    "            Observe que $Y_{t-1}$ é uma função de $X_{t-1}$ e $Y_{t-2}$. Este, por sua vez, é uma função de $X_{t-2}$ e $Y_{t-3}$. Então, as iterações anteriores sempre impactarão o resultado da atual.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36856edd-11bc-476b-84c9-1bb6121f07f7",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Memory Cell</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As camadas recorrentes também são conhecidas como células de memória, por conta da sua capacidade de memorizar os outputs de iterações anteriores.\n",
    "        </li>\n",
    "        <li> \n",
    "            Aqui é distinguido também o significado de state e output de uma memory cell. O state é a função composta $h_{(t)}=f(h_{(t-1)}, x_{(t)})$ que vimos acima e ele nem sempre será o output da célula, como sugere a representação abaixo.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='cell_state_outputs.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b3508-fdba-4fda-b053-407f38e5d3c9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Input and Output Sequences</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Existem inúmeras modalidades de RNN's, sendo cada uma delas utilizada em tarefas distintas.  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce02fb-faf3-4095-baad-11c283df3b6e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sequence-to-Sequence Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Recebe uma sequência de inputs para lançar uma sequência de outputs. Bastante utilizada na previsão de séries temporais.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='seq_seq.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed4b5f-fa73-4f0b-bba6-7900af1d7236",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sequence-to-Vector Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Essa RNN propõe receber uma sequência de inputs e levar em conta apenas o output da última iteração. \n",
    "        </li> \n",
    "        <li>\n",
    "            Por exemplo, podemos abastecê-la com uma sequência de palavras e fazê-la lançar um score sentimental (0=insatisfeito, 1=muito satisfeito) \n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='seq_vec.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc819e01-16c0-43c5-9bbb-96ad7d94691b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vector-to-Sequence Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Capaz de gerar sequências de dados com base no abastecimento de um único vetor. \n",
    "        </li> \n",
    "        <li>\n",
    "            Essa modalidade pode ser usada em sistemas de descrição de fotografias.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='vec_seq.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052b691-611b-4051-943e-ceb9e14abc14",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoder-Decoder Networks </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É uma rede híbrida que possui uma RNN sequence-to-vector cujo output alimenta outra RNN vector-to-sequence.\n",
    "        </li>\n",
    "        <li>\n",
    "            Pode ser bastante útil em sistemas de tradução de frases.\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa arquitetura inspirou o surgimento dos Transformers, modelos considerados SOTA principalmente no âmbito do NLP. \n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='encoder_decoder.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971daa21-2c74-492d-bf20-5d3d048e18bd",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Forecasting a Time Series</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>  \n",
    "            Uma série temporal consiste em uma sequência de dados ao longo de um período de tempo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa série pode envolver uma única sequência de dados (univarial), ou várias (multivarial). Por exemplo, poderíamos criar uma RNN para prever o Dividend Yield de uma companhia, ou para estimar o valor de uma série de métricas sobre a sua saúde financeira (Dívida, Lucro, etc).\n",
    "        </li>\n",
    "        <li>\n",
    "            Podemos tanto usar nossos modelos para prever valores futuros da sequência, quanto números passados, no caso de eles constarem como nulos na tabela. Esse último caso de uso leva o nome de imputação.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e7b434-80e1-4f63-9c86-e1e835495b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer uma breve demonstração do uso de RNN's em Séries Temporais.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_time_series(batch_size:int, n_steps:int)->np.array:\n",
    "    '''\n",
    "        Gera uma quantidade `batch_size` de séries temporais com `n_steps` de comprimento.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        `batch_size`: int\n",
    "            Número de séries temporais.\n",
    "        `n_steps`: int\n",
    "            Tamanho das séries.\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        Um `np.array` com os dados das séries temporais. Elas serão a soma de duas funções seno mais um noise Gaussiano.\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = .5 * np.sin((time-offsets1) * (freq1*10+10))\n",
    "    series += .2 * np.sin((time-offsets2) * (freq2*10+10))\n",
    "    series += .1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035c96e4-93a3-4b8e-b1c4-fa6b1f2be852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando 10000 séries temporais de 51 períodos.\n",
    "# Nossa target será o último valor dessas séries.\n",
    "n_steps = 50 \n",
    "series = generate_time_series(10000, n_steps+1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4f27c-04f9-4c22-826c-6089f18a25c4",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>  \n",
    "            Os dados de séries temporais costumam ser armazenados em matrizes 3-D, do formato $[\\text{n-series, n-steps, dimensionality}]$. No caso de séries multivariais, dimensionality sempre será maior do que 1.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef271b0d-8b42-476d-b47d-e8e18d03623a",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Baseline Metrics</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos criar aqui algumas baselines que representarão a menor performance esperada para o modelo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ac879-53bb-42a2-b8f5-1455d6564b84",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Naïve Approach</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Esse método consiste em avaliar a performance, caso o modelo apenas preveja o último valor de cada série de $X$ (ou o penúltimo número da série como um todo). \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f29bd78-c9c9-4952-961c-3d6f1afe4cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.014811385>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from tensorflow.math import reduce_mean\n",
    "y_pred = X_valid[:, -1]\n",
    "mean_squared_error(y_valid.flatten(), y_pred.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c602e-0e5f-4bff-b5fd-7e662b25bc51",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Simple Models</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Modelos menos complexos, como a Regressão Linear, podem também ser considerados como baseline devido à sua capacidade limitada de aprender padrões dos dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f8bac4-8574-4e22-986a-9db6ca05b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "lr = Sequential([\n",
    "    Flatten(input_shape=[50, 1]), # Vamos ter que tornar o array 2-D para que a rede o receba.\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8882ef1-a4d1-40ae-9df3-7fc0f57ecb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando esse código no Kaggle, devemos obter um MSE de cerca de 0.0017, melhor do que nossa abordagem Naïve.\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "lr.compile(optimizer='adam', loss=mean_squared_error)\n",
    "lr.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f1bdc-e149-4939-a7cb-19e8f7b18d4f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Implementing a Simple RNN</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como já mencionamos, uma camada recorrente tem funcionamento bastante parecido com o de uma Dense, com exceção da soma do state anterior $h_{(n-1)}$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para cada série temporal, passamos os dados de uma única instância para o neurônio, que computará a soma ponderada com o hidden state anterior e pasará esse produto à função de ativação. Esse resultado final ficará armazenado como o hidden state do próxima iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fc7063-fb5d-4538-b111-8599207284d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando essa pequena RNN, espera-se um MSE de aproximadamente 0.008. \n",
    "# O motivo desse modelo ser pior do que a Regressão Linear é o fato dessa ter um coeficiente por time step. Já a camada SimpleRNN\n",
    "# designa um único coeficiente à toda série."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e72db78-5914-4369-9686-f4cd3a7299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por padrão, as camadas recorrentes lançam apenas o último output. Para fazer com que ela lance os resultados da função para\n",
    "# cada time step, passe o argumento `return_sequences=True`\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "rnn = Sequential([\n",
    "    SimpleRNN(1, input_shape=[None, 1]) # Lembrando, as RNN's admitem séries de todos os tamanhos. Por isso, podemos definir a primeira\n",
    "                                    # dimensão como None.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dc936-601a-423a-9178-d932ddfda068",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', loss=mean_squared_error)\n",
    "rnn.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4719c8-e344-464e-83ca-425e3d035565",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Trend x Seasonality</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os conceitos de Trend e Seasonality são bastante discutidos no âmbito de análise de séries temporais.\n",
    "            <ul> \n",
    "                <li> \n",
    "                    <i> Trend:</i> Uma tendência consiste no crescimento ou decréscimo da variável numa janela de longo prazo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    <i> Seasonality:</i> Uma sazonalidade é uma alteração da variável por questões sazonais. Por exemplo, o consumo de açaí no Brasil tende a ser maior entre os meses de Novembro-Março por conta das temperaturas mais altas desse período de tempo. \n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696762d6-cc6b-4cc4-b260-99753d146aae",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Deep RNN's</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Podemos tentar aprimorar os resultados de nossa rede recorrente pondo mais camadas nela.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a9af6-68ec-407b-ba30-812cdcb7466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note que as camadas anteriores à de output devem retornar a sequência de suas previsões. Isso porque, caso o contrário, a próxima `SimpleRNN`\n",
    "# receberá apenas um array 2-d (lembre-se que ela espera receber um 3-D!).\n",
    "rnn2 = Sequential([\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=True),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    SimpleRNN(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e3746-a8ee-4b24-aedd-ab9ba714d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2.compile(optimizer='adam', loss=mean_squared_error)\n",
    "rnn2.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed3841-adbd-474d-aacc-2ab0a3da7b1c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor nos aconselha a utilizar uma Dense layer como camada de output. Isso oferece ganho na velocidade da convergência do modelo. \n",
    "        </li>\n",
    "        <li>\n",
    "            Nesse caso, ponha `return_sequences` da penúltima camada como False, porque camadas densas apenas admitem inputs 2-D.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7654a69-e226-4abb-9955-353fd8951c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = Sequential([\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=True),\n",
    "    SimpleRNN(20, input_shape=[None, 1], return_sequences=False),\n",
    "    Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a3fb2-947c-40ce-ae13-4f3ea3a21656",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Forecasting Several Time Steps Ahead</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            As RNN's podem ser programadas para preverem os valores da série temporal de n steps adiante do atual.\n",
    "        </li>\n",
    "        <li>\n",
    "            Nessa situação, podemos configurar o treinamento do modelo para envolver a previsão dos valores das próximas n steps da série. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940b6d-2be2-4e00-9f78-3bf8dc6b3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando uma nova time series, agora com 10 time steps a mais.\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps+10)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10a4e2-fa47-4fd5-b36c-757d49909726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A target de cada time step de cada time series serão os próximos 10 steps.\n",
    "y = np.empty((10000, n_steps, 10))\n",
    "\n",
    "# Em cada iteração, acrescentamos a n-ésima target de cada série temporal.\n",
    "for step_ahead in range(1, 10+1):\n",
    "    y[..., step_ahead-1] = series[:, step_ahead:step_ahead+n_steps, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576683c-bd02-4e67-a8d5-87ec71ec8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:7000]\n",
    "y_valid = y[7000:9000]\n",
    "y_test = y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee568c3d-41fa-4869-83ea-9979a6b5ecff",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Para garantir que nós criemos uma RNN sequence-to-sequence, passe o argumento `return_sequences` como True. A `Dense` deverá conter 10 neurônios (um para cada time step) e estar encapsulada dentro de uma `TimeDistributed` layer. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa29ef-aa21-4bf4-a237-952ed92e4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "rnn4 = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be745303-eb5f-47b1-9b7c-c53c05f67efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loss do modelo sempre levará em conta as previsões feitas em cada time step.\n",
    "# Comom nos importamos apenas com as estimativas do último step, vamos criar uma métrica de MSE específica para isso.\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "def mse_last_step(y_true, y_pred)->float:\n",
    "    return mean_squared_error(y_true[:, -1], y_pred[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e3ebb-7298-46d4-ac83-4f617c818717",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> MC Dropout</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor nos sugere usar Monte Carlo Dropout sobre nossas camadas recorrentes. Podemos criar intervalos de confiança sobre as nossas previsões.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d7e17-5295-4e8e-9de0-390d3c736d6d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Handling Long Sequences</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O treinamento de RNN's em longas sequências de dados pode levar à desestabilização dos gradientes, ou esquecimento dos primeiros inputs da série. Felizmente, temos alguns métodos que podem aliviar esses problemas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb37644-76db-404e-9f6a-64970bac796d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Fighting the Unstable Gradients Problem</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No contexto das séries temporais, o uso de funções de ativação não-saturantes pode contribuir para a explosão dos gradientes. Pensando que há uma tendência de subida no set de treino, os coeficientes da rede vão ser forçados a assumirem valores cada vez maiores para acertarem as previsões. Por isso, recorrer a funções saturantes (como a tanh) pode ajudar a impedir esse problema.\n",
    "        </li>\n",
    "        <li>\n",
    "            A Batch Normalization também não é tão boa com RNN's. Ao invés dela, costumamos usar a Layer Normalization, que extrai as estatísticas pelo eixo de features, ee não de batch.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c72e050-f5d4-4d07-9872-f124ef08691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.45006238e-01,  6.62552494e-01,  5.28482224e-02, ...,\n",
       "         -5.31063616e-02, -6.70317220e-01, -4.95059465e-01],\n",
       "        [-4.87928557e-01, -5.64580270e-01,  7.72686150e-01, ...,\n",
       "          1.97986537e-01, -3.44201573e-04, -2.84936637e-01],\n",
       "        [-5.90067912e-01,  3.37622362e-01,  4.14486050e-02, ...,\n",
       "          6.03802228e-02, -1.51728066e-01,  3.76105411e-01],\n",
       "        ...,\n",
       "        [ 7.63309635e-01, -6.58683530e-01,  9.78077620e-02, ...,\n",
       "         -6.47381469e-01,  3.53266570e-01, -5.08065751e-01],\n",
       "        [-1.03862977e-01,  1.67268031e-01, -3.81616975e-01, ...,\n",
       "          8.27266222e-01, -7.27184324e-03,  3.13620406e-01],\n",
       "        [-3.75440087e-01,  4.74706963e-01, -1.14277663e-01, ...,\n",
       "         -6.85283637e-01,  3.21354257e-01,  3.22301881e-01]],\n",
       "\n",
       "       [[-5.95305820e-01, -7.19777070e-01, -3.11994917e-01, ...,\n",
       "         -3.64202487e-02,  5.98312696e-02, -2.70819656e-01],\n",
       "        [-1.58220685e-01, -4.99833617e-01,  1.21696025e-01, ...,\n",
       "          4.83798490e-01,  1.29359365e-01, -6.33854992e-01],\n",
       "        [ 7.95287042e-01,  4.61155256e-01,  3.36471014e-01, ...,\n",
       "          6.43722306e-02, -1.51429329e-02, -6.31843754e-01],\n",
       "        ...,\n",
       "        [-2.13715275e-01,  5.20413523e-01,  5.72017979e-02, ...,\n",
       "          2.67172086e-01,  5.32247658e-02, -7.57802963e-01],\n",
       "        [-1.03831619e-01, -2.28084586e-01,  7.05035056e-01, ...,\n",
       "          1.69814457e-02,  5.20339784e-01,  6.06384795e-01],\n",
       "        [-4.67266220e-01, -9.15709598e-01, -3.23645306e-01, ...,\n",
       "         -2.71490345e-01, -2.16270670e-01,  5.90755657e-01]],\n",
       "\n",
       "       [[-7.60244642e-01,  3.49085625e-02,  2.01127104e-01, ...,\n",
       "          3.93958755e-02, -1.48705003e-02, -4.14970559e-01],\n",
       "        [-3.99169300e-01, -5.50799695e-01, -6.33711973e-01, ...,\n",
       "          5.86668184e-02, -6.64525628e-01,  2.76388496e-01],\n",
       "        [ 3.98024538e-01,  4.73020103e-02,  6.08245747e-02, ...,\n",
       "         -9.89306961e-03,  3.16023949e-01, -4.94998233e-01],\n",
       "        ...,\n",
       "        [ 2.31307769e-01,  5.56270420e-02,  1.74390515e-01, ...,\n",
       "          3.40916302e-01, -4.61772324e-01, -1.16331093e-01],\n",
       "        [ 8.67739733e-02, -7.83824168e-01,  1.02171391e-01, ...,\n",
       "          9.33496670e-01, -5.43586091e-01,  1.19584661e-01],\n",
       "        [-6.46382884e-01,  3.55167799e-01,  5.29357869e-01, ...,\n",
       "         -4.29880608e-01, -1.73590771e-01, -7.36824347e-01]],\n",
       "\n",
       "       [[ 4.92173641e-01, -9.61825992e-01, -1.90452058e-01, ...,\n",
       "          2.09728947e-01,  2.52028487e-01, -3.01361225e-01],\n",
       "        [-2.76079987e-02, -1.74148269e-01, -1.82797698e-01, ...,\n",
       "         -2.34928807e-01, -1.89904522e-01, -1.65969344e-01],\n",
       "        [ 4.54803032e-01, -5.41411436e-01,  3.66757397e-01, ...,\n",
       "         -4.36115665e-01,  3.37196204e-01,  3.32056413e-01],\n",
       "        ...,\n",
       "        [ 3.45757990e-01,  9.79450321e-02,  2.00564124e-01, ...,\n",
       "         -7.76911618e-02,  4.68736037e-01, -6.57224641e-01],\n",
       "        [ 6.53223692e-01, -1.69974775e-01, -4.46881902e-01, ...,\n",
       "          8.65044232e-01, -7.81987219e-01, -1.96716660e-01],\n",
       "        [-5.28569237e-01,  1.04870603e-01, -4.45052089e-02, ...,\n",
       "         -2.17546683e-01, -2.47620555e-01, -3.71779716e-01]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(4, 10, 30, 5)\n",
    "X.mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f646b7-f913-46a2-a4c5-1efeb59736cc",
   "metadata": {},
   "source": [
    "<p style='color:red'> Handling Long Sequences</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
