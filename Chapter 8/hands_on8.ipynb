{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5793e2fc-3995-4bcc-9a38-b0285088a63c",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Dimensionality Reduction</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Redução de Dimensionalidade, como seu nome sugere, consiste em reduzir o número de dimensões do nosso dataset. No caso, estaríamos excluindo ou condensando features.\n",
    "        </li>\n",
    "        <li> \n",
    "            No caso do dataset MNIST, por exemplo, os pixels localizados nas bordas das imagens poderiam ser facilmente removidos, já que são todos brancos para a maioria das instâncias. Seria interessante também condensarmos pixels vizinhos em um único só extraindo a média de suas intensidades.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854aa90d-1b6a-4b00-af0c-98fc1396345c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> Mas por que reduzir as dimensões?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A necessidade de se diminuir as proporções de nossos dados surge da lentidão em treino proporcionada por datasets volumosos. Para a maior parte das empresas, a rapidez dos modelos (juntamente, é claro, com sua qualidade) é algo altamente demandado. \n",
    "        </li>\n",
    "        <li> \n",
    "            Outra vantagem de se fazer essa tarefa é tornar possível a criação de gráficos sobre os dados. Conseguir ter um dataset de 2 a 5 dimensões torna possível a criação de scatter plots (eixo x, y, z, tamanho do scatter, colorbar).\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: Para grande parte dos casos, reduzir a dimensionalidade dos dados implica um tradeoff entre rapidez de treinamento e qualidade dos algoritmos.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b029fe1-1d2e-447e-988c-77f91f68959f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> The Curse of Dimesionality</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor expõe nesta seção o fato de que aumentar o número de dimensões do dataset torna mais provável o risco de overfitting. Isso porque, em um espaço multidimensional, a distância média entre dois pontos aleatoriamente escolhidos tende a ser assustadoramente grande.  \n",
    "        </li>\n",
    "        <li> \n",
    "            Com os dados esparsos, encontrar semelhanças entre eles se torna uma tarefa muito mais difícil. Para fazer uma previsão, os modelos terão poucas instâncias em suas mãos para realizarem as devidas comparações. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0af11-b865-45d3-8710-428cc12e85b2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> Main Approaches for Dimensionality Reduction</h2>\n",
    "<h3 style='font-size:30px;font-style:italic'> Projection</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6ad6e-1d50-4648-a410-bb8028ee4f6e",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Projeção consiste em encontrar um sub-espaço de menores dimensões dentro de um espaço multidimensional no qual projetamos as instâncias de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Projeção em um espaço 3D</h1>\n",
    "</center>\n",
    "<div>\n",
    "    <img src='projection1.png' width='350px'> \n",
    "    <span style='font-size:30px'> &rarr;</span>\n",
    "    <img src='projection2.png' width='300px'>\n",
    "</div>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Projeções nem sempre são interessantes. No caso de datasets como o da roleta suíça, as instâncias podem acabar se empilhando, tornando a previsão muito mais difícil.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b628222-de9b-4c18-ae25-f704f2c63737",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Manifold Learning</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Manifold Learing tem como suposição básica a de que o dataset com o qual estamos lidando surgiu da torção de um sub-espaço de dimensões muito menores - o Manifold. Tendo isso em vista, as técnicas de Manifold Learning buscam encontrar esse espaço. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='manifold1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Observe podemos entender a roleta suíça como uma dobra em três dimensões de um manifold 2-D.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7de277-b6f4-43f8-95ff-298560773f2b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> PCA</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            PCA é um dos algoritmos de redução de dimensionalidade mais populares atualmente. Ele tenta achar o hiperplano que se encontra mais próximo das instâncias de treino (menor distância ao quadrado média).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92d55c-6d42-4ed3-adb3-0448842985f7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preserving the Variance</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O algoritmo PCA escolhe como eixos aqueles que preservam a maior variância possível dos dados. Dessa maneira, o mínimo possível de informações é perdido.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='pca1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845cd6e-e851-4d82-bd09-150f8d25549d",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Na imagem, o eixo a ser escolhido seria o da linha contínua, pois preserva a maior variância entre as opções. Se quiséssemos dar uma segunda dimensão ao sub-espaço, escolheríamos a linha pontilhada, pois ela é ortogonal ao primeiro eixo e preserva grande parte da variância restante. Cada um dos eixos selecionados pelo PCA são denominados como Principal Components.\n",
    "        </li>\n",
    "        <li> \n",
    "           Se quiséssemos reduzir a dimensionalidade do dataset para duas dimensões, escolheríamos os dois primeiros Principal Components do PCA.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f0581-dcaf-4364-8114-d6ebf5b5bbce",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Principal Components</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor aqui adverte que o PCA é bastante sensível a modificações no set de treino. Por isso, é necessário ter bastante cuidado ao se tratar as informações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6463e03a-3639-4c3c-86f5-ad8b8e0a9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando PCA manualmente pelo Python.\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "# Vamos reduzir a dimensão do dataset para 2.\n",
    "X,y = make_regression(n_features=3, noise=50, bias=2)\n",
    "\n",
    "# Antes de qualquer coisa, temos que centralizar os dados.\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Extraindo os Principal Components de 'X'.\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "W2 = Vt.T[:, :2]\n",
    "\n",
    "# Reduzindo a dimensionalidade do dataset.\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb9c327b-60a9-404d-9a27-e11d61fb585e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2388.6212670387617, 2458.570542740498)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veja: não necessariamente o PCA vai aumentar a performance do modelo!\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mean_squared_error(y,LinearRegression().fit(X, y).predict(X)), mean_squared_error(y,LinearRegression().fit(X2D, y).predict(X2D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525359dc-8402-49d1-8579-b04af653d91b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using Scikit-Learn</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Scikit-learn, no módulo decomposition, possui a classe PCA. Essa é capaz de fazer as mesmas transformações realizadas nas últimas células.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b57c983-55d2-4065-ae9a-0e9b6cbb48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduzindo a dimensionalidade de 'X' para 2.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Os dados são automaticamente centralizados pelo objeto (mas não normalizados!)\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb86f725-d650-491e-a3f5-fb362f21b610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12050085,  0.62671072,  0.7698787 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acessando os Princpal Components com o atributo 'components_'.\n",
    "\n",
    "# Primeiro Principal Component.\n",
    "pca.components_[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08a58f-48ab-4fdf-b050-52e826d53610",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Explained Variance Ratio</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Outro atributo de extrema relevância é o 'explained_variance_ratio_'. Ele nos revela a proporção da variância do dataset que se encontra sobre o eixo de cada Principal Component.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23e0a256-50ac-4dd0-9fcf-05eb533edfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37644482, 0.33293361])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O primeiro PC leva 37.6% da variância, enquanto o outro 33.3%.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b477ec4-7109-4e28-92a9-5b42c0163486",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Choosing the Right Number of Dimensions</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O melhor número de dimensões para o nosso dataset é aquele que preserva a maior parte da sua variância. Para isso, existem duas estratégias para obtermos esse valor, cada uma com a sua vantagem.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59e67833-a49f-45ec-bd55-f6b44d0a75c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escolhendo um dataset com mais dimensões.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = make_regression(n_features=50, noise=50, bias=2)\n",
    "\n",
    "# Há 30 dimensões em X. Qual seria o número de dimensões que preservaria 95% da variância?\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9548946-60cd-4082-895e-7b962dc3e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos especificar a 'explained_variance_ratio' desejado em 'n_components_'\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2597ac9-db06-48fb-b2cf-78a45c293da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ou também conseguimos obter o valor exato de dimensões com o código abaixo.\n",
    "# Sua vantagem é de que podemos usar 'cumsum' para plotar um gráfico com a variância preservada em função do número de dimensões.\n",
    "\n",
    "\n",
    "# Invocando PCA sem um 'n_components' específico.\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Obtendo o número de dimensões desejado com np.where.\n",
    "d = np.argmax(cumsum>=0.95)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba515538-6615-4392-af6a-3241c0847a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4892634dc0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Montando o gráfico.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i+1 for i in range(len(cumsum))], cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f41aca4f-e0ee-48ab-8bd5-60a51256aa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05796646, 0.10872648, 0.1557608 , 0.20222471, 0.24703052,\n",
       "       0.28764546, 0.32752354, 0.36314469, 0.39810362, 0.43100949,\n",
       "       0.46356074, 0.4952065 , 0.52440906, 0.55326346, 0.58103072,\n",
       "       0.60808503, 0.63192519, 0.65506843, 0.67739336, 0.69938125,\n",
       "       0.72020989, 0.74052511, 0.75910358, 0.77742456, 0.79517274,\n",
       "       0.81093178, 0.82543563, 0.83977693, 0.85243102, 0.86489189,\n",
       "       0.87725104, 0.8888001 , 0.89928961, 0.90929897, 0.91860425,\n",
       "       0.92726472, 0.93574936, 0.94371196, 0.95122297, 0.95825319,\n",
       "       0.96500821, 0.9709281 , 0.97614064, 0.98115453, 0.98518184,\n",
       "       0.98908186, 0.99245225, 0.99530368, 0.99785086, 1.        ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3ca19-ed79-418d-af65-36ec334f7f4b",
   "metadata": {},
   "source": [
    "<p style='color:red'>Using Sckit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
