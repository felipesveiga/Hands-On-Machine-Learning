{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5793e2fc-3995-4bcc-9a38-b0285088a63c",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Dimensionality Reduction</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Redução de Dimensionalidade, como seu nome sugere, consiste em reduzir o número de dimensões do nosso dataset. No caso, estaríamos excluindo ou condensando features.\n",
    "        </li>\n",
    "        <li> \n",
    "            No caso do dataset MNIST, por exemplo, os pixels localizados nas bordas das imagens poderiam ser facilmente removidos, já que são todos brancos para a maioria das instâncias. Seria interessante também condensarmos pixels vizinhos em um único só extraindo a média de suas intensidades.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854aa90d-1b6a-4b00-af0c-98fc1396345c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> Mas por que reduzir as dimensões?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A necessidade de se diminuir as proporções de nossos dados surge da lentidão em treino proporcionada por datasets volumosos. Para a maior parte das empresas, a rapidez dos modelos (juntamente, é claro, com sua qualidade) é algo altamente demandado. \n",
    "        </li>\n",
    "        <li> \n",
    "            Outra vantagem de se fazer essa tarefa é tornar possível a criação de gráficos sobre os dados. Conseguir ter um dataset de 2 a 5 dimensões torna possível a criação de scatter plots (eixo x, y, z, tamanho do scatter, colorbar).\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: Para grande parte dos casos, reduzir a dimensionalidade dos dados implica um tradeoff entre rapidez de treinamento e qualidade dos algoritmos.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b029fe1-1d2e-447e-988c-77f91f68959f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> The Curse of Dimesionality</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor expõe nesta seção o fato de que aumentar o número de dimensões do dataset torna mais provável o risco de overfitting. Isso porque, em um espaço multidimensional, a distância média entre dois pontos aleatoriamente escolhidos tende a ser assustadoramente grande.  \n",
    "        </li>\n",
    "        <li> \n",
    "            Com os dados esparsos, encontrar semelhanças entre eles se torna uma tarefa muito mais difícil. Para fazer uma previsão, os modelos terão poucas instâncias em suas mãos para realizarem as devidas comparações. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0af11-b865-45d3-8710-428cc12e85b2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> Main Approaches for Dimensionality Reduction</h2>\n",
    "<h3 style='font-size:30px;font-style:italic'> Projection</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6ad6e-1d50-4648-a410-bb8028ee4f6e",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Projeção consiste em encontrar um sub-espaço de menores dimensões dentro de um espaço multidimensional no qual projetamos as instâncias de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Projeção em um espaço 3D</h1>\n",
    "</center>\n",
    "<div>\n",
    "    <img src='projection1.png' width='350px'> \n",
    "    <span style='font-size:30px'> &rarr;</span>\n",
    "    <img src='projection2.png' width='300px'>\n",
    "</div>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Projeções nem sempre são interessantes. No caso de datasets como o da roleta suíça, as instâncias podem acabar se empilhando, tornando a previsão muito mais difícil.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b628222-de9b-4c18-ae25-f704f2c63737",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Manifold Learning</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Manifold Learing tem como suposição básica a de que o dataset com o qual estamos lidando surgiu da torção de um sub-espaço de dimensões muito menores - o Manifold. Tendo isso em vista, as técnicas de Manifold Learning buscam encontrar esse espaço. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='manifold1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Observe podemos entender a roleta suíça como uma dobra em três dimensões de um manifold 2-D.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7de277-b6f4-43f8-95ff-298560773f2b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px;'> PCA</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            PCA é um dos algoritmos de redução de dimensionalidade mais populares atualmente. Ele tenta achar o hiperplano que se encontra mais próximo das instâncias de treino (menor distância ao quadrado média).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92d55c-6d42-4ed3-adb3-0448842985f7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preserving the Variance</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O algoritmo PCA escolhe como eixos aqueles que preservam a maior variância possível dos dados. Dessa maneira, o mínimo possível de informações é perdido.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='pca1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845cd6e-e851-4d82-bd09-150f8d25549d",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Na imagem, o eixo a ser escolhido seria o da linha contínua, pois preserva a maior variância entre as opções. Se quiséssemos dar uma segunda dimensão ao sub-espaço, escolheríamos a linha pontilhada, pois ela é ortogonal ao primeiro eixo e preserva grande parte da variância restante. Cada um dos eixos selecionados pelo PCA são denominados como Principal Components.\n",
    "        </li>\n",
    "        <li> \n",
    "           Se quiséssemos reduzir a dimensionalidade do dataset para duas dimensões, escolheríamos os dois primeiros Principal Components do PCA.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f0581-dcaf-4364-8114-d6ebf5b5bbce",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Principal Components</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor aqui adverte que o PCA é bastante sensível a modificações no set de treino. Por isso, é necessário ter bastante cuidado ao tratar as informações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6463e03a-3639-4c3c-86f5-ad8b8e0a9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando PCA manualmente pelo Python.\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "# Vamos reduzir a dimensão do dataset para 2.\n",
    "X,y = make_regression(n_features=3, noise=50, bias=2)\n",
    "\n",
    "# Antes de qualquer coisa, temos que centralizar os dados.\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Extraindo os Principal Components de 'X'.\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "W2 = Vt.T[:, :2]\n",
    "\n",
    "# Reduzindo a dimensionalidade do dataset.\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb9c327b-60a9-404d-9a27-e11d61fb585e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2388.6212670387617, 2458.570542740498)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veja: não necessariamente o PCA vai aumentar a performance do modelo!\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "mean_squared_error(y,LinearRegression().fit(X, y).predict(X)), mean_squared_error(y,LinearRegression().fit(X2D, y).predict(X2D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3ca19-ed79-418d-af65-36ec334f7f4b",
   "metadata": {},
   "source": [
    "<p style='color:red'>Using Sckit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
