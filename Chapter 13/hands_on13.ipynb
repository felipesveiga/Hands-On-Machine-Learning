{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce351e58-b23d-400f-aa5b-30744b6635fd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Loading and Preprocessing Data with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af312d-0fae-4ee5-9faf-4924dfac7c55",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Data API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Data API é um módulo do Tensor Flow voltado ao tratamento de datasets volumosos. É interessante ser usado quando os dados não cabem na memória RAM ou placa de vídeo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A classe `Dataset` é onde nossos dados são armazenados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ed8807-7b25-4b0f-be81-b238d61fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Criando um `Dataset` a partir de um `tf.range`. \n",
    "X = tf.range(10)\n",
    "\n",
    "# Cada número de `X` será encapsulado em um tensor.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cee74d4-a06f-4032-90cb-c3ea343d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo Dataset poderia ter sido montado da seguinte maneira:\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b337-72e7-41da-b9d5-0d3dd838a1ed",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chaining Transformations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As funções da classe `Dataset` nunca fazem transformações in-place; sempre retornam um novo objeto.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2a642-3990-4220-abf3-6b1c39c1a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#  `repeat` repetirá os dados do dataset, enquanto `batch` vai criar batches com n instâncias.\n",
    "for item in dataset.repeat(3).batch(7):\n",
    "    # Para evitar que o último batch com os elementos restantes do dataset seja formado, passe `drop_remainder`=True.\n",
    "    print(item)\n",
    "    \n",
    "# Observe que `batch` não embaralha os dados na hora de sua separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4d9175c-fd02-41c3-bf56-eb8040f40b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=81>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=256>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=625>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1296>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2401>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4096>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=6561>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `map` aplica uma função por elemento.\n",
    "# `apply` invoca uma transformação a todo o dataset.\n",
    "list(dataset.map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3656c0-270f-408f-88b3-268a244eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `filter`, é bastante eficaz em remover dados indesejados.\n",
    "list(dataset.filter(lambda x: x<5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06f35-e712-4ae4-bc0c-0287dc8c3379",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Shuffling the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `shuffle` embaralha as instâncias com a seguinte lógica: escolhe as primeiras x instâncias do dataset e as agrupa em um buffer. Daí, as mistura e sorteia um dado, quando solicitado. Após a extração, o buffer fica com um espaço sobrando, que é preenchido com a próxima instância do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por conta do algoritmo, é contraindicado usar `buffer_size`'s pequenos. A documentação do TF indica, até mesmo, designarmos um valor maior ou igual ao o dataset. Apenas cuidado com a memória RAM!            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af9184b-6aff-456d-a286-a94332ccc090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando um dataset artificial (`buffer_size`=10).\n",
    "\n",
    "# `reshuffle_each_iteration` garante que um novo embaralhamento dos dados ocorrerá, caso usemos `repeat`, por exemplo.\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "list(dataset.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f4d4-850f-402d-a193-431fd6db927b",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Interleaving lines from multiple files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A fim de garantirmos a ordem aleatória dos dados no abastecimento de modelos, talvez seja interessante quebrarmos o set de treino em múltiplos arquivos. Assim, os lemos simultaneamente, intercalando as suas linhas. No final, conseguimos ainda aplicar a função `shuffle` para misturar todo o produto da leitura.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2567bf-4b2a-4803-b0a1-146c07bdb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde vão ser armazenados os dados.\n",
    "from os import mkdir\n",
    "mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0c18f6c2-3711-49d5-8a57-77e25778fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "X,y = fetch_california_housing(return_X_y=True,as_frame=True)\n",
    "data = pd.merge(X,y, left_index=True, right_index=True)\n",
    "\n",
    "for i in range(0, len(data), 1000): # Forma rápida de gerar os arquivos, mesmo sabendo que vou perder algumas linhas.\n",
    "    data.iloc[i:i+1000].to_csv(f'data/housing_{int(i/1000)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee652eb7-00fd-4bc4-8315-67ca8b522f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_4.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_14.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_5.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_1.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_19.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_2.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_20.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_11.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_10.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_12.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_7.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_0.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_8.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_18.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_15.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_3.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_6.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_16.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_13.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_9.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_17.csv'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método `list_files` localiza um conjunto de arquivos baseado em certo padrão nas strings.\n",
    "# Retorna um dataset com os paths de todos os arquivos que estão de acordo com a string oferecida.\n",
    "filepath_dataset = tf.data.Dataset.list_files('data/housing_*.csv', shuffle=True)\n",
    "list(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f430140-951b-464e-b870-7a62025b989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A leitura dinâmica dos arquivos pode ser feita com o método `interleave`.\n",
    "\n",
    "# Esse aplica uma função sobre todo o `Dataset` e intercala as suas linhas.\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Por padrão, não há uso de paralelismo. Uma linha de cada dataset é lida por vez.\n",
    "    # Use `num_parallel_calls` se quiser fazer esse processo por multithreading.\n",
    "    num_parallel_calls=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b52979c-0030-40b4-9668-604c7c8b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'12000,7.5408,3.0,8.493150684931507,1.0342465753424657,519.0,3.5547945205479454,33.93,-117.57,2.719'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'16000,4.0846,52.0,4.821316614420063,0.9561128526645768,819.0,2.5673981191222572,37.74,-122.47,3.336'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3000,4.1484,10.0,4.791907514450867,0.8439306358381503,447.0,2.5838150289017343,35.3,-119.03,1.029'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'11000,13.466,26.0,8.874233128834355,1.0582822085889572,983.0,3.0153374233128836,33.75,-117.79,5.00001'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'9000,4.0,49.0,6.866295264623956,1.0362116991643453,1018.0,2.8356545961002784,34.0,-118.34,2.968'>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja: os valores das features estão todos contidos dentro de uma string bytes.\n",
    "\n",
    "# É necessário ainda fazermos mais um tratamento, coletando os números individualizados.\n",
    "list(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f9f90-e661-4e56-8d3b-05edf430febf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preprocessing the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Criaremos uma função que recebe uma linha do dataset e a retorna devidamente tratada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8226674f-cc32-4523-bc0c-e95db705394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios-padrão das colunas para padronização.\n",
    "X_mean, X_std = X.mean().to_numpy(), X.std().to_numpy()\n",
    "\n",
    "# Função de tratamento dos dados.\n",
    "def preprocess(line):\n",
    "    n_inputs = 8 # Quantidade de colunas presentes.\n",
    "    # Lista com os valores-padrão a serem imputados nas células caso haja NaN's. Um tensor vazio será gerado caso \n",
    "    # o nulo ocorra em uma target-variable, acarretando em um erro.\n",
    "    default_values = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    # Decodificando a linha. 'default_values' será usado na ocorrência de vazios.\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_values) # `fields é uma lista de tensores que contêm um único número.\n",
    "    # A função `stack` empilha os valores dos tensores em um único tensor unidimensional.\n",
    "    x, y = tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "    # Retornando as variáveis independentes padronizadas juntamente com as dependentes.\n",
    "    return (x-X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "755c26d6-3313-4608-b03d-b11cf5a64473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ -0.8635921 ,  -2.1381242 ,   2.2435777 ,   7.3079667 ,\n",
       "         -1.250785  ,  -0.29468906, -16.66791   ,  69.81657   ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.41], dtype=float32)>)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.23,1.73,10.98,4.56,9.01,0.01,0.03,20.31,5.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b196e-53cb-49c9-9b3c-3ee35055b2b1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Putting Everything Together</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Prefetching</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `prefetch` contribui para a performance de nosso programa. Enquanto um dado é processado, o método faz com que a próxima instância (ou batch) passe pelo tratamento em paralelo, economizando tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdae2d-e011-4272-9b99-d428003033f5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='prefetch.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "46c443c4-05ca-4a8c-b21b-20dc543eb3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preparando a próxima instância proveniente de 'range_dataset'.\n",
    "range_dataset.prefetch(1)\n",
    "\n",
    "# Preparando o próximo batch criado a partir de 'range_dataset'.\n",
    "range_dataset.repeat(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3178bc10-ea30-4a4a-a397-9e966232ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a seção com uma função  que realiza todos os passos demonstrados.\n",
    "def csv_reader_dataset(filepath:str, cycle_length:int=5, buffer_size:int=1000, repeat:int=3, batch_size:int=32):\n",
    "    filepaths = tf.data.Dataset.list_files(filepath) # Coletando todos os arquivos correspondentes ao padrão `filepath`.\n",
    "    # Lendo `cycle_length` arquivos, pulando as suas primeiras linhas. \n",
    "    dataset = filepaths.interleave(lambda x: tf.data.TextLineDataset(x).skip(1), \n",
    "                                   cycle_length=cycle_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache() # Tratando os dados e os armazenando em cache.\n",
    "    dataset = dataset.shuffle(buffer_size).repeat(repeat) # Embaralhando e repetindo os dados.\n",
    "    # A função segregará os dados em batches. À cada batch sendo processado, o próximo é criado.\n",
    "    return dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25104e2-5397-41bb-b85c-71b0f2ae09b7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Usamos `cache` após o tratamento, mas antes do shuffling e batching. Isso garantirá que a mistura e batches serão distintos a cada iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "12922ca9-ce52-4ba6-9971-5fdcaac4c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste da função.\n",
    "csv_reader_dataset('data/housing_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df714-107c-4d8f-aaba-316cb0996ea0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using the Dataset with tf.keras</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Com a Pipeline montada, é hora de criar uma rede neural com os dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a1a634c8-f83d-4652-ac2a-09f1a4d6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos de treino, validação e teste.\n",
    "train_paths = [f'data/housing_{i}.csv' for i in range(12)]\n",
    "valid_paths = [f'data/housing_{i}.csv' for i in range(12, 16)]\n",
    "test_paths = [f'data/housing_{i}.csv' for i in range(16, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "bd3be161-ec1b-40fa-95cb-727dd87717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets com os dados.\n",
    "train_set = csv_reader_dataset(train_paths)\n",
    "valid_set = csv_reader_dataset(valid_paths)\n",
    "test_set = csv_reader_dataset(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c9b6a9fc-3d0b-456b-859e-ecca72d70455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hora demontar o modelo.\n",
    "def data_api_model():\n",
    "    model = keras.models.Sequential()\n",
    "    # Camada de Input.\n",
    "    model.add(keras.layers.Input(shape=[8]))\n",
    "    # Hidden Layers.\n",
    "    for _ in range(5): \n",
    "        model.add(keras.layers.Dense(40, activation='elu', kernel_initializer='lecun_normal'))\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "8087f5e9-2c91-4643-a97e-6ae15049aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 10s 6ms/step - loss: 0.4308 - val_loss: 0.7267\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.3109 - val_loss: 0.6991\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2969 - val_loss: 0.6759\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2858 - val_loss: 0.8068\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2782 - val_loss: 0.5866\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2722 - val_loss: 0.6028\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2639 - val_loss: 0.5513\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2608 - val_loss: 0.5665\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2563 - val_loss: 0.5633\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2543 - val_loss: 0.5831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c15dcda60>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = data_api_model()\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "model.fit(train_set, batch_size=20, epochs=10, validation_data=valid_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b39dd-6e8f-440d-a8df-469408e090d7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> The TFRecord Format</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O formato de arquivo TFRecord é conhecido pela sua flexibilidade. É capaz de armazenar diferentes tipos de dados, como textos e áudios.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "39c8b714-1df9-400b-8739-08033d98fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pequeno arquivo TFRecord\n",
    "with tf.io.TFRecordWriter('my_tfrecord.tfrecord') as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "417543aa-1f54-4716-aab2-ed49c3430e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Row', shape=(), dtype=string)\n",
      "tf.Tensor(b'Second Row', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Agora, lendo o arquivo.\n",
    "file = tf.data.TFRecordDataset('my_tfrecord.tfrecord')\n",
    "\n",
    "for item in file:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a7f6b-2ac2-46db-9c4a-a7e60fbce906",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Compressed TFRecord Files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Conseguimos usar métodos de compressão nos arquivos .tfrecord.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3fdba58a-df44-4df0-b039-5f74b6ba5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_tfrecord2.tfrecord', options=options) as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2aa7c56e-95f4-471f-9255-b33c13fb6614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ao carregar esse mesmo arquivo, você deve informar o método que o comprimiu.\n",
    "tf.data.TFRecordDataset('my_tfrecord2.tfrecord', compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163b4d7-328c-4908-9f01-f85895d70389",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> A Brief Introduction to Protocol Buffers</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O protocol buffer (protobuf) é uma espécie de formatação de arquivos desenvolvida pelo Google. Sua sintaxe é similar com a do JSON, mas é muito mais rápida de ser lida, pois seus registros são armazenados em bytes.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nos arquivos TFRecord, os dados pertencentes às instâncias são armazenados no formato de protobufs,\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ac876-f0ba-4f33-bd1d-f0a1e2ec43ad",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> TensorFlow Protobufs</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Somos capazes de construir os nossos próprios protobufs com o módulo `tf.train`. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "771be6e6-b7ad-4fd1-b8f2-af75a101c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"emails\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"email@gmail.com\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"name\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"Alice\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A classe `Example` representa uma única instância do dataset. Essa contém o argumento 'features', que admite uma classe `Features`.\n",
    "# Essa, por sua vez, tem o campo 'feature', que recebe um dicionário <nome_feature>:Feature(ClasseDataType(valor_feature)).\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Feature, Features\n",
    "\n",
    "# Montando um protobuf-exemplo.\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'id':Feature(int64_list=Int64List(value=[1])),\n",
    "            'name':Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'emails':Feature(bytes_list=BytesList(value=[b'email@gmail.com']))\n",
    "    }))\n",
    "person_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "85f1ed25-988d-494a-bb74-0c6b6375329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos registrar a instância em um arquivo .tfrecord.\n",
    "with tf.io.TFRecordWriter('my_contacts_record.tfrecord') as f:    \n",
    "    f.write(person_example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bf366-8335-4b80-9021-7b859e61525f",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Loading and Parsing Examples</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Para carregar o protobuf de um exemplo, use a função `parse_single_example` dentro de um loop envolvendo a classe `TFRecordDataset`.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3c61eee2-7e7e-4769-b73c-19de8c84ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método requer um dicionário que mapeia o nome da feature para uma classe que sinaliza informações como shape, datatype e default_value.\n",
    "\n",
    "# 'id' e 'name' sempre serão listas de um único elemento. Por isso, a classe `FixedLenFeature` é usada.\n",
    "# Por outro lado, a lista de 'emails' admite qualquer quantidade de strings. Dessa forma, recorremos à VarLenFeature, que exige apenas\n",
    "# que o data type seja informado.\n",
    "description = {\n",
    "        'id':tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        'name':tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'emails':tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "# Lendo o `Example` do arquivo há pouco gerado.\n",
    "for serialized_example in tf.data.TFRecordDataset('my_contacts_record.tfrecord'):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e4a7b8c2-aec7-4d28-91ac-ebba26a5f8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f9bbc10b6a0>,\n",
       " 'id': <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `parsed_example` é um dicionário <feature>:<Tensor_feature>.\n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "cefed637-5425-4ba1-b9e7-3e3e3afb5545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'email@gmail.com'], dtype=object)>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que a feature informada como de tamanho variável é retornada como um tensor esparso. \n",
    "\n",
    "# Podemos ler os seus valores tanto com `tf.sparse.to_dense`, quanto usando o atributo 'values' do tensor.\n",
    "parsed_example['emails'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01a95c-9dc2-4f6a-9aa0-096705d69c16",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Mas é claro que não leríamos uma única instância do dataset em projetos reais. Use  `parse_example` para ler múltiplas linhas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e5cf2755-6531-4c58-837d-8483c366952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para exemplificar o uso, vamos recorrer ao `fetch_california_housing`, já que esse possui bastantes linhas.\n",
    "data = pd.read_csv('data/housing_0.csv')\n",
    "\n",
    "# Pequena função para a criação de uma classe `Example` para cada linha de 'data'.\n",
    "def create_example(line, columns=data.columns):\n",
    "    example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                column:Feature(float_list=FloatList(value=[line[i]]))\n",
    "                  for i, column in enumerate(columns)          \n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "94facdf9-4453-439c-bfdc-abd502bc8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando os dados em um arquivo 'tf.record'.\n",
    "with tf.io.TFRecordWriter('housing_tf.tfrecord') as f:\n",
    "    for row in data.values:\n",
    "        example = create_example(row)\n",
    "        f.write(example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "bb6147bd-4d81-4598-a898-02e28db29b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oferecendo os data types de cada coluna para a leitura do arquivo produzido.\n",
    "description = {\n",
    "    col:tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "    for col in X.columns\n",
    "}\n",
    "\n",
    "# Ao invés de lermos instância por instância, usamos `parse_example` para ler batches de 10 dados e embaralhá-los.\n",
    "for data in tf.data.TFRecordDataset('housing_tf.tfrecord').batch(10).shuffle(10, seed=42):\n",
    "    parsed_examples = tf.io.parse_example(data, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9a826fc1-8690-4f13-bf1f-6a5eedc8794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AveBedrms': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([1.0095367 , 1.0135417 , 1.0440421 , 0.9763113 , 1.0832196 ,\n",
       "        1.0850723 , 0.9019608 , 0.97894734, 0.97792023, 1.1014493 ],\n",
       "       dtype=float32)>,\n",
       " 'AveOccup': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([3.2043598, 2.9739583, 3.297217 , 3.3519459, 2.43588  , 3.165329 ,\n",
       "        3.0952382, 3.630263 , 3.0762107, 3.3064182], dtype=float32)>,\n",
       " 'AveRooms': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([5.737057 , 5.7625   , 7.635234 , 8.30626  , 5.075716 , 6.9245586,\n",
       "        5.647059 , 7.075    , 6.5292025, 6.3913045], dtype=float32)>,\n",
       " 'HouseAge': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([26., 27.,  7.,  9., 10., 23., 26., 13., 22., 26.], dtype=float32)>,\n",
       " 'Latitude': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([37.52, 37.53, 37.53, 37.49, 37.49, 37.48, 37.47, 37.47, 37.55,\n",
       "        37.55], dtype=float32)>,\n",
       " 'Longitude': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([-121.96, -121.93, -121.92, -121.89, -121.92, -121.92, -121.92,\n",
       "        -121.91, -122.03, -122.03], dtype=float32)>,\n",
       " 'MedInc': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([5.2396, 4.7478, 8.4045, 9.7194, 5.1643, 5.3813, 6.0878, 6.868 ,\n",
       "        6.992 , 4.9118], dtype=float32)>,\n",
       " 'Population': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 2352.,  2855., 12203.,  1981.,  3571.,  1972.,  1105.,  2759.,\n",
       "         4319.,  1597.], dtype=float32)>}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4540c-2c3b-400b-ab4a-a1f573de936a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Preprocessing the Input Features</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Vamos aprender aqui a implantar algumas técnicas de preprocessamento de dados pelo TensorFlow.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "30bb178a-29e8-4c63-b03f-0a8bdfdd3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20640, 8), dtype=float64, numpy=\n",
       "array([[ 0.01343958,  0.00912272,  0.00814819, ...,  0.00164245,\n",
       "         0.00738648, -0.00711444],\n",
       "       [ 0.01340116,  0.00467261,  0.00727786, ...,  0.00135599,\n",
       "         0.00738258, -0.00711386],\n",
       "       [ 0.0117158 ,  0.01157028,  0.00966954, ...,  0.00180101,\n",
       "         0.00738063, -0.00711502],\n",
       "       ...,\n",
       "       [ 0.00274435,  0.00378259,  0.00607316, ...,  0.00149468,\n",
       "         0.00768873, -0.00705565],\n",
       "       [ 0.00301427,  0.0040051 ,  0.0062178 , ...,  0.00136458,\n",
       "         0.00768873, -0.00706148],\n",
       "       [ 0.00385598,  0.00356009,  0.00613053, ...,  0.00168193,\n",
       "         0.00767703, -0.00705682]])>"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O livro ensina a fazer uma Padronização, mas achei que faria mais sentido montar uma camada de Normalização.\n",
    "class NormLayer(keras.layers.Layer):\n",
    "    # Chame `adapt` para que as normas do seu dataset sejam mensuradas.\n",
    "    def adapt(self, input_data):\n",
    "        # Normas das features são armazenadas em `self.norms`.\n",
    "        self.norms = tf.linalg.norm(input_data, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Normalizando os dados.\n",
    "        return inputs / self.norms\n",
    "    \n",
    "norm_layer = NormLayer()\n",
    "# No Keras, `adapt` tem o mesmo papel que o `fit` dos transformadores do scikit-learn!\n",
    "norm_layer.adapt(X)\n",
    "norm_layer.call(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5a01a-f4ab-460f-a0b1-a54d2c0fad7b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoding Categorical Features Using One-Hot Vectors</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Como visto no Capítulo 2, os vetores One-Hot indicam a presença ou não de uma determinada categoria no dataset. O TensorFlow possui mecanismos robustos para lidar com esse tipo de tratamento.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "7af87bc1-aecf-4f47-aea2-cba1f85dc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como ilustração, usaremos as categorias presentes no 'housing.csv'.\n",
    "df = pd.read_csv('housing.csv')\n",
    "vocab = df['ocean_proximity'].unique() # Listando as categorias.\n",
    "indices = tf.range(len(vocab), dtype=tf.int64) # Vetor com o código de cada categoria.\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices) # Inicializador da tabela de Lookup.\n",
    "num_oov_buckets = 2 # A tabela de Lookup admite até duas novas categorias.\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d42dec-7618-4ca4-973e-3aea93499c0c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Tabela de Lookup terá duas colunas extras para categorias ausentes no vocabulário informado. Uma vez que um termo desse tipo é encontrado, ele é designado a uma coluna, passando essa a pertencê-la.\n",
    "        </li>\n",
    "        <li> \n",
    "            Se a quantidade de categorias desconhecidas exceder 'num_oov_buckets', termos distintos serão designados a um mesmo bucket.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "627cfe3f-caad-42ee-a31e-7ecca8bfd17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([2, 0, 2, 3, 3, 6])>"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando a tabela de Lookup.\n",
    "categories = np.random.choice(vocab, size=5)\n",
    "categories = tf.constant(np.append(categories, 'New Cat')) # Acrescentando uma categoria ausente no vocabulário.\n",
    "\n",
    "# Veja que 'New Cat' assume o índice 6.\n",
    "table.lookup(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "d966ef76-d5d5-4427-9225-77f8b1ec1cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O output do método `lookup` pode ser aproveitado para fazer o One-Hot Encoding.\n",
    "# Precismamos informar em `depth` a quantidade de categorias esperadas.\n",
    "tf.one_hot(table.lookup(categories), depth=len(vocab) + num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3503e-983c-4538-a6c3-4d37c2405a3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoding Categorical Features Using Embeddings</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Mas como já sabemos, One-Hot Encoding pode ser uma má escolha em datasets com múltiplas categorias. Uma alternativa a isso é gerar um vetor de números aleatórios para cada feature. Esses valores se tornariam treináveis, assim como qualquer neurônio da Neural Network.\n",
    "        </li>\n",
    "        <li> \n",
    "            Os vetores admitem qualquer dimensão. Por exemplo, \"INLAND\" poderia receber $\\begin{bmatrix} 0.699 \\\\ 1.781 \\end{bmatrix}$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a60f0-ef96-4285-bb52-91036205eb48",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Curiosidades sobre Word Embedding</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Em pesquisas do Google, constatou-se que Word Embeddings podem assimilar propriedades semânticas das palavras que eles representam.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por exemplo, ao medirmos $King-Man+Woman$, as coordenadas do vetor resultante são muito próximas daquelas de $Queen$.\n",
    "        </li>\n",
    "        <li> \n",
    "            A fim de poupar processamento, é possível instalarmos Embeddings pré-treinados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "096055f1-f33d-4017-a74f-e2a45e0664b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.9441118 , 0.34810066],\n",
       "       [0.298087  , 0.62147415],\n",
       "       [0.71149886, 0.39507556],\n",
       "       [0.9049461 , 0.03807437],\n",
       "       [0.20489836, 0.7704935 ],\n",
       "       [0.6312777 , 0.83825445],\n",
       "       [0.2000736 , 0.21912193]], dtype=float32)>"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 2 # Desejamos que os embeddings tenham duas dimensões.\n",
    "# Criando a matriz inicial de embedding.\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim]) \n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "f0fd16eb-8318-48dd-ac3a-a1312bd7f650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
       "array([[0.20489836, 0.7704935 ],\n",
       "       [0.9441118 , 0.34810066],\n",
       "       [0.20489836, 0.7704935 ],\n",
       "       [0.20489836, 0.7704935 ],\n",
       "       [0.20489836, 0.7704935 ],\n",
       "       [0.2000736 , 0.21912193]], dtype=float32)>"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listando os índices correspondentes a 'categories'.\n",
    "cat_indices = table.lookup(categories)\n",
    "\n",
    "# Recortando 'embedding_matrix' pelos índices de 'cat_indices' (como se fosse um `iloc` do pandas).\n",
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "a9183870-40c9-4416-9712-0e9d2ec074a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
       "array([[-0.04930068, -0.01541485],\n",
       "       [-0.00944408, -0.02514664],\n",
       "       [-0.04930068, -0.01541485],\n",
       "       [-0.04930068, -0.01541485],\n",
       "       [-0.04930068, -0.01541485],\n",
       "       [-0.01599804, -0.04851791]], dtype=float32)>"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O Keras fornece uma camada capaz de fazer tudo o que foi apresentado nas últimas células. Ela inicializa uma matriz de números aleatórios\n",
    "# com as dimensões informadas.\n",
    "embedding = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim=embedding_dim)\n",
    "\n",
    "# Recortando a matriz, assim como fizemos anteriormente.\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "f29d8a0c-a041-43d6-9008-49f552f20bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando uma pequena Functional API com Embedding.\n",
    "regular_inputs = keras.layers.Input(shape=[8])\n",
    "categories_layer = keras.layers.Input(shape=[], dtype=tf.string) # Camada de categorias.\n",
    "cat_indices = keras.layers.Lambda(lambda x: table.lookup(x))(categories_layer) # As categorias vão ser passadas à camada Lambda para obterem\n",
    "                                                                                # seus índices\n",
    "# Criando a matriz de Embeddings (uma linha para cada expressão).\n",
    "cat_embed = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, output_dim=2)(cat_indices)\n",
    "\n",
    "# Concatenando camadas de categorias com a normal.\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "output = keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "# Montando o modelo.\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories_layer], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a205b-cb73-4fb8-9973-2bc6efcb11c2",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Dica: Pesquise sobre a `keras.layers.TextVectorization`. É um recurso bastante útil em projetos de NLP. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882bb3c-87b9-4044-bb24-11ecb8f8fde2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Keras Preprocessing Layers</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Os desenvolvedores do Keras estão buscando continuamente facilitar o pré-processamento dos nossos dados. Para isso, são criadas as preprocessing layers, que desempenham tarefas típicas do tratamento de dados.\n",
    "        </li>\n",
    "        <li> \n",
    "            É importante salientar que os coeficientes dessas camadas são congelados no treinamento, impossibilitando a extração de suas derivadas. Por isso, sempre as ponha no início do modelo para evitar que as hidden layers sejam comprometidas por esse protocolo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "583cce8e-222d-47f2-acee-9742d99ef0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=array([2, 4, 3, 3, 3, 5, 3, 4, 2, 4, 2, 4, 4, 3, 2, 5, 4, 4, 4, 2])>"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camada `Discretization` cria bins para variáveis contínuas, assim como as funções `cut` do pandas.\n",
    "\n",
    "# Montando um bin para cada intervalo entre os desvios-padrão de uma variável normal-padrão.\n",
    "discretization = keras.layers.Discretization(bin_boundaries=tf.linspace(-3, 3, 7))\n",
    "discretization(tf.random.normal([20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "696bd4bf-11ef-4f9d-aa83-734e269a980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para encadear múltiplas transformações, você pode montar uma camada customizada que contenha todas aquelas que você deseja.\n",
    "\n",
    "class PreProcess(keras.layers.experimental.preprocessing.PreprocessingLayer):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers # Lista de camadas de pré-processamento.\n",
    "        \n",
    "    def adapt(self, data):\n",
    "        '''\n",
    "            Método que adaptará todas as camadas da Pipeline.\n",
    "            \n",
    "            Inputs\n",
    "            -----\n",
    "            `data`: Os dados com os quais as camadas vão se adaptar.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            layer.adapt(data)\n",
    "        return self\n",
    "    \n",
    "    def call(self, data):\n",
    "        '''\n",
    "            Método de transformação dos dados.\n",
    "            \n",
    "            Inputs\n",
    "            -----\n",
    "            `data`: Os dados que serão modificados.\n",
    "        '''\n",
    "        Z = data\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "bf51c019-8848-449a-b24e-06139f889565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados aleatórios.\n",
    "tf_random = tf.random.normal(shape=(100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "cd4a516e-37a1-4e54-9487-bbdcd5bed53c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[ 2.258774  ,  1.8339821 ,  0.75372356,  0.9563567 ,  0.9718352 ],\n",
       "       [ 0.18391968, -0.15831836,  3.4170768 ,  4.404645  ,  0.9718352 ],\n",
       "       [ 1.2213469 ,  0.8378319 ,  1.641508  ,  3.2552156 ,  3.9396582 ],\n",
       "       [ 1.2213469 ,  3.8262825 ,  0.75372356,  4.404645  ,  3.9396582 ],\n",
       "       [ 2.258774  ,  1.8339821 , -0.13406093,  2.105786  ,  0.9718352 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOntando uma certa sequência de tranformações e adaptando-a.\n",
    "preprocess = PreProcess([keras.layers.Discretization(num_bins=5), \n",
    "                keras.layers.Normalization()])\n",
    "preprocess.adapt(tf_random)\n",
    "preprocess(tf_random)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "9e58d41b-d8e6-446c-b7b7-44c841da20c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[ 2.258774  ,  1.8339821 ,  0.75372356,  0.9563567 ,  0.9718352 ],\n",
       "       [ 0.18391968, -0.15831836,  3.4170768 ,  4.404645  ,  0.9718352 ],\n",
       "       [ 1.2213469 ,  0.8378319 ,  1.641508  ,  3.2552156 ,  3.9396582 ],\n",
       "       [ 1.2213469 ,  3.8262825 ,  0.75372356,  4.404645  ,  3.9396582 ],\n",
       "       [ 2.258774  ,  1.8339821 , -0.13406093,  2.105786  ,  0.9718352 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apenas por controle de qualidade, verificando se performar as modificações separadamente produz o mesmo resultado.\n",
    "dis = keras.layers.Discretization(num_bins=5)\n",
    "nor = keras.layers.Normalization()\n",
    "\n",
    "dis.adapt(tf_random)\n",
    "nor.adapt(tf_random)\n",
    "\n",
    "# Isso se confirma!\n",
    "nor(dis(tf_random))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8332847-37c8-460f-88e1-e262e470f331",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Codificando Textos Livres com Vetores de Contagem</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aurélien também menciona estratégias de numeralizarmos textos. A primeira delas seria criar um vetor com a contagem de vezes que uma palavra aparece no fragmento. Por exemplo, se tivermos treinado o modelo com as palavras ['amar', 'avestruz, 'e'], a frase 'amar e amar' seria codificada como $\\begin{bmatrix}2 \\ 0 \\ 1 \\end{bmatrix}$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c6465-3907-461d-b0b2-8196593dfcde",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> TF-IDF</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método apresentado acima, no entanto, é limitado. Isso porque preposições e conjunções, que possuem presença assegurada em todos os textos, terão uma quantidade maior de aparições do que termos mais significativos à análise.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por isso, podemos dividir a contagem de cada palavra pelo log do número de textos em que elas aparecem! Isso reduzirá a influência de palavras como \"e\", \"de\", \"por\", etc.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "1cca49bf-10ff-4c5f-a2cd-4fede52fd5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0.5112444, 0.       , 0.156325 ], dtype=float32)>"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso 'amar' tenha surgido apenas em 50 fragmentos; 'avestruz' em 10; e 'e' em 600, o vetor final ficaria como.\n",
    "tf.Variable([2. / tf.math.log(50.), 0./tf.math.log(10.), 1./tf.math.log(600.)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b19eab-d952-4702-a396-9c6d218aeef6",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> TF Transform</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            TF Transform faz parte da plataforma TensorFlow Extended (TFX), responsável por auxiliar o deploy de modelos.\n",
    "        </li>\n",
    "        <li> \n",
    "            No seu caso, tem o papel de contribuir para a portabilidade do código de pré-processamento, independentemente de aonde o modelo for aplicado (apps de celular, websites...)\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "5552b397-3c64-4747-ac1b-aa7b17dfa86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "\n",
    "# Pré-processando apenas duas features do 'housing.csv'\n",
    "def process(inputs):\n",
    "    median_age = inputs['housing_median_age']\n",
    "    ocean_proximity = inputs['ocean_proximity']\n",
    "    z_score_age = tft.scale_to_z_score(median_age) # Computando o z-score da mediana das idades.\n",
    "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "    \n",
    "    return {\n",
    "            'z_score_age':z_score_age,\n",
    "            'ocean_proximity_id':ocean_proximity_id\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30156e-6554-4c40-b55a-8ffde4b2a440",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The TensorFlow Datasets (TFDS) Project</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O módulo tensorflow-datasets disponibiliza diversos datasets para uso em Deep Learning. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "eff28c5e-790c-47e7-a445-ceb4e7cf6b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.load(name='mnist', as_supervised=True)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df85ce6-a1a7-41ec-a116-f4f1dab36aa1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Exercises</h2>\n",
    "<h3 style='font-size:30px;font-style:italic'> 9</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "25f588e3-e1ce-4125-85a4-51e6a083570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados.\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "921403d6-6863-4439-aeed-40408cf9159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que torna as imagens unidimensionais.\n",
    "@tf.function\n",
    "def to_1d(data):\n",
    "    return tf.reshape(data, [data.shape[0], data.shape[1]*data.shape[2]])\n",
    "\n",
    "# Imagens para 1-D.\n",
    "X_train = to_1d(X_train)\n",
    "X_test = to_1d(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "e33629e0-72f0-4f68-9a57-511ba2c5113c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘mnist’: File exists\n",
      "0 3500\n",
      "3500 7000\n",
      "7000 10500\n",
      "10500 14000\n",
      "14000 17500\n",
      "17500 21000\n",
      "21000 24500\n",
      "24500 28000\n",
      "28000 31500\n",
      "31500 35000\n",
      "35000 38500\n",
      "38500 42000\n",
      "42000 45500\n",
      "45500 49000\n",
      "49000 52500\n",
      "52500 56000\n",
      "56000 59500\n",
      "59500 63000\n",
      "63000 66500\n",
      "66500 70000\n"
     ]
    }
   ],
   "source": [
    "# Montando o dataset único.\n",
    "def make_dataset(X_train, y_train, X_test, y_test):\n",
    "    train = tf.concat((X_train, tf.reshape(y_train, (-1,1))), axis=1)\n",
    "    test = tf.concat((X_test, tf.reshape(y_test, (-1,1))), axis=1)\n",
    "    return tf.random.shuffle(tf.concat((train, test), axis=0), seed=42) # Misturando os dados antes de criarmos os .csv's. \n",
    "dataset = make_dataset(X_train, y_train, X_test, y_test).numpy()\n",
    "\n",
    "!mkdir mnist\n",
    "# Criando os vários arquivos separados\n",
    "'''\n",
    "for i in range(20):\n",
    "    np.savetxt(f'mnist/mnist_{i}.csv', dataset[3500*i:3500*(i+1)], delimiter=';')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "797ea3ce-c082-48e6-a440-6cf7b8781f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import FloatList, Int64List, Example, Feature, Features\n",
    "\n",
    "def create_mnists(line):\n",
    "    example = Example(\n",
    "                features=Features(\n",
    "                    feature={\n",
    "                    \n",
    "                    }\n",
    "                                )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "76392816-fcd9-4f25-a5ed-b919c51eb772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.protobuf.pyext._message.MessageDescriptor at 0x7f9c9d780b80>"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'id':Feature(int64_list=Int64List(value=[1])),\n",
    "            'name':Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'emails':Feature(bytes_list=BytesList(value=[b'email@gmail.com']))\n",
    "    }))\n",
    "\n",
    "a.features.DESCRIPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64bbaf-a34b-4fc1-965c-fc99ff81aa5a",
   "metadata": {},
   "source": [
    "<p style='color:red'> Montar a função que gera os Example's\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
