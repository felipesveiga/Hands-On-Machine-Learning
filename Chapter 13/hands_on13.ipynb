{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce351e58-b23d-400f-aa5b-30744b6635fd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Loading and Preprocessing Data with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af312d-0fae-4ee5-9faf-4924dfac7c55",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Data API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Data API é um módulo do Tensor Flow voltado ao tratamento de datasets volumosos. É interessante ser usado quando os dados não cabem na memória RAM ou placa de vídeo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A classe `Dataset` é onde nossos dados são armazenados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ed8807-7b25-4b0f-be81-b238d61fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Criando um `Dataset` a partir de um `tf.range`. \n",
    "X = tf.range(10)\n",
    "\n",
    "# Cada número de `X` será encapsulado em um tensor.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cee74d4-a06f-4032-90cb-c3ea343d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo Dataset poderia ter sido montado da seguinte maneira:\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b337-72e7-41da-b9d5-0d3dd838a1ed",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chaining Transformations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As funções da classe `Dataset` nunca fazem transformações in-place; sempre retornam um novo objeto.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2a642-3990-4220-abf3-6b1c39c1a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#  `repeat` repetirá os dados do dataset, enquanto `batch` vai criar batches com n instâncias.\n",
    "for item in dataset.repeat(3).batch(7):\n",
    "    # Para evitar que o último batch com os elementos restantes do dataset seja formado, passe `drop_remainder`=True.\n",
    "    print(item)\n",
    "    \n",
    "# Observe que `batch` não embaralha os dados na hora de sua separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4d9175c-fd02-41c3-bf56-eb8040f40b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=81>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=256>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=625>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1296>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2401>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4096>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=6561>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `map` aplica uma função por elemento.\n",
    "# `apply` invoca uma transformação a todo o dataset.\n",
    "list(dataset.map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3656c0-270f-408f-88b3-268a244eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `filter`, é bastante eficaz em remover dados indesejados.\n",
    "list(dataset.filter(lambda x: x<5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06f35-e712-4ae4-bc0c-0287dc8c3379",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Shuffling the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `shuffle` embaralha as instâncias com a seguinte lógica: escolhe as primeiras x instâncias do dataset e as agrupa em um buffer. Daí, as mistura e sorteia um dado, quando solicitado. Após a extração, o buffer fica com um espaço sobrando, que é preenchido com a próxima instância do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por conta do algoritmo, é contraindicado usar `buffer_size`'s pequenos. A documentação do TF indica, até mesmo, designarmos um valor maior ou igual ao o dataset. Apenas cuidado com a memória RAM!            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af9184b-6aff-456d-a286-a94332ccc090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando um dataset artificial (`buffer_size`=10).\n",
    "\n",
    "# `reshuffle_each_iteration` garante que um novo embaralhamento dos dados ocorrerá, caso usemos `repeat`, por exemplo.\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "list(dataset.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f4d4-850f-402d-a193-431fd6db927b",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Interleaving lines from multiple files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A fim de garantirmos a ordem aleatória dos dados no abastecimento de modelos, talvez seja interessante quebrarmos o set de treino em múltiplos arquivos. Assim, os lemos simultaneamente, intercalando as suas linhas. No final, conseguimos ainda aplicar a função `shuffle` para misturar todo o produto da leitura.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2567bf-4b2a-4803-b0a1-146c07bdb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde vão ser armazenados os dados.\n",
    "from os import mkdir\n",
    "mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c18f6c2-3711-49d5-8a57-77e25778fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "X,y = fetch_california_housing(return_X_y=True,as_frame=True)\n",
    "data = pd.merge(X,y, left_index=True, right_index=True)\n",
    "\n",
    "for i in range(0, len(data), 1000): # Forma rápida de gerar os arquivos, mesmo sabendo que vou perder algumas linhas.\n",
    "    data.iloc[i:i+1000].to_csv(f'data/housing_{int(i/1000)}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee652eb7-00fd-4bc4-8315-67ca8b522f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_4.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_14.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_5.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_1.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_19.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_2.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_20.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_11.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_10.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_12.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_7.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_0.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_8.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_18.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_15.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_3.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_6.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_16.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_13.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_9.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_17.csv'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método `list_files` localiza um conjunto de arquivos baseado em certo padrão nas strings.\n",
    "# Retorna um dataset com os paths de todos os arquivos que estão de acordo com a string oferecida.\n",
    "filepath_dataset = tf.data.Dataset.list_files('data/housing_*.csv', shuffle=True)\n",
    "list(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f430140-951b-464e-b870-7a62025b989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A leitura dinâmica dos arquivos pode ser feita com o método `interleave`.\n",
    "\n",
    "# Esse aplica uma função sobre todo o `Dataset` e intercala as suas linhas.\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Por padrão, não há uso de paralelismo. Uma linha de cada dataset é lida por vez.\n",
    "    # Use `num_parallel_calls` se quiser fazer esse processo por multithreading.\n",
    "    num_parallel_calls=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b52979c-0030-40b4-9668-604c7c8b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'12000,7.5408,3.0,8.493150684931507,1.0342465753424657,519.0,3.5547945205479454,33.93,-117.57,2.719'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'16000,4.0846,52.0,4.821316614420063,0.9561128526645768,819.0,2.5673981191222572,37.74,-122.47,3.336'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3000,4.1484,10.0,4.791907514450867,0.8439306358381503,447.0,2.5838150289017343,35.3,-119.03,1.029'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'11000,13.466,26.0,8.874233128834355,1.0582822085889572,983.0,3.0153374233128836,33.75,-117.79,5.00001'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'9000,4.0,49.0,6.866295264623956,1.0362116991643453,1018.0,2.8356545961002784,34.0,-118.34,2.968'>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja: os valores das features estão todos contidos dentro de uma string bytes.\n",
    "\n",
    "# É necessário ainda fazermos mais um tratamento, coletando os números individualizados.\n",
    "list(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f9f90-e661-4e56-8d3b-05edf430febf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preprocessing the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Criaremos uma função que recebe uma linha do dataset e a retorna devidamente tratada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8226674f-cc32-4523-bc0c-e95db705394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios-padrão das colunas para padronização.\n",
    "X_mean, X_std = X.mean().to_numpy(), X.std().to_numpy()\n",
    "\n",
    "# Função de tratamento dos dados.\n",
    "def preprocess(line):\n",
    "    n_inputs = 8 # Quantidade de colunas presentes.\n",
    "    # Lista com os valores-padrão a serem imputados nas células caso haja NaN's. Um tensor vazio será gerado caso \n",
    "    # o nulo ocorra em uma target-variable, acarretando em um erro.\n",
    "    default_values = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    # Decodificando a linha. 'default_values' será usado na ocorrência de vazios.\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_values) # `fields é uma lista de tensores que contêm um único número.\n",
    "    # A função `stack` empilha os valores dos tensores em um único tensor unidimensional.\n",
    "    x, y = tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "    # Retornando as variáveis independentes padronizadas juntamente com as dependentes.\n",
    "    return (x-X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "755c26d6-3313-4608-b03d-b11cf5a64473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ -0.8635921 ,  -2.1381242 ,   2.2435777 ,   7.3079667 ,\n",
       "         -1.250785  ,  -0.29468906, -16.66791   ,  69.81657   ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.41], dtype=float32)>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.23,1.73,10.98,4.56,9.01,0.01,0.03,20.31,5.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b196e-53cb-49c9-9b3c-3ee35055b2b1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Putting Everything Together</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Prefetching</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `prefetch` contribui para a performance de nosso programa. Enquanto um dado é processado, o método faz com que a próxima instância (ou batch) passe pelo tratamento em paralelo, economizando tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdae2d-e011-4272-9b99-d428003033f5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='prefetch.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "46c443c4-05ca-4a8c-b21b-20dc543eb3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preparando a próxima instância proveniente de 'range_dataset'.\n",
    "range_dataset.prefetch(1)\n",
    "\n",
    "# Preparando o próximo batch criado a partir de 'range_dataset'.\n",
    "range_dataset.repeat(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3178bc10-ea30-4a4a-a397-9e966232ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a seção com uma função  que realiza todos os passos demonstrados.\n",
    "def csv_reader_dataset(filepath:str, cycle_length:int=5, buffer_size:int=1000, repeat:int=3, batch_size:int=32):\n",
    "    filepaths = tf.data.Dataset.list_files(filepath) # Coletando todos os arquivos correspondentes ao padrão `filepath`.\n",
    "    # Lendo `cycle_length` arquivos, pulando as suas primeiras linhas. \n",
    "    dataset = filepaths.interleave(lambda x: tf.data.TextLineDataset(x).skip(1), \n",
    "                                   cycle_length=cycle_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache() # Tratando os dados e os armazenando em cache.\n",
    "    dataset = dataset.shuffle(buffer_size).repeat(repeat) # Embaralhando e repetindo os dados.\n",
    "    # \n",
    "    return dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25104e2-5397-41bb-b85c-71b0f2ae09b7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Usamos `cache` após o tratamento, mas antes do shuffling e batching. Isso garantirá que a mistura e batches serão distintos a cada iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "12922ca9-ce52-4ba6-9971-5fdcaac4c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste da função.\n",
    "csv_reader_dataset('data/housing_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe6abf-5e1b-48cc-8a2c-3364a02567e4",
   "metadata": {},
   "source": [
    "<p style='color:red'> Using the Dataset with tf.kerasUsing the Dataset with tf.keras\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
