{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce351e58-b23d-400f-aa5b-30744b6635fd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Loading and Preprocessing Data with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af312d-0fae-4ee5-9faf-4924dfac7c55",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Data API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Data API é um módulo do Tensor Flow voltado ao tratamento de datasets volumosos. É interessante ser usado quando os dados não cabem na memória RAM ou placa de vídeo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A classe `Dataset` é onde nossos dados são armazenados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ed8807-7b25-4b0f-be81-b238d61fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Criando um `Dataset` a partir de um `tf.range`. \n",
    "X = tf.range(10)\n",
    "\n",
    "# Cada número de `X` será encapsulado em um tensor.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cee74d4-a06f-4032-90cb-c3ea343d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo Dataset poderia ter sido montado da seguinte maneira:\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b337-72e7-41da-b9d5-0d3dd838a1ed",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chaining Transformations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As funções da classe `Dataset` nunca fazem transformações in-place; sempre retornam um novo objeto.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2a642-3990-4220-abf3-6b1c39c1a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#  `repeat` repetirá os dados do dataset, enquanto `batch` vai criar batches com n instâncias.\n",
    "for item in dataset.repeat(3).batch(7):\n",
    "    # Para evitar que o último batch com os elementos restantes do dataset seja formado, passe `drop_remainder`=True.\n",
    "    print(item)\n",
    "    \n",
    "# Observe que `batch` não embaralha os dados na hora de sua separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4d9175c-fd02-41c3-bf56-eb8040f40b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=81>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=256>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=625>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1296>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2401>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4096>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=6561>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `map` aplica uma função por elemento.\n",
    "# `apply` invoca uma transformação a todo o dataset.\n",
    "list(dataset.map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3656c0-270f-408f-88b3-268a244eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `filter`, é bastante eficaz em remover dados indesejados.\n",
    "list(dataset.filter(lambda x: x<5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06f35-e712-4ae4-bc0c-0287dc8c3379",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Shuffling the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `shuffle` embaralha as instâncias com a seguinte lógica: escolhe as primeiras x instâncias do dataset e as agrupa em um buffer. Daí, as mistura e sorteia um dado, quando solicitado. Após a extração, o buffer fica com um espaço sobrando, que é preenchido com a próxima instância do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por conta do algoritmo, é contraindicado usar `buffer_size`'s pequenos. A documentação do TF indica, até mesmo, designarmos um valor maior ou igual ao o dataset. Apenas cuidado com a memória RAM!            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af9184b-6aff-456d-a286-a94332ccc090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando um dataset artificial (`buffer_size`=10).\n",
    "\n",
    "# `reshuffle_each_iteration` garante que um novo embaralhamento dos dados ocorrerá, caso usemos `repeat`, por exemplo.\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "list(dataset.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f4d4-850f-402d-a193-431fd6db927b",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Interleaving lines from multiple files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A fim de garantirmos a ordem aleatória dos dados no abastecimento de modelos, talvez seja interessante quebrarmos o set de treino em múltiplos arquivos. Assim, os lemos simultaneamente, intercalando as suas linhas. No final, conseguimos ainda aplicar a função `shuffle` para misturar todo o produto da leitura.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2567bf-4b2a-4803-b0a1-146c07bdb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde vão ser armazenados os dados.\n",
    "from os import mkdir\n",
    "mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0c18f6c2-3711-49d5-8a57-77e25778fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "X,y = fetch_california_housing(return_X_y=True,as_frame=True)\n",
    "data = pd.merge(X,y, left_index=True, right_index=True)\n",
    "\n",
    "for i in range(0, len(data), 1000): # Forma rápida de gerar os arquivos, mesmo sabendo que vou perder algumas linhas.\n",
    "    data.iloc[i:i+1000].to_csv(f'data/housing_{int(i/1000)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee652eb7-00fd-4bc4-8315-67ca8b522f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_4.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_14.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_5.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_1.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_19.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_2.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_20.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_11.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_10.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_12.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_7.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_0.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_8.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_18.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_15.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_3.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_6.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_16.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_13.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_9.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_17.csv'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método `list_files` localiza um conjunto de arquivos baseado em certo padrão nas strings.\n",
    "# Retorna um dataset com os paths de todos os arquivos que estão de acordo com a string oferecida.\n",
    "filepath_dataset = tf.data.Dataset.list_files('data/housing_*.csv', shuffle=True)\n",
    "list(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f430140-951b-464e-b870-7a62025b989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A leitura dinâmica dos arquivos pode ser feita com o método `interleave`.\n",
    "\n",
    "# Esse aplica uma função sobre todo o `Dataset` e intercala as suas linhas.\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Por padrão, não há uso de paralelismo. Uma linha de cada dataset é lida por vez.\n",
    "    # Use `num_parallel_calls` se quiser fazer esse processo por multithreading.\n",
    "    num_parallel_calls=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b52979c-0030-40b4-9668-604c7c8b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'12000,7.5408,3.0,8.493150684931507,1.0342465753424657,519.0,3.5547945205479454,33.93,-117.57,2.719'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'16000,4.0846,52.0,4.821316614420063,0.9561128526645768,819.0,2.5673981191222572,37.74,-122.47,3.336'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3000,4.1484,10.0,4.791907514450867,0.8439306358381503,447.0,2.5838150289017343,35.3,-119.03,1.029'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'11000,13.466,26.0,8.874233128834355,1.0582822085889572,983.0,3.0153374233128836,33.75,-117.79,5.00001'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'9000,4.0,49.0,6.866295264623956,1.0362116991643453,1018.0,2.8356545961002784,34.0,-118.34,2.968'>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja: os valores das features estão todos contidos dentro de uma string bytes.\n",
    "\n",
    "# É necessário ainda fazermos mais um tratamento, coletando os números individualizados.\n",
    "list(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f9f90-e661-4e56-8d3b-05edf430febf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preprocessing the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Criaremos uma função que recebe uma linha do dataset e a retorna devidamente tratada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8226674f-cc32-4523-bc0c-e95db705394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios-padrão das colunas para padronização.\n",
    "X_mean, X_std = X.mean().to_numpy(), X.std().to_numpy()\n",
    "\n",
    "# Função de tratamento dos dados.\n",
    "def preprocess(line):\n",
    "    n_inputs = 8 # Quantidade de colunas presentes.\n",
    "    # Lista com os valores-padrão a serem imputados nas células caso haja NaN's. Um tensor vazio será gerado caso \n",
    "    # o nulo ocorra em uma target-variable, acarretando em um erro.\n",
    "    default_values = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    # Decodificando a linha. 'default_values' será usado na ocorrência de vazios.\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_values) # `fields é uma lista de tensores que contêm um único número.\n",
    "    # A função `stack` empilha os valores dos tensores em um único tensor unidimensional.\n",
    "    x, y = tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "    # Retornando as variáveis independentes padronizadas juntamente com as dependentes.\n",
    "    return (x-X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "755c26d6-3313-4608-b03d-b11cf5a64473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ -0.8635921 ,  -2.1381242 ,   2.2435777 ,   7.3079667 ,\n",
       "         -1.250785  ,  -0.29468906, -16.66791   ,  69.81657   ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.41], dtype=float32)>)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.23,1.73,10.98,4.56,9.01,0.01,0.03,20.31,5.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b196e-53cb-49c9-9b3c-3ee35055b2b1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Putting Everything Together</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Prefetching</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `prefetch` contribui para a performance de nosso programa. Enquanto um dado é processado, o método faz com que a próxima instância (ou batch) passe pelo tratamento em paralelo, economizando tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdae2d-e011-4272-9b99-d428003033f5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='prefetch.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "46c443c4-05ca-4a8c-b21b-20dc543eb3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preparando a próxima instância proveniente de 'range_dataset'.\n",
    "range_dataset.prefetch(1)\n",
    "\n",
    "# Preparando o próximo batch criado a partir de 'range_dataset'.\n",
    "range_dataset.repeat(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3178bc10-ea30-4a4a-a397-9e966232ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a seção com uma função  que realiza todos os passos demonstrados.\n",
    "def csv_reader_dataset(filepath:str, cycle_length:int=5, buffer_size:int=1000, repeat:int=3, batch_size:int=32):\n",
    "    filepaths = tf.data.Dataset.list_files(filepath) # Coletando todos os arquivos correspondentes ao padrão `filepath`.\n",
    "    # Lendo `cycle_length` arquivos, pulando as suas primeiras linhas. \n",
    "    dataset = filepaths.interleave(lambda x: tf.data.TextLineDataset(x).skip(1), \n",
    "                                   cycle_length=cycle_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache() # Tratando os dados e os armazenando em cache.\n",
    "    dataset = dataset.shuffle(buffer_size).repeat(repeat) # Embaralhando e repetindo os dados.\n",
    "    # A função segregará os dados em batches. À cada batch sendo processado, o próximo é criado.\n",
    "    return dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25104e2-5397-41bb-b85c-71b0f2ae09b7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Usamos `cache` após o tratamento, mas antes do shuffling e batching. Isso garantirá que a mistura e batches serão distintos a cada iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "12922ca9-ce52-4ba6-9971-5fdcaac4c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste da função.\n",
    "csv_reader_dataset('data/housing_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df714-107c-4d8f-aaba-316cb0996ea0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using the Dataset with tf.keras</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Com a Pipeline montada, é hora de criar uma rede neural com os dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a1a634c8-f83d-4652-ac2a-09f1a4d6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos de treino, validação e teste.\n",
    "train_paths = [f'data/housing_{i}.csv' for i in range(12)]\n",
    "valid_paths = [f'data/housing_{i}.csv' for i in range(12, 16)]\n",
    "test_paths = [f'data/housing_{i}.csv' for i in range(16, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "bd3be161-ec1b-40fa-95cb-727dd87717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets com os dados.\n",
    "train_set = csv_reader_dataset(train_paths)\n",
    "valid_set = csv_reader_dataset(valid_paths)\n",
    "test_set = csv_reader_dataset(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c9b6a9fc-3d0b-456b-859e-ecca72d70455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hora demontar o modelo.\n",
    "def data_api_model():\n",
    "    model = keras.models.Sequential()\n",
    "    # Camada de Input.\n",
    "    model.add(keras.layers.Input(shape=[8]))\n",
    "    # Hidden Layers.\n",
    "    for _ in range(5): \n",
    "        model.add(keras.layers.Dense(40, activation='elu', kernel_initializer='lecun_normal'))\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "8087f5e9-2c91-4643-a97e-6ae15049aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 10s 6ms/step - loss: 0.4308 - val_loss: 0.7267\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.3109 - val_loss: 0.6991\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2969 - val_loss: 0.6759\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2858 - val_loss: 0.8068\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2782 - val_loss: 0.5866\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2722 - val_loss: 0.6028\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2639 - val_loss: 0.5513\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2608 - val_loss: 0.5665\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2563 - val_loss: 0.5633\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2543 - val_loss: 0.5831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c15dcda60>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = data_api_model()\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "model.fit(train_set, batch_size=20, epochs=10, validation_data=valid_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b39dd-6e8f-440d-a8df-469408e090d7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> The TFRecord Format</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O formato de arquivo TFRecord é conhecido pela sua flexibilidade. É capaz de armazenar diferentes tipos de dados, como textos e áudios.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "39c8b714-1df9-400b-8739-08033d98fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pequeno arquivo TFRecord\n",
    "with tf.io.TFRecordWriter('my_tfrecord.tfrecord') as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "417543aa-1f54-4716-aab2-ed49c3430e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Row', shape=(), dtype=string)\n",
      "tf.Tensor(b'Second Row', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Agora, lendo o arquivo.\n",
    "file = tf.data.TFRecordDataset('my_tfrecord.tfrecord')\n",
    "\n",
    "for item in file:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a7f6b-2ac2-46db-9c4a-a7e60fbce906",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Compressed TFRecord Files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Conseguimos usar métodos de compressão nos arquivos .tfrecord.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3fdba58a-df44-4df0-b039-5f74b6ba5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_tfrecord2.tfrecord', options=options) as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2aa7c56e-95f4-471f-9255-b33c13fb6614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ao carregar esse mesmo arquivo, você deve informar o método que o comprimiu.\n",
    "tf.data.TFRecordDataset('my_tfrecord2.tfrecord', compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163b4d7-328c-4908-9f01-f85895d70389",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> A Brief Introduction to Protocol Buffers</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O protocol buffer (protobuf) é uma espécie de formatação de arquivos desenvolvida pelo Google. Sua sintaxe é similar com a do JSON, mas é muito mais rápida de ser lida, pois seus registros são armazenados em bytes.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nos arquivos TFRecord, os dados pertencentes às instâncias são armazenados no formato de protobufs,\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ac876-f0ba-4f33-bd1d-f0a1e2ec43ad",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> TensorFlow Protobufs</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Somos capazes de construir os nossos próprios protobufs com o módulo `tf.train`. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "771be6e6-b7ad-4fd1-b8f2-af75a101c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"emails\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"email@gmail.com\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"name\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"Alice\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A classe `Example` representa uma única instância do dataset. Essa contém o argumento 'features', que admite uma classe `Features`.\n",
    "# Essa, por sua vez, tem o campo 'feature', que recebe um dicionário <nome_feature>:Feature(ClasseDataType(valor_feature)).\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Feature, Features\n",
    "\n",
    "# Montando um protobuf-exemplo.\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'id':Feature(int64_list=Int64List(value=[1])),\n",
    "            'name':Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'emails':Feature(bytes_list=BytesList(value=[b'email@gmail.com']))\n",
    "    }))\n",
    "person_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "85f1ed25-988d-494a-bb74-0c6b6375329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos registrar a instância em um arquivo .tfrecord.\n",
    "with tf.io.TFRecordWriter('my_contacts_record.tfrecord') as f:    \n",
    "    f.write(person_example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bf366-8335-4b80-9021-7b859e61525f",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Loading and Parsing Examples</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Para carregar o protobuf de um exemplo, use a função `parse_single_example` dentro de um loop envolvendo a classe `TFRecordDataset`.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3c61eee2-7e7e-4769-b73c-19de8c84ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método requer um dicionário que mapeia o nome da feature para uma classe que sinaliza informações como shape, datatype e default_value.\n",
    "\n",
    "# 'id' e 'name' sempre serão listas de um único elemento. Por isso, a classe `FixedLenFeature` é usada.\n",
    "# Por outro lado, a lista de 'emails' admite qualquer quantidade de strings. Dessa forma, recorremos à VarLenFeature, que exige apenas\n",
    "# que o data type seja informado.\n",
    "description = {\n",
    "        'id':tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        'name':tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'emails':tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "# Lendo o `Example` do arquivo há pouco gerado.\n",
    "for serialized_example in tf.data.TFRecordDataset('my_contacts_record.tfrecord'):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e4a7b8c2-aec7-4d28-91ac-ebba26a5f8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f9bbc10b6a0>,\n",
       " 'id': <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `parsed_example` é um dicionário <feature>:<Tensor_feature>.\n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "cefed637-5425-4ba1-b9e7-3e3e3afb5545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'email@gmail.com'], dtype=object)>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que a feature informada como de tamanho variável é retornada como um tensor esparso. \n",
    "\n",
    "# Podemos ler os seus valores tanto com `tf.sparse.to_dense`, quanto usando o atributo 'values' do tensor.\n",
    "parsed_example['emails'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01a95c-9dc2-4f6a-9aa0-096705d69c16",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Mas é claro que não leríamos uma única instância do dataset em projetos reais. Use  `parse_example` para ler múltiplas linhas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e5cf2755-6531-4c58-837d-8483c366952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para exemplificar o uso, vamos recorrer ao `fetch_california_housing`, já que esse possui bastantes linhas.\n",
    "data = pd.read_csv('data/housing_0.csv')\n",
    "\n",
    "# Pequena função para a criação de uma classe `Example` para cada linha de 'data'.\n",
    "def create_example(line, columns=data.columns):\n",
    "    example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                column:Feature(float_list=FloatList(value=[line[i]]))\n",
    "                  for i, column in enumerate(columns)          \n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "94facdf9-4453-439c-bfdc-abd502bc8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando os dados em um arquivo 'tf.record'.\n",
    "with tf.io.TFRecordWriter('housing_tf.tfrecord') as f:\n",
    "    for row in data.values:\n",
    "        example = create_example(row)\n",
    "        f.write(example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "bb6147bd-4d81-4598-a898-02e28db29b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oferecendo os data types de cada coluna para a leitura do arquivo produzido.\n",
    "description = {\n",
    "    col:tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "    for col in X.columns\n",
    "}\n",
    "\n",
    "# Ao invés de lermos instância por instância, usamos `parse_example` para ler batches de 10 dados e embaralhá-los.\n",
    "for data in tf.data.TFRecordDataset('housing_tf.tfrecord').batch(10).shuffle(10, seed=42):\n",
    "    parsed_examples = tf.io.parse_example(data, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9a826fc1-8690-4f13-bf1f-6a5eedc8794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AveBedrms': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([1.0095367 , 1.0135417 , 1.0440421 , 0.9763113 , 1.0832196 ,\n",
       "        1.0850723 , 0.9019608 , 0.97894734, 0.97792023, 1.1014493 ],\n",
       "       dtype=float32)>,\n",
       " 'AveOccup': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([3.2043598, 2.9739583, 3.297217 , 3.3519459, 2.43588  , 3.165329 ,\n",
       "        3.0952382, 3.630263 , 3.0762107, 3.3064182], dtype=float32)>,\n",
       " 'AveRooms': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([5.737057 , 5.7625   , 7.635234 , 8.30626  , 5.075716 , 6.9245586,\n",
       "        5.647059 , 7.075    , 6.5292025, 6.3913045], dtype=float32)>,\n",
       " 'HouseAge': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([26., 27.,  7.,  9., 10., 23., 26., 13., 22., 26.], dtype=float32)>,\n",
       " 'Latitude': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([37.52, 37.53, 37.53, 37.49, 37.49, 37.48, 37.47, 37.47, 37.55,\n",
       "        37.55], dtype=float32)>,\n",
       " 'Longitude': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([-121.96, -121.93, -121.92, -121.89, -121.92, -121.92, -121.92,\n",
       "        -121.91, -122.03, -122.03], dtype=float32)>,\n",
       " 'MedInc': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([5.2396, 4.7478, 8.4045, 9.7194, 5.1643, 5.3813, 6.0878, 6.868 ,\n",
       "        6.992 , 4.9118], dtype=float32)>,\n",
       " 'Population': <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 2352.,  2855., 12203.,  1981.,  3571.,  1972.,  1105.,  2759.,\n",
       "         4319.,  1597.], dtype=float32)>}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4540c-2c3b-400b-ab4a-a1f573de936a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Preprocessing the Input Features</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Vamos aprender aqui a implantar algumas técnicas de preprocessamento de dados pelo TensorFlow.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "30bb178a-29e8-4c63-b03f-0a8bdfdd3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20640, 8), dtype=float64, numpy=\n",
       "array([[ 0.01343958,  0.00912272,  0.00814819, ...,  0.00164245,\n",
       "         0.00738648, -0.00711444],\n",
       "       [ 0.01340116,  0.00467261,  0.00727786, ...,  0.00135599,\n",
       "         0.00738258, -0.00711386],\n",
       "       [ 0.0117158 ,  0.01157028,  0.00966954, ...,  0.00180101,\n",
       "         0.00738063, -0.00711502],\n",
       "       ...,\n",
       "       [ 0.00274435,  0.00378259,  0.00607316, ...,  0.00149468,\n",
       "         0.00768873, -0.00705565],\n",
       "       [ 0.00301427,  0.0040051 ,  0.0062178 , ...,  0.00136458,\n",
       "         0.00768873, -0.00706148],\n",
       "       [ 0.00385598,  0.00356009,  0.00613053, ...,  0.00168193,\n",
       "         0.00767703, -0.00705682]])>"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O livro ensina a fazer uma Padronização, mas achei que faria mais sentido montar uma camada de Normalização.\n",
    "class NormLayer(keras.layers.Layer):\n",
    "    # Chame `adapt` para que as normas do seu dataset sejam mensuradas.\n",
    "    def adapt(self, input_data):\n",
    "        # Normas das features são armazenadas em `self.norms`.\n",
    "        self.norms = tf.linalg.norm(input_data, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Normalizando os dados.\n",
    "        return inputs / self.norms\n",
    "    \n",
    "norm_layer = NormLayer()\n",
    "norm_layer.adapt(X)\n",
    "norm_layer.call(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5a01a-f4ab-460f-a0b1-a54d2c0fad7b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoding Categorical Features Using One-Hot Vectors</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Como visto no Capítulo 2, os vetores One-Hot indicam a presença ou não de uma determinada categoria no dataset. O TensorFlow possui mecanismos robustos para lidar com esse tipo de tratament.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "7af87bc1-aecf-4f47-aea2-cba1f85dc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como ilustração, usaremos as categorias presentes no 'housing.csv'.\n",
    "df = pd.read_csv('housing.csv')\n",
    "vocab = df['ocean_proximity'].unique() # Listando as categorias.\n",
    "indices = tf.range(len(vocab), dtype=tf.int64) # Vetor com o código de cada categoria.\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices) # Inicializador da tabela de Lookup.\n",
    "num_oov_buckets = 2 # A tabela de Lookup admite até duas novas categorias.\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d42dec-7618-4ca4-973e-3aea93499c0c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Tabela de Lookup terá duas colunas extras para categorias ausentes no vocabulário informado. Uma vez que um termo desse tipo é encontrado, ele é designado a uma coluna, passando essa a pertencê-la.\n",
    "        </li>\n",
    "        <li> \n",
    "            Se a quantidade de categorias desconhecidas exceder 'num_oov_buckets', termos distintos serão designados a um mesmo bucket.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "627cfe3f-caad-42ee-a31e-7ecca8bfd17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([4, 0, 4, 4, 4, 6])>"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando a tabela de Lookup.\n",
    "categories = np.random.choice(vocab, size=5)\n",
    "categories = tf.constant(np.append(categories, 'New Cat')) # Acrescentando uma categoria ausente no vocabulário.\n",
    "\n",
    "# Veja que 'New Cat' assume o índice 6.\n",
    "table.lookup(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "d966ef76-d5d5-4427-9225-77f8b1ec1cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O output do método `lookup` pode ser aproveitado para fazer o One-Hot Encoding.\n",
    "# Precismamos informar em `depth` a quantidade de categorias esperadas.\n",
    "tf.one_hot(table.lookup(categories), depth=len(vocab) + num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3503e-983c-4538-a6c3-4d37c2405a3c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Encoding Categorical Features Using Embeddings</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64bbaf-a34b-4fc1-965c-fc99ff81aa5a",
   "metadata": {},
   "source": [
    "<p style='color:red'> Encoding Categorical Features Using Embeddings\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
