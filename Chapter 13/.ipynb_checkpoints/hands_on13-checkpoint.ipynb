{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce351e58-b23d-400f-aa5b-30744b6635fd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Loading and Preprocessing Data with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af312d-0fae-4ee5-9faf-4924dfac7c55",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Data API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Data API é um módulo do Tensor Flow voltado ao tratamento de datasets volumosos. É interessante ser usado quando os dados não cabem na memória RAM ou placa de vídeo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A classe `Dataset` é onde nossos dados são armazenados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ed8807-7b25-4b0f-be81-b238d61fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Criando um `Dataset` a partir de um `tf.range`. \n",
    "X = tf.range(10)\n",
    "\n",
    "# Cada número de `X` será encapsulado em um tensor.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cee74d4-a06f-4032-90cb-c3ea343d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo Dataset poderia ter sido montado da seguinte maneira:\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b337-72e7-41da-b9d5-0d3dd838a1ed",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chaining Transformations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As funções da classe `Dataset` nunca fazem transformações in-place; sempre retornam um novo objeto.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2a642-3990-4220-abf3-6b1c39c1a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#  `repeat` repetirá os dados do dataset, enquanto `batch` vai criar batches com n instâncias.\n",
    "for item in dataset.repeat(3).batch(7):\n",
    "    # Para evitar que o último batch com os elementos restantes do dataset seja formado, passe `drop_remainder`=True.\n",
    "    print(item)\n",
    "    \n",
    "# Observe que `batch` não embaralha os dados na hora de sua separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4d9175c-fd02-41c3-bf56-eb8040f40b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=81>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=256>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=625>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1296>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2401>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4096>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=6561>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `map` aplica uma função por elemento.\n",
    "# `apply` invoca uma transformação a todo o dataset.\n",
    "list(dataset.map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3656c0-270f-408f-88b3-268a244eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `filter`, é bastante eficaz em remover dados indesejados.\n",
    "list(dataset.filter(lambda x: x<5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06f35-e712-4ae4-bc0c-0287dc8c3379",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Shuffling the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `shuffle` embaralha as instâncias com a seguinte lógica: escolhe as primeiras x instâncias do dataset e as agrupa em um buffer. Daí, as mistura e sorteia um dado, quando solicitado. Após a extração, o buffer fica com um espaço sobrando, que é preenchido com a próxima instância do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por conta do algoritmo, é contraindicado usar `buffer_size`'s pequenos. A documentação do TF indica, até mesmo, designarmos um valor maior ou igual ao o dataset. Apenas cuidado com a memória RAM!            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af9184b-6aff-456d-a286-a94332ccc090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando um dataset artificial (`buffer_size`=10).\n",
    "\n",
    "# `reshuffle_each_iteration` garante que um novo embaralhamento dos dados ocorrerá, caso usemos `repeat`, por exemplo.\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "list(dataset.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f4d4-850f-402d-a193-431fd6db927b",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Interleaving lines from multiple files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A fim de garantirmos a ordem aleatória dos dados no abastecimento de modelos, talvez seja interessante quebrarmos o set de treino em múltiplos arquivos. Assim, os lemos simultaneamente, intercalando as suas linhas. No final, conseguimos ainda aplicar a função `shuffle` para misturar todo o produto da leitura.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2567bf-4b2a-4803-b0a1-146c07bdb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde vão ser armazenados os dados.\n",
    "from os import mkdir\n",
    "mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0c18f6c2-3711-49d5-8a57-77e25778fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "X,y = fetch_california_housing(return_X_y=True,as_frame=True)\n",
    "data = pd.merge(X,y, left_index=True, right_index=True)\n",
    "\n",
    "for i in range(0, len(data), 1000): # Forma rápida de gerar os arquivos, mesmo sabendo que vou perder algumas linhas.\n",
    "    data.iloc[i:i+1000].to_csv(f'data/housing_{int(i/1000)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee652eb7-00fd-4bc4-8315-67ca8b522f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_4.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_14.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_5.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_1.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_19.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_2.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_20.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_11.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_10.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_12.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_7.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_0.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_8.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_18.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_15.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_3.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_6.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_16.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_13.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_9.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_17.csv'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método `list_files` localiza um conjunto de arquivos baseado em certo padrão nas strings.\n",
    "# Retorna um dataset com os paths de todos os arquivos que estão de acordo com a string oferecida.\n",
    "filepath_dataset = tf.data.Dataset.list_files('data/housing_*.csv', shuffle=True)\n",
    "list(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f430140-951b-464e-b870-7a62025b989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A leitura dinâmica dos arquivos pode ser feita com o método `interleave`.\n",
    "\n",
    "# Esse aplica uma função sobre todo o `Dataset` e intercala as suas linhas.\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Por padrão, não há uso de paralelismo. Uma linha de cada dataset é lida por vez.\n",
    "    # Use `num_parallel_calls` se quiser fazer esse processo por multithreading.\n",
    "    num_parallel_calls=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b52979c-0030-40b4-9668-604c7c8b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'12000,7.5408,3.0,8.493150684931507,1.0342465753424657,519.0,3.5547945205479454,33.93,-117.57,2.719'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'16000,4.0846,52.0,4.821316614420063,0.9561128526645768,819.0,2.5673981191222572,37.74,-122.47,3.336'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3000,4.1484,10.0,4.791907514450867,0.8439306358381503,447.0,2.5838150289017343,35.3,-119.03,1.029'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'11000,13.466,26.0,8.874233128834355,1.0582822085889572,983.0,3.0153374233128836,33.75,-117.79,5.00001'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'9000,4.0,49.0,6.866295264623956,1.0362116991643453,1018.0,2.8356545961002784,34.0,-118.34,2.968'>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja: os valores das features estão todos contidos dentro de uma string bytes.\n",
    "\n",
    "# É necessário ainda fazermos mais um tratamento, coletando os números individualizados.\n",
    "list(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f9f90-e661-4e56-8d3b-05edf430febf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preprocessing the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Criaremos uma função que recebe uma linha do dataset e a retorna devidamente tratada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8226674f-cc32-4523-bc0c-e95db705394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios-padrão das colunas para padronização.\n",
    "X_mean, X_std = X.mean().to_numpy(), X.std().to_numpy()\n",
    "\n",
    "# Função de tratamento dos dados.\n",
    "def preprocess(line):\n",
    "    n_inputs = 8 # Quantidade de colunas presentes.\n",
    "    # Lista com os valores-padrão a serem imputados nas células caso haja NaN's. Um tensor vazio será gerado caso \n",
    "    # o nulo ocorra em uma target-variable, acarretando em um erro.\n",
    "    default_values = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    # Decodificando a linha. 'default_values' será usado na ocorrência de vazios.\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_values) # `fields é uma lista de tensores que contêm um único número.\n",
    "    # A função `stack` empilha os valores dos tensores em um único tensor unidimensional.\n",
    "    x, y = tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "    # Retornando as variáveis independentes padronizadas juntamente com as dependentes.\n",
    "    return (x-X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "755c26d6-3313-4608-b03d-b11cf5a64473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ -0.8635921 ,  -2.1381242 ,   2.2435777 ,   7.3079667 ,\n",
       "         -1.250785  ,  -0.29468906, -16.66791   ,  69.81657   ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.41], dtype=float32)>)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.23,1.73,10.98,4.56,9.01,0.01,0.03,20.31,5.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b196e-53cb-49c9-9b3c-3ee35055b2b1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Putting Everything Together</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Prefetching</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `prefetch` contribui para a performance de nosso programa. Enquanto um dado é processado, o método faz com que a próxima instância (ou batch) passe pelo tratamento em paralelo, economizando tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdae2d-e011-4272-9b99-d428003033f5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='prefetch.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "46c443c4-05ca-4a8c-b21b-20dc543eb3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preparando a próxima instância proveniente de 'range_dataset'.\n",
    "range_dataset.prefetch(1)\n",
    "\n",
    "# Preparando o próximo batch criado a partir de 'range_dataset'.\n",
    "range_dataset.repeat(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3178bc10-ea30-4a4a-a397-9e966232ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a seção com uma função  que realiza todos os passos demonstrados.\n",
    "def csv_reader_dataset(filepath:str, cycle_length:int=5, buffer_size:int=1000, repeat:int=3, batch_size:int=32):\n",
    "    filepaths = tf.data.Dataset.list_files(filepath) # Coletando todos os arquivos correspondentes ao padrão `filepath`.\n",
    "    # Lendo `cycle_length` arquivos, pulando as suas primeiras linhas. \n",
    "    dataset = filepaths.interleave(lambda x: tf.data.TextLineDataset(x).skip(1), \n",
    "                                   cycle_length=cycle_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache() # Tratando os dados e os armazenando em cache.\n",
    "    dataset = dataset.shuffle(buffer_size).repeat(repeat) # Embaralhando e repetindo os dados.\n",
    "    # A função segregará os dados em batches. À cada batch sendo processado, o próximo é criado.\n",
    "    return dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25104e2-5397-41bb-b85c-71b0f2ae09b7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Usamos `cache` após o tratamento, mas antes do shuffling e batching. Isso garantirá que a mistura e batches serão distintos a cada iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "12922ca9-ce52-4ba6-9971-5fdcaac4c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste da função.\n",
    "csv_reader_dataset('data/housing_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df714-107c-4d8f-aaba-316cb0996ea0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using the Dataset with tf.keras</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Com a Pipeline montada, é hora de criar uma rede neural com os dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a1a634c8-f83d-4652-ac2a-09f1a4d6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos de treino, validação e teste.\n",
    "train_paths = [f'data/housing_{i}.csv' for i in range(12)]\n",
    "valid_paths = [f'data/housing_{i}.csv' for i in range(12, 16)]\n",
    "test_paths = [f'data/housing_{i}.csv' for i in range(16, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "bd3be161-ec1b-40fa-95cb-727dd87717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets com os dados.\n",
    "train_set = csv_reader_dataset(train_paths)\n",
    "valid_set = csv_reader_dataset(valid_paths)\n",
    "test_set = csv_reader_dataset(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "e960f022-8e73-40ef-9b23-9ffa1bdd391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c9b6a9fc-3d0b-456b-859e-ecca72d70455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hora demontar o modelo.\n",
    "def data_api_model():\n",
    "    model = keras.models.Sequential()\n",
    "    # Camada de Input.\n",
    "    model.add(keras.layers.Input(shape=[8]))\n",
    "    # Hidden Layers.\n",
    "    for _ in range(5): \n",
    "        model.add(keras.layers.Dense(40, activation='elu', kernel_initializer='lecun_normal'))\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "8087f5e9-2c91-4643-a97e-6ae15049aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 10s 6ms/step - loss: 0.4308 - val_loss: 0.7267\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.3109 - val_loss: 0.6991\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2969 - val_loss: 0.6759\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2858 - val_loss: 0.8068\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2782 - val_loss: 0.5866\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2722 - val_loss: 0.6028\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2639 - val_loss: 0.5513\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2608 - val_loss: 0.5665\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2563 - val_loss: 0.5633\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 0.2543 - val_loss: 0.5831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c15dcda60>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = data_api_model()\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "model.fit(train_set, batch_size=20, epochs=10, validation_data=valid_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b39dd-6e8f-440d-a8df-469408e090d7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> The TFRecord Format</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O formato de arquivo TFRecord é conhecido pela sua flexibilidade. É capaz de armazenar diferentes tipos de dados, como textos e áudios.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "39c8b714-1df9-400b-8739-08033d98fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pequeno arquivo TFRecord\n",
    "with tf.io.TFRecordWriter('my_tfrecord.tfrecord') as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "417543aa-1f54-4716-aab2-ed49c3430e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Row', shape=(), dtype=string)\n",
      "tf.Tensor(b'Second Row', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Agora, lendo o arquivo.\n",
    "file = tf.data.TFRecordDataset('my_tfrecord.tfrecord')\n",
    "\n",
    "for item in file:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a7f6b-2ac2-46db-9c4a-a7e60fbce906",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Compressed TFRecord Files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Conseguimos usar métodos de compressão nos arquivos .tfrecord.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3fdba58a-df44-4df0-b039-5f74b6ba5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_tfrecord2.tfrecord', options=options) as f:\n",
    "    f.write('First Row')\n",
    "    f.write('Second Row')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2aa7c56e-95f4-471f-9255-b33c13fb6614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ao carregar esse mesmo arquivo, você deve informar o método que o comprimiu.\n",
    "tf.data.TFRecordDataset('my_tfrecord2.tfrecord', compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163b4d7-328c-4908-9f01-f85895d70389",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> A Brief Introduction to Protocol Buffers</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O protocol buffer (protobuf) é uma espécie de formatação de arquivos desenvolvida pelo Google. Sua sintaxe é similar com a do JSON, mas é muito mais rápida de ser lida, pois seus registros são armazenados em bytes.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nos arquivos TFRecord, os dados pertencentes às instâncias são armazenados no formato de protobufs,\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ac876-f0ba-4f33-bd1d-f0a1e2ec43ad",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> TensorFlow Protobufs</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Somos capazes de construir os nossos próprios protobufs com o módulo `tf.train`. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "771be6e6-b7ad-4fd1-b8f2-af75a101c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"emails\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"email@gmail.com\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"name\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"Alice\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A classe `Example` representa uma única instância do dataset. Essa contém o argumento 'features', que admite uma classe `Features`.\n",
    "# Essa, por sua vez, tem o campo 'feature', que recebe um dicionário <nome_feature>:Feature(ClasseDataType(valor_feature)).\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Feature, Features\n",
    "\n",
    "# Montando um protobuf-exemplo.\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'id':Feature(int64_list=Int64List(value=[1])),\n",
    "            'name':Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'emails':Feature(bytes_list=BytesList(value=[b'email@gmail.com']))\n",
    "    }))\n",
    "person_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "85f1ed25-988d-494a-bb74-0c6b6375329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos registrar a instância em um arquivo .tfrecord.\n",
    "with tf.io.TFRecordWriter('my_contacts_record.tfrecord') as f:    \n",
    "    f.write(person_example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bf366-8335-4b80-9021-7b859e61525f",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Loading and Parsing Examples</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Para carregar o protobuf de um exemplo, use a função `parse_single_example` dentro de um loop envolvendo a classe `TFRecordDataset`.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3c61eee2-7e7e-4769-b73c-19de8c84ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método requer um dicionário que mapeia o nome da feature para uma classe que sinaliza informações como shape, datatype e default_value.\n",
    "\n",
    "# 'id' e 'name' sempre serão listas de um único elemento. Por isso, a classe `FixedLenFeature` é usada.\n",
    "# Por outro lado, a lista de 'emails' admite qualquer quantidade de strings. Dessa forma, recorremos à VarLenFeature, que exige apenas\n",
    "# que o data type seja informado.\n",
    "description = {\n",
    "        'id':tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "        'name':tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'emails':tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "# Lendo o `Example` do arquivo há pouco gerado.\n",
    "for serialized_example in tf.data.TFRecordDataset('my_contacts_record.tfrecord'):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e4a7b8c2-aec7-4d28-91ac-ebba26a5f8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f9bbc10b6a0>,\n",
       " 'id': <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `parsed_example` é um dicionário <feature>:<Tensor_feature>.\n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "cefed637-5425-4ba1-b9e7-3e3e3afb5545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'email@gmail.com'], dtype=object)>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que a feature informada como de tamanho variável é retornada como um tensor esparso. \n",
    "\n",
    "# Podemos ler os seus valores tanto com `tf.sparse.to_dense`, quanto usando o atributo 'values' do tensor.\n",
    "parsed_example['emails'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01a95c-9dc2-4f6a-9aa0-096705d69c16",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Mas é claro que não leríamos uma única instância do dataset em prrojetos reais. Use  `parse_example` para ler múltiplas linhas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "b9465745-5cad-4fb2-9f19-ba2123d027e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataLossError",
     "evalue": "corrupted record at 0 [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_689758/110815842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data/housing_0.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mserialized_examples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#parsed_examples = tf.io.parse_example(serialized_example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: corrupted record at 0 [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(['data/housing_0.csv']).batch(10)\n",
    "\n",
    "for serialized_examples in dataset:\n",
    "    print(serialized_examples)\n",
    "    #parsed_examples = tf.io.parse_example(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64bbaf-a34b-4fc1-965c-fc99ff81aa5a",
   "metadata": {},
   "source": [
    "<p style='color:red'> Criar um .tfrecord com um dos arquivos do fetch_california_housing apenas para exemplificar o uso de `parse_example`.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
