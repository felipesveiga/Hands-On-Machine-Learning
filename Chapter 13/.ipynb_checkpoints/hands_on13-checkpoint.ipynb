{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce351e58-b23d-400f-aa5b-30744b6635fd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Loading and Preprocessing Data with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af312d-0fae-4ee5-9faf-4924dfac7c55",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Data API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Data API é um módulo do Tensor Flow voltado ao tratamento de datasets volumosos. É interessante ser usado quando os dados não cabem na memória RAM ou placa de vídeo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A classe `Dataset` é onde nossos dados são armazenados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ed8807-7b25-4b0f-be81-b238d61fef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Criando um `Dataset` a partir de um `tf.range`. \n",
    "X = tf.range(10)\n",
    "\n",
    "# Cada número de `X` será encapsulado em um tensor.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cee74d4-a06f-4032-90cb-c3ea343d4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo Dataset poderia ter sido montado da seguinte maneira:\n",
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b337-72e7-41da-b9d5-0d3dd838a1ed",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chaining Transformations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As funções da classe `Dataset` nunca fazem transformações in-place; sempre retornam um novo objeto.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2a642-3990-4220-abf3-6b1c39c1a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#  `repeat` repetirá os dados do dataset, enquanto `batch` vai criar batches com n instâncias.\n",
    "for item in dataset.repeat(3).batch(7):\n",
    "    # Para evitar que o último batch com os elementos restantes do dataset seja formado, passe `drop_remainder`=True.\n",
    "    print(item)\n",
    "    \n",
    "# Observe que `batch` não embaralha os dados na hora de sua separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4d9175c-fd02-41c3-bf56-eb8040f40b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=81>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=256>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=625>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1296>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2401>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4096>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=6561>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `map` aplica uma função por elemento.\n",
    "# `apply` invoca uma transformação a todo o dataset.\n",
    "list(dataset.map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3656c0-270f-408f-88b3-268a244eba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `filter`, é bastante eficaz em remover dados indesejados.\n",
    "list(dataset.filter(lambda x: x<5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06f35-e712-4ae4-bc0c-0287dc8c3379",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Shuffling the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `shuffle` embaralha as instâncias com a seguinte lógica: escolhe as primeiras x instâncias do dataset e as agrupa em um buffer. Daí, as mistura e sorteia um dado, quando solicitado. Após a extração, o buffer fica com um espaço sobrando, que é preenchido com a próxima instância do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por conta do algoritmo, é contraindicado usar `buffer_size`'s pequenos. A documentação do TF indica, até mesmo, designarmos um valor maior ou igual ao o dataset. Apenas cuidado com a memória RAM!            \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af9184b-6aff-456d-a286-a94332ccc090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=2>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embaralhando um dataset artificial (`buffer_size`=10).\n",
    "\n",
    "# `reshuffle_each_iteration` garante que um novo embaralhamento dos dados ocorrerá, caso usemos `repeat`, por exemplo.\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "list(dataset.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f4d4-850f-402d-a193-431fd6db927b",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Interleaving lines from multiple files</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A fim de garantirmos a ordem aleatória dos dados no abastecimento de modelos, talvez seja interessante quebrarmos o set de treino em múltiplos arquivos. Assim, os lemos simultaneamente, intercalando as suas linhas. No final, conseguimos ainda aplicar a função `shuffle` para misturar todo o produto da leitura.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b2567bf-4b2a-4803-b0a1-146c07bdb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde vão ser armazenados os dados.\n",
    "from os import mkdir\n",
    "mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0c18f6c2-3711-49d5-8a57-77e25778fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "X,y = fetch_california_housing(return_X_y=True,as_frame=True)\n",
    "data = pd.merge(X,y, left_index=True, right_index=True)\n",
    "\n",
    "for i in range(0, len(data), 1000): # Forma rápida de gerar os arquivos, mesmo sabendo que vou perder algumas linhas.\n",
    "    data.iloc[i:i+1000].to_csv(f'data/housing_{int(i/1000)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee652eb7-00fd-4bc4-8315-67ca8b522f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_4.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_14.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_5.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_1.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_19.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_2.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_20.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_11.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_10.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_12.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_7.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_0.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_8.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_18.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_15.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_3.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_6.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_16.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_13.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_9.csv'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'data/housing_17.csv'>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O método `list_files` localiza um conjunto de arquivos baseado em certo padrão nas strings.\n",
    "# Retorna um dataset com os paths de todos os arquivos que estão de acordo com a string oferecida.\n",
    "filepath_dataset = tf.data.Dataset.list_files('data/housing_*.csv', shuffle=True)\n",
    "list(filepath_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f430140-951b-464e-b870-7a62025b989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A leitura dinâmica dos arquivos pode ser feita com o método `interleave`.\n",
    "\n",
    "# Esse aplica uma função sobre todo o `Dataset` e intercala as suas linhas.\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Por padrão, não há uso de paralelismo. Uma linha de cada dataset é lida por vez.\n",
    "    # Use `num_parallel_calls` se quiser fazer esse processo por multithreading.\n",
    "    num_parallel_calls=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8b52979c-0030-40b4-9668-604c7c8b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'12000,7.5408,3.0,8.493150684931507,1.0342465753424657,519.0,3.5547945205479454,33.93,-117.57,2.719'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'16000,4.0846,52.0,4.821316614420063,0.9561128526645768,819.0,2.5673981191222572,37.74,-122.47,3.336'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3000,4.1484,10.0,4.791907514450867,0.8439306358381503,447.0,2.5838150289017343,35.3,-119.03,1.029'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'11000,13.466,26.0,8.874233128834355,1.0582822085889572,983.0,3.0153374233128836,33.75,-117.79,5.00001'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'9000,4.0,49.0,6.866295264623956,1.0362116991643453,1018.0,2.8356545961002784,34.0,-118.34,2.968'>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja: os valores das features estão todos contidos dentro de uma string bytes.\n",
    "\n",
    "# É necessário ainda fazermos mais um tratamento, coletando os números individualizados.\n",
    "list(dataset.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f9f90-e661-4e56-8d3b-05edf430febf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Preprocessing the Data</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Criaremos uma função que recebe uma linha do dataset e a retorna devidamente tratada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "8226674f-cc32-4523-bc0c-e95db705394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios-padrão das colunas para padronização.\n",
    "X_mean, X_std = X.mean().to_numpy(), X.std().to_numpy()\n",
    "\n",
    "# Função de tratamento dos dados.\n",
    "def preprocess(line):\n",
    "    n_inputs = 8 # Quantidade de colunas presentes.\n",
    "    # Lista com os valores-padrão a serem imputados nas células caso haja NaN's. Um tensor vazio será gerado caso \n",
    "    # o nulo ocorra em uma target-variable, acarretando em um erro.\n",
    "    default_values = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    # Decodificando a linha. 'default_values' será usado na ocorrência de vazios.\n",
    "    fields = tf.io.decode_csv(line, record_defaults=default_values) # `fields é uma lista de tensores que contêm um único número.\n",
    "    # A função `stack` empilha os valores dos tensores em um único tensor unidimensional.\n",
    "    x, y = tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "    # Retornando as variáveis independentes padronizadas juntamente com as dependentes.\n",
    "    return (x-X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "755c26d6-3313-4608-b03d-b11cf5a64473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ -0.8635921 ,  -2.1381242 ,   2.2435777 ,   7.3079667 ,\n",
       "         -1.250785  ,  -0.29468906, -16.66791   ,  69.81657   ],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.41], dtype=float32)>)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'2.23,1.73,10.98,4.56,9.01,0.01,0.03,20.31,5.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b196e-53cb-49c9-9b3c-3ee35055b2b1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Putting Everything Together</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Prefetching</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O método `prefetch` contribui para a performance de nosso programa. Enquanto um dado é processado, o método faz com que a próxima instância (ou batch) passe pelo tratamento em paralelo, economizando tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdae2d-e011-4272-9b99-d428003033f5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='prefetch.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "46c443c4-05ca-4a8c-b21b-20dc543eb3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preparando a próxima instância proveniente de 'range_dataset'.\n",
    "range_dataset.prefetch(1)\n",
    "\n",
    "# Preparando o próximo batch criado a partir de 'range_dataset'.\n",
    "range_dataset.repeat(3).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3178bc10-ea30-4a4a-a397-9e966232ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizando a seção com uma função  que realiza todos os passos demonstrados.\n",
    "def csv_reader_dataset(filepath:str, cycle_length:int=5, buffer_size:int=1000, repeat:int=3, batch_size:int=32):\n",
    "    filepaths = tf.data.Dataset.list_files(filepath) # Coletando todos os arquivos correspondentes ao padrão `filepath`.\n",
    "    # Lendo `cycle_length` arquivos, pulando as suas primeiras linhas. \n",
    "    dataset = filepaths.interleave(lambda x: tf.data.TextLineDataset(x).skip(1), \n",
    "                                   cycle_length=cycle_length, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache() # Tratando os dados e os armazenando em cache.\n",
    "    dataset = dataset.shuffle(buffer_size).repeat(repeat) # Embaralhando e repetindo os dados.\n",
    "    # \n",
    "    return dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25104e2-5397-41bb-b85c-71b0f2ae09b7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Usamos `cache` após o tratamento, mas antes do shuffling e batching. Isso garantirá que a mistura e batches serão distintos a cada iteração. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "12922ca9-ce52-4ba6-9971-5fdcaac4c3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste da função.\n",
    "csv_reader_dataset('data/housing_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df714-107c-4d8f-aaba-316cb0996ea0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using the Dataset with tf.keras</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Com a Pipeline montada, é hora de criar uma rede neural com os dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a1a634c8-f83d-4652-ac2a-09f1a4d6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos de treino, validação e teste.\n",
    "train_paths = [f'data/housing_{i}.csv' for i in range(12)]\n",
    "valid_paths = [f'data/housing_{i}.csv' for i in range(12, 16)]\n",
    "test_paths = [f'data/housing_{i}.csv' for i in range(16, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "bd3be161-ec1b-40fa-95cb-727dd87717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets com os dados.\n",
    "train_set = csv_reader_dataset(train_paths)\n",
    "valid_set = csv_reader_dataset(valid_paths)\n",
    "test_set = csv_reader_dataset(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "e960f022-8e73-40ef-9b23-9ffa1bdd391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c9b6a9fc-3d0b-456b-859e-ecca72d70455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hora demontar o modelo.\n",
    "def data_api_model():\n",
    "    model = keras.models.Sequential()\n",
    "    # Camada de Input.\n",
    "    model.add(keras.layers.Input(shape=[8]))\n",
    "    # Hidden Layers.\n",
    "    for _ in range(5): \n",
    "        model.add(keras.layers.Dense(40, activation='elu', kernel_initializer='lecun_normal'))\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "11242842-0bfd-430a-8271-355dd52160cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expect 9 fields but have 10 in record 0\n\t [[{{node DecodeCSV}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_689758/1073470368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expect 9 fields but have 10 in record 0\n\t [[{{node DecodeCSV}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "x,y = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8087f5e9-2c91-4643-a97e-6ae15049aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nExpect 9 fields but have 10 in record 0\n\t [[{{node DecodeCSV}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_268331]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_689758/3526805451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_api_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nadam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_set\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nExpect 9 fields but have 10 in record 0\n\t [[{{node DecodeCSV}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_268331]"
     ]
    }
   ],
   "source": [
    "model = data_api_model()\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "model.fit(train_set, batch_size=20, epochs=100, validation_data=valid_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71d830-1bf2-4a40-b962-3a7578d58e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe6abf-5e1b-48cc-8a2c-3364a02567e4",
   "metadata": {},
   "source": [
    "<p style='color:red'> Terminar de montar o modelo\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
