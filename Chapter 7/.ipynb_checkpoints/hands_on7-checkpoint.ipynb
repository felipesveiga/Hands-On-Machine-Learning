{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1d68fc-44e9-44dc-b46c-8b0c20003b41",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Ensemble Learning and Random Forests</h1>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O aprendizado Ensemble consiste em fazer previsões embasadas em um conjunto de modelos, ao invés de um único. Essa técnica é inspirada no conceito de <em> sabedoria da multidão</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42021786-4210-422d-ae79-3785fcd182be",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Voting Classifiers</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Um classificador por voto é um conjunto de classificadores diferentes que operam como um só. A previsão final é a mais recorrente entre cada um dos algoritmos individuais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8425be-470a-4e8b-8db5-1be214ad2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcb2057d-e54b-43e6-94ab-02e452de4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.969187675070028\n",
      "KNeighborsClassifier 0.9747899159663865\n",
      "GaussianNB 0.9719887955182073\n",
      "VotingClassifier 0.9859943977591037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "voting_clf = VotingClassifier([\n",
    "    ('log_reg', log_reg),\n",
    "    ('knn', knn),\n",
    "    ('naive_bayes', naive_bayes)\n",
    "], voting='hard')\n",
    "\n",
    "for clf in (log_reg, knn, naive_bayes, voting_clf):\n",
    "    y_pred = clf.fit(X,y)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(clf.__class__.__name__, recall_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa69d75-c145-4d88-ae4f-459d46053b01",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Podemos definir o valor do argumento \"voting\" como soft, caso todos os algoritmos usados possam retornar as probabilidades de classe para cada instância. Isso faz com que as probabilidades de cada classe entre os algoritmos tenham as suas médias calculadas. Ao final, a categoria com a maior probabilidade média será aquela prevista.\n",
    "        </li>\n",
    "        <li> \n",
    "            Alguns classificadores, como o SVC, retornam as probabilidades apenas se o argumento \"probability\" estiver como True. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29705025-16b6-4889-800a-5adb04487100",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bagging and Pasting</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bagging e Pasting representam dois tipos de aprendizado em conjunto. Nos seus casos, várias instâncias de um mesmo algoritmo são treinadas em partições aleatórias do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            No Bagging, a repetição de uma dada instância do conjunto de treino é substituída por outra. Em pasting, duplicatas são permitidas.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em classificação, os ensembles elegem a classe mais prevista entre os modelos individuais. Em regressão, a média das previsões é computada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1fc2f-8680-49d2-85a3-a6a98f9475f8",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Out-of-Bag Evaluation</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Cada previsor que compõe um ensemble é treinado em uma porção restrita do dataset de treino. As instâncias que não o alimentam são denominadas de instâncias out-of-bag (oob). \n",
    "        </li>\n",
    "        <li> \n",
    "            O objeto BaggingClassifier nos permite que cada previsor seja avaliado entre suas oob (oob_score=True), nos fornecendo assim uma validação antecipada do ensemble. Ao final, poderemos extrair a acurácia média obtida.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "782126a8-437e-4565-a295-e45290923747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666080843585237"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O oob Evaluation é apenas possível em classificações 'bagging'. Por isso, sette o argumento 'bootstrap' como True.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "#n_jobs informa ao Python o número de cores do CPU para serem usados no treinamento e previsões.\n",
    "bag_clf = BaggingClassifier(tree_clf, n_estimators=500, bootstrap=True, oob_score=True, n_jobs=-1)\n",
    "bag_clf.fit(X,y)\n",
    "\n",
    "# A acurácia média do Bagging Classifier foi de 96.3%\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5e8048e-1b9d-4582-bf1b-76241a15bae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88823529, 0.11176471],\n",
       "       [0.97938144, 0.02061856],\n",
       "       [1.        , 0.        ],\n",
       "       ...,\n",
       "       [0.97      , 0.03      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01666667, 0.98333333]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'oob_decision_function' neste caso retorna as probabilidades de classe para cada instância.\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3253d4-76eb-4ef1-8b00-e6f31c2f698e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Random Patches and Random Subspaces </h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como em Random Forest, podemos definir uma quantidade máxima de features que cada estimador poderá analisar. Dessa maneira, obteremos uma diversidade ainda maior em nosso ensemble. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c35d2684-8ff1-4b4e-977d-785e6b99cccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507908611599297"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para fazer isso, sette 'max_features' como um float ou int e 'bootstrap_features' como True.\n",
    "bag_clf_features = BaggingClassifier(tree_clf, n_estimators=250, max_features=2, bootstrap_features=True, oob_score=True)\n",
    "bag_clf_features.fit(X,y)\n",
    "\n",
    "# Infelizmente, essa estratégia foi um pouco pior do que a anterior. Mas vale a pena a considerarmos em nossos projetos!\n",
    "bag_clf_features.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81eac-9205-46e3-9f05-222213a637da",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Random Forests</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Random Forests é um algoritmo de Bagging voltado às Árvores de Decisão. A necessidade de se ter um objeto próprio surgiu da aleatoriedade de formação das árvores e de sua tendência a se viciarem ao dataset de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5283e035-9386-4d5a-a848-4d5f9f348bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945518453427065"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# O objeto RandomForestClassfier possui tanto argumentos do DecisionTreeClassifier, quanto do BaggingClassifier.\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 1000, min_impurity_decrease=0.05, max_features=3,n_jobs=-1)\n",
    "rnd_clf.fit(X,y)\n",
    "rnd_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06bae2-f557-4487-b05f-e1f0b53122a2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Extra-Trees</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           O algoritmo de Extra-Trees confere ainda mais aleatoriedade ao ensemble. Dessa vez, os thresholds utilizados na criação dos nós também são escolhidos de maneira aleatória. Isso torna o modelo mais rápido do que o Random Forests, já que ele não precisa perder tempo computando o melhor threshold para os splits.\n",
    "        </li>\n",
    "        <li> \n",
    "            Vale lembrar que os Extra-Trees funcionam tanto para classificação, quanto para regressão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac7306f5-9569-451a-b493-40e25b1ae102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525483304042179"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_clf = ExtraTreesClassifier(n_estimators=1000,max_features=3, max_leaf_nodes=7)\n",
    "extra_clf.fit(X,y)\n",
    "\n",
    "# E veja! Obtivemos um score ainda melhor do que o último Random Forest criado.\n",
    "extra_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55fe9-8c63-4959-aeeb-bde52bba9a54",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Feature Importance</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Outra enorme qualidade dos objetos de ensemble com Árvores de Decisão é a existência do atributo \"feature_importances_\". Ele apresenta o grau de relevância de cada feature do dataset para o algoritmo. Isso, por sua vez, é calculado com base na redução do grau de impureza que o uso de tal feature acarreta.\n",
    "        </li>\n",
    "        <li> \n",
    "            O uso desse atributo pode ser útil em tarefas de limpeza dos DataFrames.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1ffd622-58b1-4101-8ed5-9223608b8bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06035907, 0.0174541 , 0.07361051, 0.0624011 , 0.00837459,\n",
       "       0.03097108, 0.0675814 , 0.09423887, 0.00662938, 0.00209498,\n",
       "       0.02771771, 0.00038206, 0.02903179, 0.03064096, 0.00060547,\n",
       "       0.00562876, 0.00706598, 0.01053226, 0.00096648, 0.00136401,\n",
       "       0.0713237 , 0.02699531, 0.08079771, 0.06790419, 0.01466226,\n",
       "       0.03370015, 0.04694028, 0.09776072, 0.0147883 , 0.00747681])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observando o 'feature_importances_' do Extra-Trees feito.\n",
    "extra_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd611283-f924-48d3-b347-099d2af5f70d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Boosting</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Os algoritmos de Boosting representam uma outra natureza de ensemble. Nela, os previsores são treinados em sequência, com um tentando corrigir os defeitos de seu antecessor.                                                                              \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942485f1-4c89-476d-aab8-fb8f710ade83",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> AdaBoost</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           O foco de um ensemble AdaBoost é o de fazer o previsor melhorar as previsões em instâncias que o seu antecessor errou.          \n",
    "        </li>\n",
    "        <li> \n",
    "            Como todo algoritmo de Boosting, ele possui uma learning rate, que pode ser alterada.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em caso de overfitting, tente reduzir o número de estimadores ou regularize o modelo-base.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aec4d558-b56d-4f09-86fe-397581db2b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O algoritmo-padrão de montagem de um Adaboost é o SAMME.R. Utilize-o se o algoritmo-base poder retornar probabilidades de classe.\n",
    "# Caso o contrário, sette 'algorithm' como 'SAMME'.\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=500,\n",
    "                            algorithm='SAMME.R', learning_rate=.5)\n",
    "\n",
    "# Há um overfitting! Em um projeto real, deveríamos buscar soluções como as mencionadas.\n",
    "ada_clf.fit(X,y)\n",
    "ada_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b89920-02a6-42b3-8976-0e91746d8afc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Gradient Boosting</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           A estratégia do Gradient Boosting é focar nos erros residuais produzidos pelos estimadores. Em regressão, para cada instância, o valor retornado pelo ensemble é a soma das previsões de cada estimador.       \n",
    "        </li>\n",
    "        <li> \n",
    "            Suponha que o valor-alvo de uma instância seja 100, mas o seu primeiro previsor tenha estimado um valor de 95. Nesse contexto, há uma diferença de 5 unidades entre o verdadeiro número e a previsão. O segundo modelo será treinado tendo agora, para aquela instância, um valor-alvo de 5 (valor_alvo_real - estimativa_ultimo_algoritmo). Caso o algoritmo acerte a previsão, a estimativa geral do ensemble será de 95+5=100, ou seja, o valor que buscávamos chegar!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4c044c-b0b6-4d64-be75-2e22f297c402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18.10766436])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Um Gradient Boosting caseiro.\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X,y = make_regression(n_features=2)\n",
    "\n",
    "# Primeiro algoritmo do ensemble e sua previsão.\n",
    "tree1 = DecisionTreeRegressor(max_depth=2)\n",
    "y2 = y - tree1.fit(X,y).predict(X)\n",
    "\n",
    "# Segundo algoritmo, agora treinado sobre os erros do primeiro.\n",
    "tree2 = DecisionTreeRegressor(max_depth=2)\n",
    "y3 = y2 - tree2.fit(X,y2).predict(X)\n",
    "\n",
    "# Último modelo.\n",
    "tree3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree3.fit(X,y3)\n",
    "\n",
    "# Fazendo uma previsão para uma certa instância do dataset.\n",
    "y_pred = sum(tree.predict(X[0].reshape(1, -1)) for tree in (tree1, tree2, tree3))\n",
    "\n",
    "# Por quanto o ensemble errou?\n",
    "y[0] - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8541b034-24e1-44b1-80e0-8c0f8a9abde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3099716666509157"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas, como é de se esperar, o sklearn já possui uma classe de Gradient Boosting. Mais precisamente, essa consiste em um ensemble\n",
    "# de árvores de decisão.\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=50, learning_rate=1.0)\n",
    "gbrt.fit(X,y)\n",
    "\n",
    "mean_squared_error(y, gbrt.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efed9877-d6d3-4f7e-b52c-cdbf16172a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005180717412484529"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Você pode fazer com que cada árvore seja treinada em um pedaço aleatório do dataset de treino com 'subsample'.\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=5000, learning_rate=0.05, subsample=0.15)\n",
    "gbrt.fit(X,y)\n",
    "\n",
    "mean_squared_error(y, gbrt.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee60c0-2b7d-4d9c-ad95-a17c1441f9ae",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Os objetos de Gradient Boosting também contam com o método 'stage_predict', que mostra as previsões feitas pelas árvores de cada iteração. Pode ser bastante útil em implementações de early stopping.      \n",
    "        </li>\n",
    "        <li>\n",
    "            Lembre-se, assim como em modelos de regressão, os de Gradient Boosting podem não nos oferecer a solução ótima, pois o seu treinamento é interrompido apenas quando a taxa de erro do algoritmo começa a subir. Dessa maneira, é interessante pensar em fazer um early stopping a fim de buscarmos o melhor modelo o possível.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25190d4-e65e-4817-a550-1e2abf4e535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43.52422393  43.52422393 -41.35910897  43.52422393 -41.35910897\n",
      "   0.13613748  43.52422393   0.13613748   0.13613748 113.08344214\n",
      "   0.13613748   0.13613748   0.13613748 113.08344214   0.13613748\n",
      " -41.35910897   0.13613748   0.13613748  43.52422393 113.08344214\n",
      " -41.35910897 -41.35910897   0.13613748  43.52422393   0.13613748\n",
      "   0.13613748  43.52422393 -41.35910897  43.52422393   0.13613748\n",
      "  43.52422393 -41.35910897  43.52422393 -41.35910897 -41.35910897\n",
      " 113.08344214  43.52422393   0.13613748  43.52422393  43.52422393\n",
      " -41.35910897   0.13613748 -41.35910897  43.52422393   0.13613748\n",
      "   0.13613748 -41.35910897 113.08344214  43.52422393 -41.35910897\n",
      "  43.52422393   0.13613748 -41.35910897 -41.35910897 -41.35910897\n",
      " -41.35910897  43.52422393 -41.35910897  43.52422393  43.52422393\n",
      " -41.35910897 -41.35910897  43.52422393 -41.35910897   0.13613748\n",
      " -41.35910897 -41.35910897 -41.35910897  43.52422393   0.13613748\n",
      "  43.52422393 -41.35910897  43.52422393 -41.35910897 -41.35910897\n",
      "  43.52422393  43.52422393 -41.35910897 -41.35910897 -41.35910897\n",
      "  43.52422393  43.52422393  43.52422393 -41.35910897  43.52422393\n",
      "   0.13613748  43.52422393 -41.35910897 -41.35910897 113.08344214\n",
      "  43.52422393  43.52422393   0.13613748 -41.35910897 -41.35910897\n",
      "  43.52422393   0.13613748 -41.35910897   0.13613748  43.52422393]\n"
     ]
    }
   ],
   "source": [
    "# Pegando apenas o primeiro round de previsões.\n",
    "a= 0\n",
    "for i in gbrt.staged_predict(X):\n",
    "    if a<1:\n",
    "        print(i)\n",
    "        a+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9012f5-dacb-499a-96ca-56b94a9a2a64",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> XGBoost</h4>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           A biblioteca XGBoost é bastante recomendável em implementações de Gradient Boostings. Suas vantagens são a sua velocidade, escalabilidade e a existência da opção de fazermos early stoppings.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d5aa80-8c62-45e2-a0de-3f2ac163e2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014715955498924103"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_reg = XGBRegressor(max_depth=2, n_estimators=5000, learning_rate=0.05, subsample=0.15).fit(X,y)\n",
    "mean_squared_error(y, xgb_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637a705d-ff2e-42e3-a19f-94b96b23047e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84587985, 0.15412016], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hpa muitas similaridades entre os objetos do XGBoost e do sklearn!\n",
    "xgb_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c72ec44-1cdd-4fa4-8bb2-4f15573785f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 192.9 MB 85 kB/s  eta 0:00:01    |█▍                              | 8.4 MB 145 kB/s eta 0:21:06     |███████▌                        | 45.5 MB 39 kB/s eta 1:02:49     |█████████████▋                  | 82.0 MB 251 kB/s eta 0:07:22     |█████████████▋                  | 82.0 MB 153 kB/s eta 0:12:03     |████████████████▊               | 100.9 MB 330 kB/s eta 0:04:39     |█████████████████               | 101.8 MB 210 kB/s eta 0:07:12     |█████████████████▏              | 103.6 MB 324 kB/s eta 0:04:35     |█████████████████▋              | 106.0 MB 76 kB/s eta 0:18:50     |████████████████████            | 120.0 MB 166 kB/s eta 0:07:19     |████████████████████            | 121.1 MB 247 kB/s eta 0:04:50     |█████████████████████▎          | 128.4 MB 2.7 MB/s eta 0:00:25     |█████████████████████▋          | 130.1 MB 251 kB/s eta 0:04:10     |██████████████████████▏         | 133.4 MB 111 kB/s eta 0:08:53     |████████████████████████▍       | 147.1 MB 335 kB/s eta 0:02:17     |███████████████████████████▏    | 163.4 MB 328 kB/s eta 0:01:30     |█████████████████████████████   | 175.1 MB 101 kB/s eta 0:02:55     |██████████████████████████████▌ | 184.0 MB 200 kB/s eta 0:00:45     |███████████████████████████████ | 186.5 MB 506 kB/s eta 0:00:13     |███████████████████████████████▎| 188.6 MB 281 kB/s eta 0:00:16     |███████████████████████████████▍| 189.3 MB 166 kB/s eta 0:00:22\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/veiga/.local/lib/python3.8/site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in /home/veiga/.local/lib/python3.8/site-packages (from xgboost) (1.22.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4e65e-eb31-4f7f-8204-2d78735026b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22bb860b-f6d7-415b-a535-451dcf21975e",
   "metadata": {},
   "source": [
    "<p style='color:red'> Gradient Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
