{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1d68fc-44e9-44dc-b46c-8b0c20003b41",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Ensemble Learning and Random Forests</h1>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O aprendizado Ensemble consiste em fazer previsões embasadas em um conjunto de modelos, ao invés de um único. Essa técnica é inspirada no conceito de <em> sabedoria da multidão</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42021786-4210-422d-ae79-3785fcd182be",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Voting Classifiers</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Um classificador por voto é um conjunto de classificadores diferentes que operam como um só. A previsão final é a mais recorrente entre cada um dos algoritmos individuais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8425be-470a-4e8b-8db5-1be214ad2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb2057d-e54b-43e6-94ab-02e452de4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.969187675070028\n",
      "KNeighborsClassifier 0.9747899159663865\n",
      "GaussianNB 0.9719887955182073\n",
      "VotingClassifier 0.9859943977591037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "voting_clf = VotingClassifier([\n",
    "    ('log_reg', log_reg),\n",
    "    ('knn', knn),\n",
    "    ('naive_bayes', naive_bayes)\n",
    "], voting='hard')\n",
    "\n",
    "for clf in (log_reg, knn, naive_bayes, voting_clf):\n",
    "    y_pred = clf.fit(X,y)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(clf.__class__.__name__, recall_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa69d75-c145-4d88-ae4f-459d46053b01",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Podemos definir o valor do argumento \"voting\" como soft, caso todos os algoritmos usados possam retornar as probabilidades de classe para cada instância. Isso faz com que as probabilidades de cada classe entre os algoritmos tenham as suas médias calculadas. Ao final, a categoria com a maior probabilidade média será aquela prevista.\n",
    "        </li>\n",
    "        <li> \n",
    "            Alguns classificadores, como o SVC, retornam as probabilidades apenas se o argumento \"probability\" estiver como True. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29705025-16b6-4889-800a-5adb04487100",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bagging and Pasting</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bagging e Pasting representam dois tipos de aprendizado em conjunto. Nos seus casos, várias instâncias de um mesmo algoritmo são treinadas em partições aleatórias do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            No Bagging, a repetição de uma dada instância do conjunto de treino é substituída por outra. Em pasting, duplicatas são permitidas.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em classificação, os ensembles elegem a classe mais prevista entre os modelos individuais. Em regressão, a média das previsões é computada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1fc2f-8680-49d2-85a3-a6a98f9475f8",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Out-of-Bag Evaluation</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Cada previsor que compõe um ensemble é treinado em uma porção restrita do dataset de treino. As instâncias que não o alimentam são denominadas de instâncias out-of-bag (oob). \n",
    "        </li>\n",
    "        <li> \n",
    "            O objeto BaggingClassifier nos permite que cada previsor seja avaliado entre suas oob (oob_score=True), nos fornecendo assim uma validação antecipada do ensemble. Ao final, poderemos extrair a acurácia média obtida.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "782126a8-437e-4565-a295-e45290923747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630931458699473"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O oob Evaluation é apenas possível em classificações 'bagging'. Por isso, sette o argumento 'bootstrap' como True.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "#n_jobs informa ao Python o número de cores do CPU para serem usados no treinamento e previsões.\n",
    "bag_clf = BaggingClassifier(tree_clf, n_estimators=500, bootstrap=True, oob_score=True, n_jobs=-1)\n",
    "bag_clf.fit(X,y)\n",
    "\n",
    "# A acurácia média do Bagging Classifier foi de 96.3%\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5e8048e-1b9d-4582-bf1b-76241a15bae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84656085, 0.15343915],\n",
       "       [0.98305085, 0.01694915],\n",
       "       [1.        , 0.        ],\n",
       "       ...,\n",
       "       [0.97159091, 0.02840909],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0060241 , 0.9939759 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'oob_decision_function' neste caso retorna as probabilidades de classe para cada instância.\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3253d4-76eb-4ef1-8b00-e6f31c2f698e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Random Patches and Random Subspaces </h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como em Random Forest, podemos definir uma quantidade máxima de features que cada estimador poderá utilizar. Dessa maneira, obteremos uma diversidade ainda maior em nosso ensemble. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c35d2684-8ff1-4b4e-977d-785e6b99cccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945518453427065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para fazer isso, sette 'max_features' como um float ou int e 'bootstrap_features' como True.\n",
    "bag_clf_features = BaggingClassifier(tree_clf, n_estimators=250, max_features=2, bootstrap_features=True, oob_score=True)\n",
    "bag_clf_features.fit(X,y)\n",
    "\n",
    "# Infelizmente, essa estratégia foi um pouco pior do que a anterior. Mas vale a pena a considerarmos em nossos projetos!\n",
    "bag_clf_features.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81eac-9205-46e3-9f05-222213a637da",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Random Forests</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Random Forests é um algoritmo de Bagging voltado às Árvores de Decisão. A necessidade de se ter um objeto próprio surgiu da aleatoriedade de formação das árvores e de sua tendência de vício ao dataset de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5283e035-9386-4d5a-a848-4d5f9f348bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420035149384886"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# O objeto RandomForestClassfier possui tanto argumentos do DecisionTreeClassifier, quanto do BaggingClassifier.\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 1000, min_impurity_decrease=0.05, max_features=3,n_jobs=-1)\n",
    "rnd_clf.fit(X,y)\n",
    "rnd_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06bae2-f557-4487-b05f-e1f0b53122a2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Extra-Trees</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb860b-f6d7-415b-a535-451dcf21975e",
   "metadata": {},
   "source": [
    "<p style='color:red'> Fazer a explicação do Extra-Trees (p.200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
