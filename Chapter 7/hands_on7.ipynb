{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1d68fc-44e9-44dc-b46c-8b0c20003b41",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Ensemble Learning and Random Forests</h1>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O aprendizado Ensemble consiste em fazer previsões embasadas em um conjunto de modelos, ao invés de um único. Essa técnica é inspirada no conceito de <em> sabedoria da multidão</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42021786-4210-422d-ae79-3785fcd182be",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Voting Classifiers</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Um classificador por voto é um conjunto de classificadores diferentes que operam como um só. A previsão final é a mais recorrente entre cada um dos algoritmos individuais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8425be-470a-4e8b-8db5-1be214ad2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb2057d-e54b-43e6-94ab-02e452de4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.969187675070028\n",
      "KNeighborsClassifier 0.9747899159663865\n",
      "GaussianNB 0.9719887955182073\n",
      "VotingClassifier 0.9859943977591037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "voting_clf = VotingClassifier([\n",
    "    ('log_reg', log_reg),\n",
    "    ('knn', knn),\n",
    "    ('naive_bayes', naive_bayes)\n",
    "], voting='hard')\n",
    "\n",
    "for clf in (log_reg, knn, naive_bayes, voting_clf):\n",
    "    y_pred = clf.fit(X,y)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(clf.__class__.__name__, recall_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa69d75-c145-4d88-ae4f-459d46053b01",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Podemos definir o valor do argumento \"voting\" como soft, caso todos os algoritmos usados possam retornar as probabilidades de classe para cada instância. Isso faz com que as probabilidades de cada classe entre os algoritmos tenham as suas médias calculadas. Ao final, a categoria com a maior probabilidade média será aquela prevista.\n",
    "        </li>\n",
    "        <li> \n",
    "            Alguns classificadores, como o SVC, retornam as probabilidades apenas se o argumento \"probability\" estiver como True. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29705025-16b6-4889-800a-5adb04487100",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bagging and Pasting</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bagging e Pasting representam dois tipos de aprendizado em conjunto. Nos seus casos, várias instâncias de um mesmo algoritmo são treinadas em partições aleatórias do dataset.\n",
    "        </li>\n",
    "        <li> \n",
    "            No Bagging, a repetição de uma dada instância do conjunto de treino é substituída por outra. Em pasting, duplicatas são permitidas.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em classificação, os ensembles elegem a classe mais prevista entre os modelos individuais. Em regressão, a média das previsões é computada.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1fc2f-8680-49d2-85a3-a6a98f9475f8",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Out-of-Bag Evaluation</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Cada previsor que compõe um ensemble é treinado em uma porção restrita do dataset de treino. As instâncias que não o alimentam são denominadas de instâncias out-of-bag (oob). \n",
    "        </li>\n",
    "        <li> \n",
    "            O objeto BaggingClassifier nos permite que cada previsor seja avaliado entre suas oob (oob_score=True), nos fornecendo assim uma validação antecipada do ensemble. Ao final, poderemos extrair a acurácia média obtida.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "782126a8-437e-4565-a295-e45290923747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630931458699473"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O oob Evaluation é apenas possível em classificações 'bagging'. Por isso, sette o argumento 'bootstrap' como True.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "#n_jobs informa ao Python o número de cores do CPU para serem usados no treinamento e previsões.\n",
    "bag_clf = BaggingClassifier(tree_clf, n_estimators=500, bootstrap=True, oob_score=True, n_jobs=-1)\n",
    "bag_clf.fit(X,y)\n",
    "\n",
    "# A acurácia média do Bagging Classifier foi de 96.3%\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5e8048e-1b9d-4582-bf1b-76241a15bae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84656085, 0.15343915],\n",
       "       [0.98305085, 0.01694915],\n",
       "       [1.        , 0.        ],\n",
       "       ...,\n",
       "       [0.97159091, 0.02840909],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0060241 , 0.9939759 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'oob_decision_function' neste caso retorna as probabilidades de classe para cada instância.\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3253d4-76eb-4ef1-8b00-e6f31c2f698e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Random Patches and Random Subspaces </h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como em Random Forest, podemos definir uma quantidade máxima de features que cada estimador poderá analisar. Dessa maneira, obteremos uma diversidade ainda maior em nosso ensemble. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c35d2684-8ff1-4b4e-977d-785e6b99cccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945518453427065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para fazer isso, sette 'max_features' como um float ou int e 'bootstrap_features' como True.\n",
    "bag_clf_features = BaggingClassifier(tree_clf, n_estimators=250, max_features=2, bootstrap_features=True, oob_score=True)\n",
    "bag_clf_features.fit(X,y)\n",
    "\n",
    "# Infelizmente, essa estratégia foi um pouco pior do que a anterior. Mas vale a pena a considerarmos em nossos projetos!\n",
    "bag_clf_features.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81eac-9205-46e3-9f05-222213a637da",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Random Forests</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Random Forests é um algoritmo de Bagging voltado às Árvores de Decisão. A necessidade de se ter um objeto próprio surgiu da aleatoriedade de formação das árvores e de sua tendência a se viciarem ao dataset de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5283e035-9386-4d5a-a848-4d5f9f348bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420035149384886"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# O objeto RandomForestClassfier possui tanto argumentos do DecisionTreeClassifier, quanto do BaggingClassifier.\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 1000, min_impurity_decrease=0.05, max_features=3,n_jobs=-1)\n",
    "rnd_clf.fit(X,y)\n",
    "rnd_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06bae2-f557-4487-b05f-e1f0b53122a2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Extra-Trees</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           O algoritmo de Extra-Trees confere ainda mais aleatoriedade ao ensemble. Dessa vez, os thresholds utilizados na criação dos nós também são escolhidos de maneira aleatória. Isso torna o modelo mais rápido do que o Random Forests, já que ele não precisa perder tempo computando o melhor threshold para os splits.\n",
    "        </li>\n",
    "        <li> \n",
    "            Vale lembrar que os Extra-Trees funcionam tanto para classificação, quanto para regressão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac7306f5-9569-451a-b493-40e25b1ae102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507908611599297"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_clf = ExtraTreesClassifier(n_estimators=1000,max_features=3, max_leaf_nodes=7)\n",
    "extra_clf.fit(X,y)\n",
    "\n",
    "# E veja! Obtivemos um score ainda melhor do que o último Random Forest criado.\n",
    "extra_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55fe9-8c63-4959-aeeb-bde52bba9a54",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Feature Importance</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Outra enorme qualidade dos objetos de ensemble com Árvores de Decisão é a existência do atributo \"feature_importances_\". Ele apresenta o grau de relevância de cada feature do dataset para o algoritmo. Isso, por sua vez, é calculado com base na redução do grau de impureza que o uso de tal feature acarreta.\n",
    "        </li>\n",
    "        <li> \n",
    "            O uso desse atributo pode ser útil em tarefas de limpeza dos DataFrames.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1ffd622-58b1-4101-8ed5-9223608b8bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06026044, 0.01538051, 0.07240843, 0.05797074, 0.00934074,\n",
       "       0.03516164, 0.05926578, 0.08247204, 0.00772602, 0.00257745,\n",
       "       0.03292132, 0.00070543, 0.02685925, 0.03482733, 0.00101769,\n",
       "       0.00621144, 0.00841838, 0.0072371 , 0.00142528, 0.00158883,\n",
       "       0.07618602, 0.02329403, 0.07440795, 0.07211935, 0.02041379,\n",
       "       0.03340561, 0.06021188, 0.09254465, 0.01607732, 0.00756358])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observando o 'feature_importances_' do Extra-Trees feito.\n",
    "extra_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd611283-f924-48d3-b347-099d2af5f70d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Boosting</h2>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Os algoritmos de Boosting representam uma outra natureza de ensemble. Nela, os previsores são treinados em sequência, com um tentando corrigir os defeitos de seu antecessor.                                                                              \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb860b-f6d7-415b-a535-451dcf21975e",
   "metadata": {},
   "source": [
    "<p style='color:red'> AdaBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
