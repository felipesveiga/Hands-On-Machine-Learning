{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9d226c-dc57-4d07-ae9d-bf8972506b4e",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2616b-f8bb-4bed-be9f-adaeeda1e40c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Esclarecimentos</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Este é o meu segundo estudo sobre o quarto capítulo do livro. O objetivo deste registro é, agora, consolidar o meu entendimento sobre a esfera matemática do treinamento em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1e84-4614-494c-9762-6f2573d902a3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Notations (p.43)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            m é o tamanho do dataset com o qual avaliamos o modelo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se o set de validação tem 5000 instâncias, $m=5000$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $x^{(i)}$ é o vetor com os valores de todas as features independentes da i-ésima instância do dataset. $y^{(i)}$ é a sua variável-alvo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se a nona feature do dataset tem como features independentes 200, 4500, 100 e 9 $x^{(9)}=\\begin{bmatrix} 200\\\\4500\\\\1000\\\\9\\end{bmatrix}$. \n",
    "                </li>\n",
    "                <li> \n",
    "                    Caso sua target-variable seja 3, $y^{(9)}=\\begin{bmatrix}3\\end{bmatrix}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            X é a matrix com os valores das features independentes de cada instância. Esses, por sua vez, estão contidos em um vetor-linha, e não coluna. Portanto, para um dataset com 2000 instâncias:\n",
    "            $$\n",
    "                X=\\begin{bmatrix} \n",
    "                (x^{(1)})^{T} \\\\\n",
    "                (x^{(2)})^{T} \\\\\n",
    "                \\vdots \\\\\n",
    "                (x^{(2000)})^{T}\n",
    "                \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $h$ representa o nosso modelo de ML, que, para cada instância, lança uma previsão $ŷ$. Dessa maneira, $h(x^{(i)})=ŷ^{(i)}$\n",
    "             <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    O erro absoluto de um algoritmo regressor será $ŷ^{(i)}-y^{(i)}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $RMSE(X, h)$, $R^{2}(X,h)$ representam a função custo aplicada no set $X$ com o modelo $h$. \n",
    "        </li>\n",
    "        <li> \n",
    "            $J(\\theta)$ se referirá a qualquer função-custo com nome muito extenso. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8acb2-9142-4cc9-a0c6-0f8456501f98",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Métricas de Regressão</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             RMSE, também conhecida como Euclidean Norm ou $l_2$ norm. Sua fórmula é:\n",
    "            $$\n",
    "            RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            MAE, também conhecida como Manhattan Norm ou $l_1$ norm. Sua fórmula é:\n",
    "            $$\n",
    "                    MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m} |h(x^{(i)})-y^{(i)}| \n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f10c4-aa3b-4e1a-94c6-96e601fd087e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Início de Fato do Capítulo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Neste capítulo, aprenderemos os principais métodos de treinamento em ML, assim como alguns modelos mais simples.\n",
    "        </li>\n",
    "        <li> \n",
    "            Há maneiras mais diretas de alcançarmos os parâmetros ideais do modelo, e outras que envolvem iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb59b-7e9c-4c45-ac63-f5260c0db7ff",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Linear Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Linear é o modelo mais simples do ML. Consiste na soma ponderada de cada feature por um coeficiente. Uma previsão ŷ é dada como:\n",
    "            <p>$\n",
    "                ŷ=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $ ($\\theta_0$ é o termo de bias).\n",
    "            </p>         \n",
    "        </li>\n",
    "        <li> \n",
    "            Uma maneira mais concisa de formular essa equação é:\n",
    "            $$\n",
    "                ŷ=h_{\\theta}(x)=\\theta \\cdot x\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    $\\theta$ é o vetor com os coeficientes do modelo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    x é o vetor com os valores das features.\n",
    "                </li>\n",
    "                <li> \n",
    "                    $\\theta \\cdot x$ é o produto escalar entre os vetores mencionados, igual a $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A previsão do modelo ainda pode ser dada como $ŷ=\\theta^{T}x$, que é a multiplicação entre a transposta dos coeficientes e x.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b02d65-c5e2-41af-88b0-240a6d07501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016455765915880542, 0.016455765915880542)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "theta = np.random.random(size=3)\n",
    "x = np.random.normal(size=3)\n",
    "\n",
    "# Observe como o dot product e a multiplicação matricial se equivalem.\n",
    "theta.dot(x), theta.T@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f35b-7e12-4ea8-9840-07a6dfed8477",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A criação de uma Regressão Linear envolve encontrar o $\\theta$ que minimiza o MSE. Agora, é hora de explorarmos os diferentes métodos para que isso ocorra.        \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d21c99-f22f-4ad4-b84e-4f3edd712308",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Normal Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1914c93-553b-4b44-8413-dff53028206f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Equação Normal é uma maneira direta de alcançarmos o $\\theta$ minimizador do MSE. A sua fórmula é:\n",
    "            $$\n",
    "                \\theta=(X^{T}X)^{-1}X^{T}y\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    y é o vetor com as target variables.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adb2587-e96d-4b5b-8ae7-4d1333c2b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97970517],\n",
       "       [4.0322335 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Equação Normal com o Numpy.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 10 + 4*X - np.random.normal(size=(1000,1))\n",
    "\n",
    "# Adicionando a coluna de bias a X.\n",
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T@X_b) @ X_b.T @ y\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f8e0d1-36cf-43c3-8d4b-6959bb626488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.14087266]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo uma previsão com uma nova instância.\n",
    "X_new = np.array([5])\n",
    "X_new_b = np.c_[np.ones((1,1)), X_new]\n",
    "\n",
    "X_new_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2b34e4-7734-4bfe-94d5-8fdf737b701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97970517],\n",
       "       [4.0322335 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa mesma equação poderia ser escrita com os dot products entre as matrizes.\n",
    "np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400cb21-7938-416d-8abb-228bfbc5e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.97970517]), array([[4.0322335]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'LinearRegression' obtém os mesmos parâmetros.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397930-54b7-4070-a66f-d001487c7b52",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Moore-Penrose Inverse</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             Uma outra maneira de obtermos os coeficientes da nossa Regressão Linear é com o produto escalar entre a matriz Moore-Penrose de X (conhecida também como pseudoinversa) e y.\n",
    "            $$\n",
    "                \\theta=X^{+}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603d0744-cdf7-4734-8b9a-5417c8e67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97970517],\n",
       "       [4.0322335 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'np.linalg.pinv' para computar a pseudoinversa de X.\n",
    "theta_best = np.linalg.pinv(X_b).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c43f5-4553-4a14-b4ff-4cf12d30cfee",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Computational Complexity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Time Complexity para treinamento de uma Regressão Linear com a Normal Equation pode ser de $O(n^{2.4})$ a $O(n^{3})$ e $O(n^{2})$ com a Inversa Moore-Penrose (sendo n o número de features). Ambos os métodos são $O(m)$ (m é o número de instâncias).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com previsões, as duas estratégias têm $O(n)$ e $O(m)$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa20fd-c544-425f-b5b6-475275b08d05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gradient Descent</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O autor introduz os conceitos de Descida de Gradiente e learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30555-4c42-4704-ba61-21d3dfd8342b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Batch Gradient Descent consiste em fazer a descida de gradiente com base em todo o conjunto de treino. O Gradiente de uma MSE para a Regressão Linear é dado como:\n",
    "            <p style='margin-top:10px'>\n",
    "            $$\n",
    "                \\nabla_{\\theta}MSE(\\theta)=\n",
    "                    \\begin{bmatrix} \n",
    "                        \\frac{\\delta}{\\delta\\theta_{0}}MSE(\\theta) \\\\ \n",
    "                        \\frac{\\delta}{\\delta\\theta_{1}}MSE(\\theta) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\frac{\\delta}{\\delta\\theta_{n}}MSE(\\theta) \n",
    "                    \\end{bmatrix}                    \n",
    "                    = \\frac{2}{m}X^{T}(X\\theta-y)\n",
    "            $$\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bf40b5-f24a-4f74-884c-0e50852f8f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97970515],\n",
       "       [4.03223348]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Batch Gradient Descent em código.\n",
    "\n",
    "eta = .01 # learning rate.\n",
    "n_iterations = 1000 # número de iterações total.\n",
    "\n",
    "# Inicialização aleatória dos parâmetros (segundo uma Distribuição Normal).\n",
    "theta = np.random.uniform(size=(2,1))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    #print(gradient, end='\\n\\n')\n",
    "    theta -=  eta*gradient\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21756b1-75fb-4561-b490-8f46169df9e7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stochastic Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Descida de Gradiente Estocástica surge pela demora na computação do Gradiente com o set de treino completo. Esse novo método propõe selecionar uma única instância aleatória para a conta.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, por outro lado, a descida de gradiente será menos regular. Outliers também ganham um peso maior na computação, o dado da iteração pode ser um desses. \n",
    "        </li>\n",
    "        <li> \n",
    "            Como o treinamento é mais instável, raramente alcançaremos a solução ótima, e sim, algo próximo. Uma estratégia para SGD é definir uma alta learning rate no início do fitting, para escaparmos dos mínimos locais, e reduzi-la, gradativamente, a fim de tentarmos nos alocar no mínimo global; se estivermos, de fato, na solução ótima, o vetor de gradiente estará severamente podado pelo menor $\\eta$, impedindo de escaparmos dela.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: O scikit-learn oferece uma série de outras estratégias para tratamento das learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b8f008-8748-4fb3-b1f9-523258ce4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O treinamento é bastante similar ao de uma Rede Neural. À cada época, uma certa quantidade de instâncias são selecionadas para\n",
    "# o ajuste dos parâmetros.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SGDLinearRegression(RegressorMixin):\n",
    "    # 'theta' stores the LR coefficients.\n",
    "    theta = None\n",
    "    def __init__(self, eta0=.05, epochs=20, validation_split=.1, tol=1e-3, n_iter_no_change=3):\n",
    "        self.eta0 = eta0 # Initial Learning Rate.\n",
    "        self.epochs = epochs  # Number of Epochs.\n",
    "        self.validation_split = validation_split # % of training set held for validation.\n",
    "        self.tol = tol # Tolerance.\n",
    "        self.n_iter_no_change = n_iter_no_change # Number of epochs with no improvement for training halt.\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        SGDLinearRegression.theta = np.random.rand(X.shape[1],1)\n",
    "        # Segregating training from validation data.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = self.validation_split)\n",
    "        # Storing the 'theta' from the best epoch (lowest 'best_mse'). \n",
    "        best_theta, self.best_mse = SGDLinearRegression.theta, np.inf\n",
    "        no_change = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X_train)):\n",
    "                # Randomly selecting the epoch's iteration instance.\n",
    "                index = np.random.randint(len(X_train))\n",
    "                xi = X_train[index:index+1]\n",
    "                yi = y_train[index:index+1]\n",
    "                # Computing the gradient based on the instance's data.\n",
    "                gradients = (2 * xi.T.dot(xi.dot(SGDLinearRegression.theta ) - yi))\n",
    "                SGDLinearRegression.theta = SGDLinearRegression.theta - (gradients * self.eta0)\n",
    "            # At the end of the current epoch, the model performs a validation on 'X_val' and 'y_val'.\n",
    "            # If no improvement is occurs, the learning rate is divided by 5.\n",
    "            MSE = mean_squared_error(y_val, X_val.dot(SGDLinearRegression.theta))\n",
    "            if MSE > self.best_mse - self.tol:\n",
    "                no_change+=1\n",
    "                self.eta0 /= 5\n",
    "                # If the number of consecutive epochs with no MSE decay reaches 'n_iter_no_change', training is interrupted and the model\n",
    "                # is re-adjusted with the coefficients from the best epoch.\n",
    "                if no_change == self.n_iter_no_change:\n",
    "                    SGDLinearRegression.theta =  best_theta\n",
    "                    return self\n",
    "            else:\n",
    "                no_change = 0\n",
    "                self.best_mse = MSE\n",
    "                best_theta = SGDLinearRegression.theta            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e55553-7396-45f5-9873-943cf0a4c357",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Mini-Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Mini-Batch GD surge como um meio termo entre os dois métodos já apresentados. Sua estratégia é medir, à cada iteração, o vetor de gradiente com base em um conjunto de instâncias aleatórias (e não apenas uma, como no SGD).\n",
    "        </li>\n",
    "        <li> \n",
    "            Essa abordagem torna a descida de gradiente mais estável e com uma sensibilidade menor à presença de outliers.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ffb697-ce53-4d98-bfd9-a11d208bb241",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Polynomial Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, é apresentada a classe PolynomialFeatures, responsável por aumentar a dimensionalidade do dataset com base no produto entre as features.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f3865-2c8f-424b-92ff-6122d88733d3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Learning Curves</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As Curvas de Aprendizado surgem como um recurso para analisarmos, em primeiro lugar, o quanto de instâncias seriam adequadas para que o modelo atinja o seu pleno potencial e, também, se está ocorrendo um under ou overfitting.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87cbaf7-97e9-4044-8dbe-a42c174ed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Criando novas features.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 2.5*X**2 - 4*X + 5 + np.random.normal()\n",
    "\n",
    "_, train_mse, test_mse = learning_curve(LinearRegression(), X, y, cv=5, scoring='neg_mean_squared_error', \n",
    "                                        train_sizes=np.linspace(.05, .75, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42caaeab-0efe-4742-8a95-0a4c9811a2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f774d340be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Observe, como treinamos uma Regressão Linear em uma função quadrática, recebemos um erro elevado nos sets de treino e teste.\n",
    "# Ou seja, há um underfitting.\n",
    "plt.plot(_, np.sqrt(-train_mse)[:, 0], label='Train')\n",
    "plt.plot(_, np.sqrt(-test_mse)[:, 0], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca35030-d7b2-4d4b-afaa-4c33a040fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f774cee8760>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agora, forçaremos um overfitting treinando uma árvore de decisão sem regularizações.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "_, train_mse, test_mse = learning_curve(DecisionTreeRegressor(), X, y, train_sizes=np.linspace(.01, .75, 11), cv=5,\n",
    "                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# O overfitting pode ser detectado com altas performances no set de treino e baixas no de teste. O resultado poderia ter sido mais\n",
    "# expressivo por aqui, mas fica como um exemplo.\n",
    "plt.plot(_, np.sqrt(-train_mse)[:, 0], label='Train')\n",
    "plt.plot(_, np.sqrt(-test_mse)[:, 0], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd42a6-88e2-419e-bad8-6084064f9cc3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bias/Variance Tradeoff</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            <u>Bias</u>: seria a incapacidade de um modelo de se ajustar à toda complexidade do dataset. Algoritmos como a Regressão Linear têm um alto nível de bias por apenas conseguirem capturar relações estritamente lineares; é provável que haja um underfitting ao usá-los.\n",
    "        </li>\n",
    "        <li> \n",
    "            <u> Variance</u> consiste na elevada variância qualitativa das previsões em diferentes datasets. Isso, normalmente, ocorre pelo uso de um modelo muito complexo, que se superadequa aos padrões do set de treino e, com isso, perde o seu poder de genealização.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee771191-54e9-495e-bc9d-f580ce0b666f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Norm</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A norma de um vetor consiste em um número indicando a magnitude desse. É na p-ésima raiz da soma das dimensões de um vetor elevadas à p-ésima potência. Sua notação é $||X||_p$ \n",
    "        </li>\n",
    "        <li> \n",
    "            O valor retornado deve ser sempre positivo. Nós extraímos o módulo de cada dimensão antes de elevá-la à p-ésima potência.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em Machine Learning, costumamos usar a norma L-1 e L-2. Segue abaixo a fórmula geral de uma norma L-p:\n",
    "            $$\n",
    "                ||X||_p=[\\sum_{i=1}^{n}|x_i|^{p}]^{\\frac{1}{p}}\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ee917-8c5c-4225-af41-441b53c431fa",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> L-1 Norm</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Norma L-1 também é conhecida como Distância Manhattan, ou Taxicab norm.\n",
    "            $$\n",
    "            ||X||_1=\\sum_{i=1}^{n}x_i\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a879f55-0f4b-4dee-9de3-1c67733a2454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.001847187102745"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando 'np.linalg.norm' para medir a L-1 norm de um vetor específico.\n",
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "np.linalg.norm(x,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baf0dc-a05c-4ec7-84c3-c2d854e58de9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> L-2 Norm</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Norma L-1 também é conhecida como Distância Euclidiana.\n",
    "            $$\n",
    "            ||X||_2 = [\\sum_{i=1}^{n}|x_i|^{2}]^{\\frac{1}{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "            Essa norma é usada em treinamentos de modelos também, em que estimamos a Distância Euclidiana entre o valor previsto e o número-alvo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4d673c-fe35-4207-b503-8357cc0d4777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A função 'np.linalg.norm' está configurada para medir a norma L-2, por padrão.\n",
    "np.linalg.norm([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aceb6c-d43c-4755-8855-01f296148943",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style='font-size:30px'> Ridge Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Ridge consiste em uma versão regularizada da Regressão Linear.\n",
    "        </li>\n",
    "        <li> \n",
    "            Isso ocorre com a inserção de um termo na fórmula da função-custo: a metade do quadrado da norma l-2 do vetor com os coeficientes (sem o bias) $\\frac{1}{2}(||w||_2)^{2}$. \n",
    "        </li>\n",
    "        <li> \n",
    "            A regularização se dá pela redução dos coeficientes da fórmula. Quanto maior o valor de $\\alpha$, menores esses serão. A fórmula da função-custo final será:\n",
    "            $$\n",
    "                J(\\theta)=MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i=1}^{n}|\\theta_i|^{2}\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39da2af-62af-4e86-aee1-e360f3932d35",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> SGD</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             O vetor de gradiente da Regressão Ridge tem a seguinte fórmula (w é o vetor de coeficientes sem o termo de bias):\n",
    "            $$\n",
    "                \\nabla J(\\theta)=\\frac{2}{m}X^{T}(X\\theta-y)+\\alpha w\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8baac9-19d5-448d-8d9d-02421b4f63d1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Closed-form Solution</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             É possível, também, alcançarmos os melhores coeficientes da Regressão Ridge por meio de uma equação. <strong> A</strong> é uma matriz-identidade com um zero em seu canto superior esquerdo (vetor-coluna do termo de bias).\n",
    "            $$\n",
    "                \\theta=(X^{T}X + \\alpha A)^{-1} X^{T}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4226328-f231-40c0-b78a-8b3fb8ac3073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge com a classe-padrão do scikit-learn.\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier # Curiosidade, há um 'RidgeClassifier' no módulo!\n",
    "Ridge().fit(X_b, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41997a5-aa4a-4c4e-9daf-773663ab107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge por SGD.\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Observe, aqui é utilizada a norma L-2 na função-custo.\n",
    "SGDRegressor(penalty='l2').fit(X_b, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212c743-e150-4516-8a6d-44e71556805b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Lasso Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Regressão Lasso é uma outra versão regularizada da Regressão Linear, mas em que é usada a norma L-1 do vetor de coeficientes na função-custo.\n",
    "            $$\n",
    "                J(\\theta)=MSE(\\theta) + \\alpha \\sum_{i=1}^{n}|\\theta_i|\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "             Essa modalidade de regressão tende a fazer seleção de features, zerando os pesos daquelas menos relevantes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95c4a0-7c91-46f5-8e78-c13c06bc253c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> SGD</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A função-custo da Lasso não é diferenciável quando qualquer$\\theta_i=0$. Para a descida de gradiente ocorrer, usamos o subvetor de gradiente:\n",
    "            $$\n",
    "            g(\\theta,J)=\\nabla_{\\theta}MSE(\\theta)+\\alpha \\begin{bmatrix}\n",
    "            sign(\\theta_{1}) \\\\ \n",
    "            sign(\\theta_{2}) \\\\ \n",
    "            \\vdots \\\\\n",
    "            sign(\\theta_{n})\n",
    "            \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "            sign é uma função que extrai o sinal de um número. Ou seja, se for positivo, seu valor é 1; se negativo, -1; e se 0, 0.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c791f4a-e023-40d0-b128-4eb9c097ca38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Lasso Regression com a classe Lasso.\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso().fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be956a5-e017-49e9-a7f0-b5f1cd0686f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(penalty='l1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora, usando esse modelo com treinamento por SGD.\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDRegressor(penalty='l1').fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52401307-74b9-446b-a4cd-6a5758874804",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Elastic Net</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Elastic Net tenta ser um modelo meio-termo entre a Lasso e Ridge. A preponderância de uma técnica sobre a outra é definida pelo termo r. Segue a função-custo desse modelo:\n",
    "            $$\n",
    "                J(\\theta)=MSE(\\theta)+r\\alpha ||w||_1 + \\frac{1-r}{2}\\alpha(||w||_2)^{2}\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "            Quanto maior r, mais enfoque à Lasso será dado; quanto menor esse termo, mais próximo o modelo será da Ridge.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21ef6b9a-aa5e-472b-8cf5-9090fb6f1445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo um Elastic Net meio a meio com a classe-padrão.\n",
    "from sklearn.linear_model import ElasticNet\n",
    "ElasticNet(l1_ratio=.5).fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f8ec0e8-ab69-49ab-9f47-2b1027fc6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(penalty='elasticnet')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora, por SGD.\n",
    "SGDRegressor(penalty='elasticnet').fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce40d1e-e60b-4f66-b190-8c3b766c1ea1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Early Stopping</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Early Stopping é uma outra técnica de regularização. Ela se dá pela interrupção do treinamento após não conseguirmos melhorar a pontuação de validação por n-epochs.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c6a83-8dd2-4902-8bd2-85a4c62a6978",
   "metadata": {},
   "source": [
    "<p style='color:red'> Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
