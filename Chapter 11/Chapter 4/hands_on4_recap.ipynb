{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9d226c-dc57-4d07-ae9d-bf8972506b4e",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2616b-f8bb-4bed-be9f-adaeeda1e40c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Esclarecimentos</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Este é o meu segundo estudo sobre o quarto capítulo do livro. O objetivo deste registro é, agora, consolidar o meu entendimento sobre a esfera matemática do treinamento em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1e84-4614-494c-9762-6f2573d902a3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Notations (p.43)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            m é o tamanho do dataset com o qual avaliamos o modelo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se o set de validação tem 5000 instâncias, $m=5000$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $x^{(i)}$ é o vetor com os valores de todas as features independentes da i-ésima instância do dataset. $y^{(i)}$ é a sua variável-alvo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se a nona feature do dataset tem como features independentes 200, 4500, 100 e 9 $x^{(9)}=\\begin{bmatrix} 200\\\\4500\\\\1000\\\\9\\end{bmatrix}$. \n",
    "                </li>\n",
    "                <li> \n",
    "                    Caso sua target-variable seja 3, $y^{(9)}=\\begin{bmatrix}3\\end{bmatrix}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            X é a matrix com os valores das features independentes de cada instância. Esses, por sua vez, estão contidos em um vetor-linha, e não coluna. Portanto, para um dataset com 2000 instâncias:\n",
    "            $$\n",
    "                X=\\begin{bmatrix} \n",
    "                (x^{(1)})^{T} \\\\\n",
    "                (x^{(2)})^{T} \\\\\n",
    "                \\vdots \\\\\n",
    "                (x^{(2000)})^{T}\n",
    "                \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $h$ representa o nosso modelo de ML, que, para cada instância, lança uma previsão $ŷ$. Dessa maneira, $h(x^{(i)})=ŷ^{(i)}$\n",
    "             <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    O erro absoluto de um algoritmo regressor será $ŷ^{(i)}-y^{(i)}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $RMSE(X, h)$, $R^{2}(X,h)$ representam a função custo aplicada no set $X$ com o modelo $h$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8acb2-9142-4cc9-a0c6-0f8456501f98",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Métricas de Regressão</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             RMSE, também conhecida como Euclidean Norm ou $l_2$ norm. Sua fórmula é:\n",
    "            $$\n",
    "            RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            MAE, também conhecida como Manhattan Norm ou $l_1$ norm. Sua fórmula é:\n",
    "            $$\n",
    "                    MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m} |h(x^{(i)})-y^{(i)}| \n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f10c4-aa3b-4e1a-94c6-96e601fd087e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Início de Fato do Capítulo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Neste capítulo, aprenderemos os principais métodos de treinamento em ML, assim como alguns modelos mais simples.\n",
    "        </li>\n",
    "        <li> \n",
    "            Há maneiras mais diretas de alcançarmos os parâmetros ideais do modelo, e outras que envolvem iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb59b-7e9c-4c45-ac63-f5260c0db7ff",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Linear Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Linear é o modelo mais simples do ML. Consiste na soma ponderada de cada feature por um coeficiente. Uma previsão ŷ é dada como:\n",
    "            <p>$\n",
    "                ŷ=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $ ($\\theta_0$ é o termo de bias).\n",
    "            </p>         \n",
    "        </li>\n",
    "        <li> \n",
    "            Uma maneira mais concisa de formular essa equação é:\n",
    "            $$\n",
    "                ŷ=h_{\\theta}(x)=\\theta \\cdot x\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    $\\theta$ é o vetor com os coeficientes do modelo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    x é o vetor com os valores das features.\n",
    "                </li>\n",
    "                <li> \n",
    "                    $\\theta \\cdot x$ é o produto escalar entre os vetores mencionados, igual a $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A previsão do modelo ainda pode ser dada como $ŷ=\\theta^{T}x$, que é a multiplicação entre a transposta dos coeficientes e x.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b02d65-c5e2-41af-88b0-240a6d07501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2771035567696405, 1.2771035567696405)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "theta = np.random.random(size=3)\n",
    "x = np.random.normal(size=3)\n",
    "\n",
    "# Observe como o dot product e a multiplicação matricial se equivalem.\n",
    "theta.dot(x), theta.T@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f35b-7e12-4ea8-9840-07a6dfed8477",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A criação de uma Regressão Linear envolve encontrar o $\\theta$ que minimiza o MSE. Agora, é hora de explorarmos os diferentes métodos para que isso ocorra.        \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d21c99-f22f-4ad4-b84e-4f3edd712308",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Normal Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1914c93-553b-4b44-8413-dff53028206f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Equação Normal é uma maneira direta de alcançarmos o $\\theta$ minimizador do MSE. A sua fórmula é:\n",
    "            $$\n",
    "                \\theta=(X^{T}X)^{-1}X^{T}y\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    y é o vetor com as target variables.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2adb2587-e96d-4b5b-8ae7-4d1333c2b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97671164],\n",
       "       [3.97782923]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Equação Normal com o Numpy.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 10 + 4*X - np.random.normal(size=(1000,1))\n",
    "\n",
    "# Adicionando a coluna de bias a X.\n",
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T@X_b) @ X_b.T @ y\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f8e0d1-36cf-43c3-8d4b-6959bb626488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.8658578]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo uma previsão com uma nova instância.\n",
    "X_new = np.array([5])\n",
    "X_new_b = np.c_[np.ones((1,1)), X_new]\n",
    "\n",
    "X_new_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2b34e4-7734-4bfe-94d5-8fdf737b701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97671164],\n",
       "       [3.97782923]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa mesma equação poderia ser escrita com os dot products entre as matrizes.\n",
    "np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e400cb21-7938-416d-8abb-228bfbc5e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.97671164]), array([[3.97782923]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'LinearRegression' obtém os mesmos parâmetros.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397930-54b7-4070-a66f-d001487c7b52",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Moore-Penrose Inverse</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             Uma outra maneira de obtermos os coeficientes da nossa Regressão Linear é com o produto escalar entre a matriz Moore-Penrose de X (conhecida também como pseudoinversa) e y.\n",
    "            $$\n",
    "                \\theta=X^{+}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603d0744-cdf7-4734-8b9a-5417c8e67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97671164],\n",
       "       [3.97782923]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'np.linalg.pinv' para computar a pseudoinversa de X.\n",
    "theta_best = np.linalg.pinv(X_b).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c43f5-4553-4a14-b4ff-4cf12d30cfee",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Computational Complexity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Time Complexity para treinamento de uma Regressão Linear com a Normal Equation pode ser de $O(n^{2.4})$ a $O(n^{3})$ e $O(n^{2})$ com a Inversa Moore-Penrose (sendo n o número de features). Ambos os métodos são $O(m)$ (m é o número de instãncias).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com previsões, ambas as estratégias têm $O(n)$ e $O(m)$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa20fd-c544-425f-b5b6-475275b08d05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gradient Descent</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O autor introduz os conceitos de Descida de Gradiente e learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc9e57-decd-497d-9487-117acf0dd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c6a83-8dd2-4902-8bd2-85a4c62a6978",
   "metadata": {},
   "source": [
    "<p style='color:red'> Pesquisar sobre SVD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
