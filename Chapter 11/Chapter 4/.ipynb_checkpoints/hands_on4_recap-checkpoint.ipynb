{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9d226c-dc57-4d07-ae9d-bf8972506b4e",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2616b-f8bb-4bed-be9f-adaeeda1e40c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Esclarecimentos</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Este é o meu segundo estudo sobre o quarto capítulo do livro. O objetivo deste registro é, agora, consolidar o meu entendimento sobre a esfera matemática do treinamento em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1e84-4614-494c-9762-6f2573d902a3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Notations (p.43)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            m é o tamanho do dataset com o qual avaliamos o modelo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se o set de validação tem 5000 instâncias, $m=5000$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $x^{(i)}$ é o vetor com os valores de todas as features independentes da i-ésima instância do dataset. $y^{(i)}$ é a sua variável-alvo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se a nona feature do dataset tem como features independentes 200, 4500, 100 e 9 $x^{(9)}=\\begin{bmatrix} 200\\\\4500\\\\1000\\\\9\\end{bmatrix}$. \n",
    "                </li>\n",
    "                <li> \n",
    "                    Caso sua target-variable seja 3, $y^{(9)}=\\begin{bmatrix}3\\end{bmatrix}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            X é a matrix com os valores das features independentes de cada instância. Esses, por sua vez, estão contidos em um vetor-linha, e não coluna. Portanto, para um dataset com 2000 instâncias:\n",
    "            $$\n",
    "                X=\\begin{bmatrix} \n",
    "                (x^{(1)})^{T} \\\\\n",
    "                (x^{(2)})^{T} \\\\\n",
    "                \\vdots \\\\\n",
    "                (x^{(2000)})^{T}\n",
    "                \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $h$ representa o nosso modelo de ML, que, para cada instância, lança uma previsão $ŷ$. Dessa maneira, $h(x^{(i)})=ŷ^{(i)}$\n",
    "             <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    O erro absoluto de um algoritmo regressor será $ŷ^{(i)}-y^{(i)}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $RMSE(X, h)$, $R^{2}(X,h)$ representam a função custo aplicada no set $X$ com o modelo $h$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8acb2-9142-4cc9-a0c6-0f8456501f98",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Métricas de Regressão</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             RMSE, também conhecida como Euclidean Norm ou $l_2$ norm. Sua fórmula é:\n",
    "            $$\n",
    "            RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            MAE, também conhecida como Manhattan Norm ou $l_1$ norm. Sua fórmula é:\n",
    "            $$\n",
    "                    MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m} |h(x^{(i)})-y^{(i)}| \n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f10c4-aa3b-4e1a-94c6-96e601fd087e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Início de Fato do Capítulo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Neste capítulo, aprenderemos os principais métodos de treinamento em ML, assim como alguns modelos mais simples.\n",
    "        </li>\n",
    "        <li> \n",
    "            Há maneiras mais diretas de alcançarmos os parâmetros ideais do modelo, e outras que envolvem iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb59b-7e9c-4c45-ac63-f5260c0db7ff",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Linear Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Linear é o modelo mais simples do ML. Consiste na soma ponderada de cada feature por um coeficiente. Uma previsão ŷ é dada como:\n",
    "            <p>$\n",
    "                ŷ=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $ ($\\theta_0$ é o termo de bias).\n",
    "            </p>         \n",
    "        </li>\n",
    "        <li> \n",
    "            Uma maneira mais concisa de formular essa equação é:\n",
    "            $$\n",
    "                ŷ=h_{\\theta}(x)=\\theta \\cdot x\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    $\\theta$ é o vetor com os coeficientes do modelo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    x é o vetor com os valores das features.\n",
    "                </li>\n",
    "                <li> \n",
    "                    $\\theta \\cdot x$ é o produto escalar entre os vetores mencionados, igual a $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A previsão do modelo ainda pode ser dada como $ŷ=\\theta^{T}x$, que é a multiplicação entre a transposta dos coeficientes e x.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b02d65-c5e2-41af-88b0-240a6d07501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2753030570997286, -1.2753030570997286)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "theta = np.random.random(size=3)\n",
    "x = np.random.normal(size=3)\n",
    "\n",
    "# Observe como o dot product e a multiplicação matricial se equivalem.\n",
    "theta.dot(x), theta.T@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f35b-7e12-4ea8-9840-07a6dfed8477",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A criação de uma Regressão Linear envolve encontrar o $\\theta$ que minimiza o MSE. Agora, é hora de explorarmos os diferentes métodos para que isso ocorra.        \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d21c99-f22f-4ad4-b84e-4f3edd712308",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Normal Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1914c93-553b-4b44-8413-dff53028206f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Equação Normal é uma maneira direta de alcançarmos o $\\theta$ minimizador do MSE. A sua fórmula é:\n",
    "            $$\n",
    "                \\theta=(X^{T}X)^{-1}X^{T}y\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    y é o vetor com as target variables.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adb2587-e96d-4b5b-8ae7-4d1333c2b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Equação Normal com o Numpy.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 10 + 4*X - np.random.normal(size=(1000,1))\n",
    "\n",
    "# Adicionando a coluna de bias a X.\n",
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T@X_b) @ X_b.T @ y\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f8e0d1-36cf-43c3-8d4b-6959bb626488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.88411678]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo uma previsão com uma nova instância.\n",
    "X_new = np.array([5])\n",
    "X_new_b = np.c_[np.ones((1,1)), X_new]\n",
    "\n",
    "X_new_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2b34e4-7734-4bfe-94d5-8fdf737b701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa mesma equação poderia ser escrita com os dot products entre as matrizes.\n",
    "np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400cb21-7938-416d-8abb-228bfbc5e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.02114206]), array([[3.97259494]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'LinearRegression' obtém os mesmos parâmetros.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397930-54b7-4070-a66f-d001487c7b52",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Moore-Penrose Inverse</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             Uma outra maneira de obtermos os coeficientes da nossa Regressão Linear é com o produto escalar entre a matriz Moore-Penrose de X (conhecida também como pseudoinversa) e y.\n",
    "            $$\n",
    "                \\theta=X^{+}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603d0744-cdf7-4734-8b9a-5417c8e67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'np.linalg.pinv' para computar a pseudoinversa de X.\n",
    "theta_best = np.linalg.pinv(X_b).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c43f5-4553-4a14-b4ff-4cf12d30cfee",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Computational Complexity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Time Complexity para treinamento de uma Regressão Linear com a Normal Equation pode ser de $O(n^{2.4})$ a $O(n^{3})$ e $O(n^{2})$ com a Inversa Moore-Penrose (sendo n o número de features). Ambos os métodos são $O(m)$ (m é o número de instâncias).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com previsões, as duas estratégias têm $O(n)$ e $O(m)$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa20fd-c544-425f-b5b6-475275b08d05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gradient Descent</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O autor introduz os conceitos de Descida de Gradiente e learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30555-4c42-4704-ba61-21d3dfd8342b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Batch Gradient Descent consiste em fazer a descida de gradiente com base em todo o conjunto de treino. O Gradiente de uma MSE para a Regressão Linear é dado como:\n",
    "            <p style='margin-top:10px'>\n",
    "            $$\n",
    "                \\nabla_{\\theta}MSE(\\theta)=\n",
    "                    \\begin{bmatrix} \n",
    "                        \\frac{\\delta}{\\delta\\theta_{0}}MSE(\\theta) \\\\ \n",
    "                        \\frac{\\delta}{\\delta\\theta_{1}}MSE(\\theta) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\frac{\\delta}{\\delta\\theta_{n}}MSE(\\theta) \n",
    "                    \\end{bmatrix}                    \n",
    "                    = \\frac{2}{m}X^{T}(X\\theta-y)\n",
    "            $$\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99bf40b5-f24a-4f74-884c-0e50852f8f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114205],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Batch Gradient Descent em código.\n",
    "\n",
    "eta = .01 # learning rate.\n",
    "n_iterations = 1000 # número de iterações total.\n",
    "\n",
    "# Inicialização aleatória dos parâmetros (segundo uma Distribuição Normal).\n",
    "theta = np.random.uniform(size=(2,1))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    #print(gradient, end='\\n\\n')\n",
    "    theta -=  eta*gradient\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21756b1-75fb-4561-b490-8f46169df9e7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stochastic Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Descida de Gradiente Estocástica surge pela demora na computação do Gradiente com o set de treino completo. Esse novo método propõe selecionar uma única instância aleatória para a conta.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, por outro lado, a descida de gradiente será menos regular. Outliers também ganham um peso maior na computação, o dado da iteração pode ser um desses. \n",
    "        </li>\n",
    "        <li> \n",
    "            Como o treinamento é mais instável, raramente alcançaremos a solução ótima, e sim, algo próximo. Uma estratégia para SGD é definir uma alta learning rate no início do fitting, para escaparmos dos mínimos locais, e reduzi-la, gradativamente, a fim de tentarmos nos alocar no mínimo global; se estivermos, de fato, na solução ótima, o vetor de gradiente estará severamente podado pelo menor $\\eta$, impedindo de escaparmos dela.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: O scikit-learn oferece uma série de outras estratégias para tratamento das learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d1e08-6580-4ecf-ba34-d86b1f52cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 1.         -0.03098772]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "28bc9e34-841a-42ff-ba07-fb3ed79983a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1430377 ],\n",
       "       [0.72524716]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "2a053f25-ea04-43ba-854b-29efe1b79259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão 1.\n",
    "# Fazendo uma breve SGD.\n",
    "\n",
    "# O treinamento é bastante similar ao de uma Rede Neural. À cada época, uma certa quantidade de instâncias são selecionadas para\n",
    "# o ajuste dos parâmetros.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SGDLinearRegression(RegressorMixin):\n",
    "    # 'theta' stores the LR coefficients.\n",
    "    theta = None\n",
    "    def __init__(self, eta0=.05, epochs=20, validation_split=.1, tol=1e-3, n_iter_no_change=3):\n",
    "        self.eta0 = eta0 # Initial Learning Rate.\n",
    "        self.epochs = epochs  # Number of Epochs.\n",
    "        self.validation_split = validation_split # % of training set held for validation.\n",
    "        self.tol = tol # Tolerance.\n",
    "        self.n_iter_no_change = n_iter_no_change # Number of epochs with no improvement for training halt.\n",
    "        \n",
    "    def learning_schedule(t, t0=5, t1=50):\n",
    "        return t0/(t+t1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        SGDLinearRegression.theta = np.random.rand(X.shape[1],1)\n",
    "        # Segregating training from validation data.\n",
    "        #X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = self.validation_split)\n",
    "        #print(y_val)\n",
    "        best_theta = SGDLinearRegression.theta\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                # Randomly selecting the epoch's iteration instance.\n",
    "                index = np.random.randint(len(X))\n",
    "                #print(index)\n",
    "                xi = X[index:index+1]\n",
    "                yi = y[index:index+1]\n",
    "                #print(xi, yi)\n",
    "                # Computing the MSE gradient based on the instance's data.\n",
    "                gradients = (2 * xi.T.dot(xi.dot(SGDLinearRegression.theta ) - yi))\n",
    "                self.eta0 = SGDLinearRegression.learning_schedule(epoch*100+i)\n",
    "                SGDLinearRegression.theta = SGDLinearRegression.theta - (gradients * self.eta0)\n",
    "        return SGDLinearRegression.theta #'SGDLinearRegressionSGDLinearRegression.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8f008-8748-4fb3-b1f9-523258ce4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão Aprimorada.\n",
    "# Fazendo uma breve SGD.\n",
    "\n",
    "# O treinamento é bastante similar ao de uma Rede Neural. À cada época, uma certa quantidade de instâncias são selecionadas para\n",
    "# o ajuste dos parâmetros.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SGDLinearRegression(RegressorMixin):\n",
    "    # 'theta' stores the LR coefficients.\n",
    "    theta = None\n",
    "    def __init__(self, eta0=.05, epochs=20, validation_split=.1, tol=1e-3, n_iter_no_change=3):\n",
    "        self.eta0 = eta0 # Initial Learning Rate.\n",
    "        self.epochs = epochs  # Number of Epochs.\n",
    "        self.validation_split = validation_split # % of training set held for validation.\n",
    "        self.tol = tol # Tolerance.\n",
    "        self.n_iter_no_change = n_iter_no_change # Number of epochs with no improvement for training halt.\n",
    "        \n",
    "    def learning_schedule(t, t0=5, t1=50):\n",
    "        return t0/(t+t1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        SGDLinearRegression.theta = np.random.rand(X.shape[1],1)\n",
    "        # Segregating training from validation data.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = self.validation_split)\n",
    "        #print(y_val)\n",
    "        best_theta = SGDLinearRegression.theta\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X_train)):\n",
    "                # Randomly selecting the epoch's iteration instance.\n",
    "                index = np.random.randint(len(X_train))\n",
    "                xi = X_train[index:index+1]\n",
    "                yi = y_train[index:index+1]\n",
    "                #print(xi, yi)\n",
    "                # Computing the MSE gradient based on the instance's data.\n",
    "                gradients = (2 * xi.T.dot(xi.dot(SGDLinearRegression.theta ) - yi))\n",
    "                #self.eta0 = SGDLinearRegression.learning_schedule(epoch*100+i)\n",
    "                SGDLinearRegression.theta = SGDLinearRegression.theta - (gradients * self.eta0)\n",
    "        return SGDLinearRegression.theta #'SGDLinearRegressionSGDLinearRegression.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6b7ecf3f-4ea2-4465-8a97-1a425d23acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "382e4f64-69a3-466a-b9ce-4bd2932179c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.01998819],\n",
       "       [ 3.97359144]])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDLinearRegression(epochs=50, eta0=.1, n_iter_no_change=10).fit(X_b, y.ravel())\n",
    "#y, y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "35d96bff-f01d-425f-80d9-3dfc364a4a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.01918459,  3.97536481])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDRegressor(fit_intercept=False).fit(X_b, y.ravel()).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c6a83-8dd2-4902-8bd2-85a4c62a6978",
   "metadata": {},
   "source": [
    "<p style='color:red'> Agora, montar a classe conforme meus desejos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
