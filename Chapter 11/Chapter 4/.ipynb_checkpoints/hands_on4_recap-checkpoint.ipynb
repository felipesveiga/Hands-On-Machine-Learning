{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9d226c-dc57-4d07-ae9d-bf8972506b4e",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2616b-f8bb-4bed-be9f-adaeeda1e40c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Esclarecimentos</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Este é o meu segundo estudo sobre o quarto capítulo do livro. O objetivo deste registro é, agora, consolidar o meu entendimento sobre a esfera matemática do treinamento em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1e84-4614-494c-9762-6f2573d902a3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Notations (p.43)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            m é o tamanho do dataset com o qual avaliamos o modelo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se o set de validação tem 5000 instâncias, $m=5000$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $x^{(i)}$ é o vetor com os valores de todas as features independentes da i-ésima instância do dataset. $y^{(i)}$ é a sua variável-alvo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se a nona feature do dataset tem como features independentes 200, 4500, 100 e 9 $x^{(9)}=\\begin{bmatrix} 200\\\\4500\\\\1000\\\\9\\end{bmatrix}$. \n",
    "                </li>\n",
    "                <li> \n",
    "                    Caso sua target-variable seja 3, $y^{(9)}=\\begin{bmatrix}3\\end{bmatrix}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            X é a matrix com os valores das features independentes de cada instância. Esses, por sua vez, estão contidos em um vetor-linha, e não coluna. Portanto, para um dataset com 2000 instâncias:\n",
    "            $$\n",
    "                X=\\begin{bmatrix} \n",
    "                (x^{(1)})^{T} \\\\\n",
    "                (x^{(2)})^{T} \\\\\n",
    "                \\vdots \\\\\n",
    "                (x^{(2000)})^{T}\n",
    "                \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $h$ representa o nosso modelo de ML, que, para cada instância, lança uma previsão $ŷ$. Dessa maneira, $h(x^{(i)})=ŷ^{(i)}$\n",
    "             <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    O erro absoluto de um algoritmo regressor será $ŷ^{(i)}-y^{(i)}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $RMSE(X, h)$, $R^{2}(X,h)$ representam a função custo aplicada no set $X$ com o modelo $h$. \n",
    "        </li>\n",
    "        <li> \n",
    "            $J(\\theta)$ se referirá a qualquer função-custo com nome muito extenso. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8acb2-9142-4cc9-a0c6-0f8456501f98",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Métricas de Regressão</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             RMSE, também conhecida como Euclidean Norm ou $l_2$ norm. Sua fórmula é:\n",
    "            $$\n",
    "            RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            MAE, também conhecida como Manhattan Norm ou $l_1$ norm. Sua fórmula é:\n",
    "            $$\n",
    "                    MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m} |h(x^{(i)})-y^{(i)}| \n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f10c4-aa3b-4e1a-94c6-96e601fd087e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Início de Fato do Capítulo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Neste capítulo, aprenderemos os principais métodos de treinamento em ML, assim como alguns modelos mais simples.\n",
    "        </li>\n",
    "        <li> \n",
    "            Há maneiras mais diretas de alcançarmos os parâmetros ideais do modelo, e outras que envolvem iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb59b-7e9c-4c45-ac63-f5260c0db7ff",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Linear Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Linear é o modelo mais simples do ML. Consiste na soma ponderada de cada feature por um coeficiente. Uma previsão ŷ é dada como:\n",
    "            <p>$\n",
    "                ŷ=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $ ($\\theta_0$ é o termo de bias).\n",
    "            </p>         \n",
    "        </li>\n",
    "        <li> \n",
    "            Uma maneira mais concisa de formular essa equação é:\n",
    "            $$\n",
    "                ŷ=h_{\\theta}(x)=\\theta \\cdot x\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    $\\theta$ é o vetor com os coeficientes do modelo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    x é o vetor com os valores das features.\n",
    "                </li>\n",
    "                <li> \n",
    "                    $\\theta \\cdot x$ é o produto escalar entre os vetores mencionados, igual a $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A previsão do modelo ainda pode ser dada como $ŷ=\\theta^{T}x$, que é a multiplicação entre a transposta dos coeficientes e x.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b02d65-c5e2-41af-88b0-240a6d07501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2753030570997286, -1.2753030570997286)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "theta = np.random.random(size=3)\n",
    "x = np.random.normal(size=3)\n",
    "\n",
    "# Observe como o dot product e a multiplicação matricial se equivalem.\n",
    "theta.dot(x), theta.T@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f35b-7e12-4ea8-9840-07a6dfed8477",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A criação de uma Regressão Linear envolve encontrar o $\\theta$ que minimiza o MSE. Agora, é hora de explorarmos os diferentes métodos para que isso ocorra.        \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d21c99-f22f-4ad4-b84e-4f3edd712308",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Normal Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1914c93-553b-4b44-8413-dff53028206f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Equação Normal é uma maneira direta de alcançarmos o $\\theta$ minimizador do MSE. A sua fórmula é:\n",
    "            $$\n",
    "                \\theta=(X^{T}X)^{-1}X^{T}y\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    y é o vetor com as target variables.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adb2587-e96d-4b5b-8ae7-4d1333c2b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Equação Normal com o Numpy.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 10 + 4*X - np.random.normal(size=(1000,1))\n",
    "\n",
    "# Adicionando a coluna de bias a X.\n",
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T@X_b) @ X_b.T @ y\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f8e0d1-36cf-43c3-8d4b-6959bb626488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.88411678]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo uma previsão com uma nova instância.\n",
    "X_new = np.array([5])\n",
    "X_new_b = np.c_[np.ones((1,1)), X_new]\n",
    "\n",
    "X_new_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2b34e4-7734-4bfe-94d5-8fdf737b701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa mesma equação poderia ser escrita com os dot products entre as matrizes.\n",
    "np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400cb21-7938-416d-8abb-228bfbc5e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.02114206]), array([[3.97259494]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'LinearRegression' obtém os mesmos parâmetros.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397930-54b7-4070-a66f-d001487c7b52",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Moore-Penrose Inverse</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             Uma outra maneira de obtermos os coeficientes da nossa Regressão Linear é com o produto escalar entre a matriz Moore-Penrose de X (conhecida também como pseudoinversa) e y.\n",
    "            $$\n",
    "                \\theta=X^{+}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603d0744-cdf7-4734-8b9a-5417c8e67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'np.linalg.pinv' para computar a pseudoinversa de X.\n",
    "theta_best = np.linalg.pinv(X_b).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c43f5-4553-4a14-b4ff-4cf12d30cfee",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Computational Complexity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Time Complexity para treinamento de uma Regressão Linear com a Normal Equation pode ser de $O(n^{2.4})$ a $O(n^{3})$ e $O(n^{2})$ com a Inversa Moore-Penrose (sendo n o número de features). Ambos os métodos são $O(m)$ (m é o número de instâncias).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com previsões, as duas estratégias têm $O(n)$ e $O(m)$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa20fd-c544-425f-b5b6-475275b08d05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gradient Descent</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O autor introduz os conceitos de Descida de Gradiente e learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30555-4c42-4704-ba61-21d3dfd8342b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Batch Gradient Descent consiste em fazer a descida de gradiente com base em todo o conjunto de treino. O Gradiente de uma MSE para a Regressão Linear é dado como:\n",
    "            <p style='margin-top:10px'>\n",
    "            $$\n",
    "                \\nabla_{\\theta}MSE(\\theta)=\n",
    "                    \\begin{bmatrix} \n",
    "                        \\frac{\\delta}{\\delta\\theta_{0}}MSE(\\theta) \\\\ \n",
    "                        \\frac{\\delta}{\\delta\\theta_{1}}MSE(\\theta) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\frac{\\delta}{\\delta\\theta_{n}}MSE(\\theta) \n",
    "                    \\end{bmatrix}                    \n",
    "                    = \\frac{2}{m}X^{T}(X\\theta-y)\n",
    "            $$\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99bf40b5-f24a-4f74-884c-0e50852f8f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114205],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Batch Gradient Descent em código.\n",
    "\n",
    "eta = .01 # learning rate.\n",
    "n_iterations = 1000 # número de iterações total.\n",
    "\n",
    "# Inicialização aleatória dos parâmetros (segundo uma Distribuição Normal).\n",
    "theta = np.random.uniform(size=(2,1))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    #print(gradient, end='\\n\\n')\n",
    "    theta -=  eta*gradient\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21756b1-75fb-4561-b490-8f46169df9e7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stochastic Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Descida de Gradiente Estocástica surge pela demora na computação do Gradiente com o set de treino completo. Esse novo método propõe selecionar uma única instância aleatória para a conta.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, por outro lado, a descida de gradiente será menos regular. Outliers também ganham um peso maior na computação, o dado da iteração pode ser um desses. \n",
    "        </li>\n",
    "        <li> \n",
    "            Como o treinamento é mais instável, raramente alcançaremos a solução ótima, e sim, algo próximo. Uma estratégia para SGD é definir uma alta learning rate no início do fitting, para escaparmos dos mínimos locais, e reduzi-la, gradativamente, a fim de tentarmos nos alocar no mínimo global; se estivermos, de fato, na solução ótima, o vetor de gradiente estará severamente podado pelo menor $\\eta$, impedindo de escaparmos dela.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: O scikit-learn oferece uma série de outras estratégias para tratamento das learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "91b8f008-8748-4fb3-b1f9-523258ce4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O treinamento é bastante similar ao de uma Rede Neural. À cada época, uma certa quantidade de instâncias são selecionadas para\n",
    "# o ajuste dos parâmetros.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SGDLinearRegression(RegressorMixin):\n",
    "    # 'theta' stores the LR coefficients.\n",
    "    theta = None\n",
    "    def __init__(self, eta0=.05, epochs=20, validation_split=.1, tol=1e-3, n_iter_no_change=3):\n",
    "        self.eta0 = eta0 # Initial Learning Rate.\n",
    "        self.epochs = epochs  # Number of Epochs.\n",
    "        self.validation_split = validation_split # % of training set held for validation.\n",
    "        self.tol = tol # Tolerance.\n",
    "        self.n_iter_no_change = n_iter_no_change # Number of epochs with no improvement for training halt.\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        SGDLinearRegression.theta = np.random.rand(X.shape[1],1)\n",
    "        # Segregating training from validation data.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = self.validation_split)\n",
    "        # Storing the 'theta' from the best epoch (lowest 'best_mse'). \n",
    "        best_theta, self.best_mse = SGDLinearRegression.theta, np.inf\n",
    "        no_change = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X_train)):\n",
    "                # Randomly selecting the epoch's iteration instance.\n",
    "                index = np.random.randint(len(X_train))\n",
    "                xi = X_train[index:index+1]\n",
    "                yi = y_train[index:index+1]\n",
    "                # Computing the gradient based on the instance's data.\n",
    "                gradients = (2 * xi.T.dot(xi.dot(SGDLinearRegression.theta ) - yi))\n",
    "                SGDLinearRegression.theta = SGDLinearRegression.theta - (gradients * self.eta0)\n",
    "            # At the end of the current epoch, the model performs a validation on 'X_val' and 'y_val'.\n",
    "            # If no improvement is occurs, the learning rate is divided by 5.\n",
    "            MSE = mean_squared_error(y_val, X_val.dot(SGDLinearRegression.theta))\n",
    "            if MSE > self.best_mse - self.tol:\n",
    "                no_change+=1\n",
    "                self.eta0 /= 5\n",
    "                # If the number of consecutive epochs with no MSE decay reaches 'n_iter_no_change', training is interrupted and the model\n",
    "                # is re-adjusted with the coefficients from the best epoch.\n",
    "                if no_change == self.n_iter_no_change:\n",
    "                    SGDLinearRegression.theta =  best_theta\n",
    "                    return self\n",
    "            else:\n",
    "                no_change = 0\n",
    "                self.best_mse = MSE\n",
    "                best_theta = SGDLinearRegression.theta            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e55553-7396-45f5-9873-943cf0a4c357",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Mini-Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Mini-Batch GD surge como um meio termo entre os dois métodos já apresentados. Sua estratégia é medir, à cada iteração, o vetor de gradiente com base em um conjunto de instâncias aleatórias (e não apenas uma, como no SGD).\n",
    "        </li>\n",
    "        <li> \n",
    "            Essa abordagem torna a descida de gradiente mais estável e com uma sensibilidade menor à presença de outliers.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ffb697-ce53-4d98-bfd9-a11d208bb241",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Polynomial Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, é apresentada a classe PolynomialFeatures, responsável por aumentar a dimensionalidade do dataset com base no produto entre as features.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f3865-2c8f-424b-92ff-6122d88733d3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Learning Curves</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As Curvas de Aprendizado surgem como um recurso para analisarmos, em primeiro lugar, o quanto de instâncias seriam adequadas para que o modelo atinja o seu pleno potencial e, também, se está ocorrendo um under ou overfitting.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c87cbaf7-97e9-4044-8dbe-a42c174ed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Criando novas features.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 2.5*X**2 - 4*X + 5 + np.random.normal()\n",
    "\n",
    "_, train_mse, test_mse = learning_curve(LinearRegression(), X, y, cv=5, scoring='neg_mean_squared_error', \n",
    "                                        train_sizes=np.linspace(.05, .75, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "42caaeab-0efe-4742-8a95-0a4c9811a2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41d9f391f0>"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Observe, como treinamos uma Regressão Linear em uma função quadrática, recebemos um erro elevado nos sets de treino e teste.\n",
    "# Ou seja, há um underfitting.\n",
    "plt.plot(_, np.sqrt(-train_mse)[:, 0], label='Train')\n",
    "plt.plot(_, np.sqrt(-test_mse)[:, 0], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "8ca35030-d7b2-4d4b-afaa-4c33a040fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41d82a3790>"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agora, forçaremos um overfitting treinando uma árvore de decisão sem regularizações.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "_, train_mse, test_mse = learning_curve(DecisionTreeRegressor(), X, y, train_sizes=np.linspace(.01, .75, 11), cv=5,\n",
    "                                        scoring='neg_mean_squared_error')\n",
    "\n",
    "# O overfitting pode ser detectado com altas performances no set de treino e baixas no de teste. O resultado poderia ter sido mais\n",
    "# expressivo por aqui, mas fica como um exemplo.\n",
    "plt.plot(_, np.sqrt(-train_mse)[:, 0], label='Train')\n",
    "plt.plot(_, np.sqrt(-test_mse)[:, 0], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd42a6-88e2-419e-bad8-6084064f9cc3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bias/Variance Tradeoff</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            <u>Bias</u>: seria a incapacidade de um modelo de se ajustar à toda complexidade do dataset. Algoritmos como a Regressão Linear têm um alto nível de bias por apenas conseguirem capturar relações estritamente lineares; é provável que haja um underfitting ao usá-los.\n",
    "        </li>\n",
    "        <li> \n",
    "            <u> Variance</u> consiste na elevada variância qualitativa das previsões em diferentes datasets. Isso, normalmente, ocorre pelo uso de um modelo muito complexo, que se superadequa aos padrões do set de treino e, com isso, perde o seu poder de genealização.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee771191-54e9-495e-bc9d-f580ce0b666f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Norm</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A norma de um vetor consiste em um número indicando a magnitude desse. É na p-ésima raiz da soma das dimensões de um vetor elevadas à p-ésima potência. Sua notação é $||X||_p$ \n",
    "        </li>\n",
    "        <li> \n",
    "            O valor retornado deve ser sempre positivo. Nós extraímos o módulo de cada dimensão antes de elevá-la à p-ésima potência.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em Machine Learning, costumamos usar a norma L-1 e L-2. Segue abaixo a fórmula geral de uma norma L-p:\n",
    "            $$\n",
    "                ||X||_p=[\\sum_{i=1}^{n}|x_i|^{p}]^{\\frac{1}{p}}\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ee917-8c5c-4225-af41-441b53c431fa",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> L-1 Norm</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Norma L-1 também é conhecida como Distância Manhattan, ou Taxicab norm.\n",
    "            $$\n",
    "            ||X||_1=\\sum_{i=1}^{n}x_i\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "4a879f55-0f4b-4dee-9de3-1c67733a2454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.329286290751439"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando 'np.linalg.norm' para medir a L-1 norm de um vetor específico.\n",
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "np.linalg.norm(x,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baf0dc-a05c-4ec7-84c3-c2d854e58de9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> L-2 Norm</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Norma L-1 também é conhecida como Distância Euclidiana.\n",
    "            $$\n",
    "            ||X||_2 = [\\sum_{i=1}^{n}|x_i|^{2}]^{\\frac{1}{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "            Essa norma é usada em treinamentos de modelos também, em que estimamos a Distância Euclidiana entre o valor previsto e o número-alvo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "4b4d673c-fe35-4207-b503-8357cc0d4777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A função 'np.linalg.norm' está configurada para medir a norma L-2, por padrão.\n",
    "np.linalg.norm([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aceb6c-d43c-4755-8855-01f296148943",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Ridge Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Ridge consiste em uma versão regularizada da Regressão Linear.\n",
    "        </li>\n",
    "        <li> \n",
    "            Isso ocorre com a inserção de um termo na fórmula da função-custo: a metade do quadrado da norma l-2 do vetor com os coeficientes (sem o bias) $\\frac{1}{2}(||w||_2)^{2}$. \n",
    "        </li>\n",
    "        <li> \n",
    "            A regularização se dá pela redução dos coeficientes da fórmula. Quanto maior o valor de $\\alpha$, menores esses serão. função-custo final será:\n",
    "            $$\n",
    "                J(\\theta)=MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta_i^{2}\n",
    "            $$\n",
    "        </li>\n",
    "        <li> \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c6a83-8dd2-4902-8bd2-85a4c62a6978",
   "metadata": {},
   "source": [
    "<p style='color:red'> Escrever a fórmula do gradiente da Ridge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
