{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9d226c-dc57-4d07-ae9d-bf8972506b4e",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2616b-f8bb-4bed-be9f-adaeeda1e40c",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Esclarecimentos</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Este é o meu segundo estudo sobre o quarto capítulo do livro. O objetivo deste registro é, agora, consolidar o meu entendimento sobre a esfera matemática do treinamento em Machine Learning.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1e84-4614-494c-9762-6f2573d902a3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Notations (p.43)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            m é o tamanho do dataset com o qual avaliamos o modelo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se o set de validação tem 5000 instâncias, $m=5000$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $x^{(i)}$ é o vetor com os valores de todas as features independentes da i-ésima instância do dataset. $y^{(i)}$ é a sua variável-alvo.\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    Se a nona feature do dataset tem como features independentes 200, 4500, 100 e 9 $x^{(9)}=\\begin{bmatrix} 200\\\\4500\\\\1000\\\\9\\end{bmatrix}$. \n",
    "                </li>\n",
    "                <li> \n",
    "                    Caso sua target-variable seja 3, $y^{(9)}=\\begin{bmatrix}3\\end{bmatrix}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            X é a matrix com os valores das features independentes de cada instância. Esses, por sua vez, estão contidos em um vetor-linha, e não coluna. Portanto, para um dataset com 2000 instâncias:\n",
    "            $$\n",
    "                X=\\begin{bmatrix} \n",
    "                (x^{(1)})^{T} \\\\\n",
    "                (x^{(2)})^{T} \\\\\n",
    "                \\vdots \\\\\n",
    "                (x^{(2000)})^{T}\n",
    "                \\end{bmatrix}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $h$ representa o nosso modelo de ML, que, para cada instância, lança uma previsão $ŷ$. Dessa maneira, $h(x^{(i)})=ŷ^{(i)}$\n",
    "             <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    O erro absoluto de um algoritmo regressor será $ŷ^{(i)}-y^{(i)}$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            $RMSE(X, h)$, $R^{2}(X,h)$ representam a função custo aplicada no set $X$ com o modelo $h$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8acb2-9142-4cc9-a0c6-0f8456501f98",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Métricas de Regressão</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             RMSE, também conhecida como Euclidean Norm ou $l_2$ norm. Sua fórmula é:\n",
    "            $$\n",
    "            RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}\n",
    "            $$\n",
    "        </li>\n",
    "        <li style='margin-top:10px'> \n",
    "            MAE, também conhecida como Manhattan Norm ou $l_1$ norm. Sua fórmula é:\n",
    "            $$\n",
    "                    MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^{m} |h(x^{(i)})-y^{(i)}| \n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f10c4-aa3b-4e1a-94c6-96e601fd087e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Início de Fato do Capítulo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Neste capítulo, aprenderemos os principais métodos de treinamento em ML, assim como alguns modelos mais simples.\n",
    "        </li>\n",
    "        <li> \n",
    "            Há maneiras mais diretas de alcançarmos os parâmetros ideais do modelo, e outras que envolvem iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb59b-7e9c-4c45-ac63-f5260c0db7ff",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Linear Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Regressão Linear é o modelo mais simples do ML. Consiste na soma ponderada de cada feature por um coeficiente. Uma previsão ŷ é dada como:\n",
    "            <p>$\n",
    "                ŷ=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $ ($\\theta_0$ é o termo de bias).\n",
    "            </p>         \n",
    "        </li>\n",
    "        <li> \n",
    "            Uma maneira mais concisa de formular essa equação é:\n",
    "            $$\n",
    "                ŷ=h_{\\theta}(x)=\\theta \\cdot x\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    $\\theta$ é o vetor com os coeficientes do modelo.\n",
    "                </li>\n",
    "                <li> \n",
    "                    x é o vetor com os valores das features.\n",
    "                </li>\n",
    "                <li> \n",
    "                    $\\theta \\cdot x$ é o produto escalar entre os vetores mencionados, igual a $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "            $\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A previsão do modelo ainda pode ser dada como $ŷ=\\theta^{T}x$, que é a multiplicação entre a transposta dos coeficientes e x.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b02d65-c5e2-41af-88b0-240a6d07501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2753030570997286, -1.2753030570997286)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "theta = np.random.random(size=3)\n",
    "x = np.random.normal(size=3)\n",
    "\n",
    "# Observe como o dot product e a multiplicação matricial se equivalem.\n",
    "theta.dot(x), theta.T@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f35b-7e12-4ea8-9840-07a6dfed8477",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A criação de uma Regressão Linear envolve encontrar o $\\theta$ que minimiza o MSE. Agora, é hora de explorarmos os diferentes métodos para que isso ocorra.        \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d21c99-f22f-4ad4-b84e-4f3edd712308",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Normal Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1914c93-553b-4b44-8413-dff53028206f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             A Equação Normal é uma maneira direta de alcançarmos o $\\theta$ minimizador do MSE. A sua fórmula é:\n",
    "            $$\n",
    "                \\theta=(X^{T}X)^{-1}X^{T}y\n",
    "            $$\n",
    "            <ul style='list-style-type:circle'> \n",
    "                <li> \n",
    "                    y é o vetor com as target variables.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adb2587-e96d-4b5b-8ae7-4d1333c2b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Equação Normal com o Numpy.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 10 + 4*X - np.random.normal(size=(1000,1))\n",
    "\n",
    "# Adicionando a coluna de bias a X.\n",
    "X_b = np.c_[np.ones((1000,1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T@X_b) @ X_b.T @ y\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f8e0d1-36cf-43c3-8d4b-6959bb626488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.88411678]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo uma previsão com uma nova instância.\n",
    "X_new = np.array([5])\n",
    "X_new_b = np.c_[np.ones((1,1)), X_new]\n",
    "\n",
    "X_new_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2b34e4-7734-4bfe-94d5-8fdf737b701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa mesma equação poderia ser escrita com os dot products entre as matrizes.\n",
    "np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400cb21-7938-416d-8abb-228bfbc5e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.02114206]), array([[3.97259494]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O 'LinearRegression' obtém os mesmos parâmetros.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397930-54b7-4070-a66f-d001487c7b52",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Moore-Penrose Inverse</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "             Uma outra maneira de obtermos os coeficientes da nossa Regressão Linear é com o produto escalar entre a matriz Moore-Penrose de X (conhecida também como pseudoinversa) e y.\n",
    "            $$\n",
    "                \\theta=X^{+}y\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603d0744-cdf7-4734-8b9a-5417c8e67f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114206],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'np.linalg.pinv' para computar a pseudoinversa de X.\n",
    "theta_best = np.linalg.pinv(X_b).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c43f5-4553-4a14-b4ff-4cf12d30cfee",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Computational Complexity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            A Time Complexity para treinamento de uma Regressão Linear com a Normal Equation pode ser de $O(n^{2.4})$ a $O(n^{3})$ e $O(n^{2})$ com a Inversa Moore-Penrose (sendo n o número de features). Ambos os métodos são $O(m)$ (m é o número de instâncias).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com previsões, as duas estratégias têm $O(n)$ e $O(m)$. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa20fd-c544-425f-b5b6-475275b08d05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gradient Descent</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            O autor introduz os conceitos de Descida de Gradiente e learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30555-4c42-4704-ba61-21d3dfd8342b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Batch Gradient Descent consiste em fazer a descida de gradiente com base em todo o conjunto de treino. O Gradiente de uma MSE para a Regressão Linear é dado como:\n",
    "            <p style='margin-top:10px'>\n",
    "            $$\n",
    "                \\nabla_{\\theta}MSE(\\theta)=\n",
    "                    \\begin{bmatrix} \n",
    "                        \\frac{\\delta}{\\delta\\theta_{0}}MSE(\\theta) \\\\ \n",
    "                        \\frac{\\delta}{\\delta\\theta_{1}}MSE(\\theta) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\frac{\\delta}{\\delta\\theta_{n}}MSE(\\theta) \n",
    "                    \\end{bmatrix}                    \n",
    "                    = \\frac{2}{m}X^{T}(X\\theta-y)\n",
    "            $$\n",
    "            </p>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99bf40b5-f24a-4f74-884c-0e50852f8f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.02114205],\n",
       "       [ 3.97259494]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a Batch Gradient Descent em código.\n",
    "\n",
    "eta = .01 # learning rate.\n",
    "n_iterations = 1000 # número de iterações total.\n",
    "\n",
    "# Inicialização aleatória dos parâmetros (segundo uma Distribuição Normal).\n",
    "theta = np.random.uniform(size=(2,1))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    #print(gradient, end='\\n\\n')\n",
    "    theta -=  eta*gradient\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21756b1-75fb-4561-b490-8f46169df9e7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stochastic Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Descida de Gradiente Estocástica surge pela demora na computação do Gradiente com o set de treino completo. Esse novo método propõe selecionar uma única instância aleatória para a conta.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, por outro lado, a descida de gradiente será menos regular. Outliers também ganham um peso maior na computação, o dado da iteração pode ser um desses. \n",
    "        </li>\n",
    "        <li> \n",
    "            Como o treinamento é mais instável, raramente alcançaremos a solução ótima, e sim, algo próximo. Uma estratégia para SGD é definir uma alta learning rate no início do fitting, para escaparmos dos mínimos locais, e reduzi-la, gradativamente, a fim de tentarmos nos alocar no mínimo global; se estivermos, de fato, na solução ótima, o vetor de gradiente estará severamente podado pelo menor $\\eta$, impedindo de escaparmos dela.\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: O scikit-learn oferece uma série de outras estratégias para tratamento das learning rates.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "91b8f008-8748-4fb3-b1f9-523258ce4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O treinamento é bastante similar ao de uma Rede Neural. À cada época, uma certa quantidade de instâncias são selecionadas para\n",
    "# o ajuste dos parâmetros.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SGDLinearRegression(RegressorMixin):\n",
    "    # 'theta' stores the LR coefficients.\n",
    "    theta = None\n",
    "    def __init__(self, eta0=.05, epochs=20, validation_split=.1, tol=1e-3, n_iter_no_change=3):\n",
    "        self.eta0 = eta0 # Initial Learning Rate.\n",
    "        self.epochs = epochs  # Number of Epochs.\n",
    "        self.validation_split = validation_split # % of training set held for validation.\n",
    "        self.tol = tol # Tolerance.\n",
    "        self.n_iter_no_change = n_iter_no_change # Number of epochs with no improvement for training halt.\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        SGDLinearRegression.theta = np.random.rand(X.shape[1],1)\n",
    "        # Segregating training from validation data.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = self.validation_split)\n",
    "        # Storing the 'theta' from the best epoch (lowest 'best_mse'). \n",
    "        best_theta, self.best_mse = SGDLinearRegression.theta, np.inf\n",
    "        no_change = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X_train)):\n",
    "                # Randomly selecting the epoch's iteration instance.\n",
    "                index = np.random.randint(len(X_train))\n",
    "                xi = X_train[index:index+1]\n",
    "                yi = y_train[index:index+1]\n",
    "                # Computing the gradient based on the instance's data.\n",
    "                gradients = (2 * xi.T.dot(xi.dot(SGDLinearRegression.theta ) - yi))\n",
    "                SGDLinearRegression.theta = SGDLinearRegression.theta - (gradients * self.eta0)\n",
    "            # At the end of the current epoch, the model performs a validation on 'X_val' and 'y_val'.\n",
    "            # If no improvement is occurs, the learning rate is divided by 5.\n",
    "            MSE = mean_squared_error(y_val, X_val.dot(SGDLinearRegression.theta))\n",
    "            if MSE > self.best_mse - self.tol:\n",
    "                no_change+=1\n",
    "                self.eta0 /= 5\n",
    "                # If the number of consecutive epochs with no MSE decay reaches 'n_iter_no_change', training is interrupted and the model\n",
    "                # is re-adjusted with the coefficients from the best epoch.\n",
    "                if no_change == self.n_iter_no_change:\n",
    "                    SGDLinearRegression.theta =  best_theta\n",
    "                    return self\n",
    "            else:\n",
    "                no_change = 0\n",
    "                self.best_mse = MSE\n",
    "                best_theta = SGDLinearRegression.theta            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e55553-7396-45f5-9873-943cf0a4c357",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Mini-Batch Gradient Descent</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Mini-Batch GD surge como um meio termo entre os dois métodos já apresentados. Sua estratégia é medir, à cada iteração, o vetor de gradiente com base em um conjunto de instâncias aleatórias (e não apenas uma, como no SGD).\n",
    "        </li>\n",
    "        <li> \n",
    "            Essa abordagem torna a descida de gradiente mais estável e com uma sensibilidade menor à presença de outliers.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ffb697-ce53-4d98-bfd9-a11d208bb241",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Polynomial Regression</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            Aqui, é apresentada a classe PolynomialFeatures, responsável por aumentar a dimensionalidade do dataset com base no produto entre as features.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f3865-2c8f-424b-92ff-6122d88733d3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Learning Curves</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li> \n",
    "            As Curvas de Aprendizado surgem como um recurso para analisarmos, em primeiro lugar, o quanto de instâncias seriam adequadas para que o modelo atinja o seu pleno potencial e, também, se está ocorrendo um under ou overfitting..\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "c87cbaf7-97e9-4044-8dbe-a42c174ed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Criando novas features.\n",
    "X = np.random.normal(size=(1000,1))\n",
    "y = 2.5*X**2 - 4*X + 5 + np.random.normal()\n",
    "\n",
    "_, train_mse, test_mse = learning_curve(LinearRegression(), X, y, cv=5, scoring='neg_mean_squared_error', \n",
    "                                        train_sizes=np.linspace(.05, 1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "42caaeab-0efe-4742-8a95-0a4c9811a2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41da3acfd0>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Observe, comom treinamos uma Regressão Linear em uma função quadrática, recebemos um erro elevado nos sets de treino e teste.\n",
    "# Ou seja, há um underfitting.\n",
    "plt.plot(_, -train_mse[:, 0], label='Train')\n",
    "plt.plot(_, -test_mse[:,0], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c6a83-8dd2-4902-8bd2-85a4c62a6978",
   "metadata": {},
   "source": [
    "<p style='color:red'> Agora, forçar um overfitting com PolynomialFeatures(degree=10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
