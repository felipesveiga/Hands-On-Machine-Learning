{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e91510-7582-402a-99b6-255ef2f2b365",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Deep Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc4a71-2ec3-4d00-8da6-53407feb1b05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Vanishing/Exploding Gradients</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em NN muito profundas, as camadas inferiores tendem a ter os seus pesos pouco modificados em cada ciclo do otimizador. Esse fenômeno é conhecido como o desaparecimento de gradientes (Gradient Vanishing).\n",
    "        </li>\n",
    "        <li> \n",
    "            Por outro lado, a seção mais superficial do modelo também é capaz de sofrer incoveniências. É comum os seus pesos terem uma alteração muito grande em cada ciclo, situação nomeada como Gradient Explosion.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462f015-2d9d-404d-b8c2-64ef2ec66d05",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Glorot and He Initialization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Xavier Glorot argumenta que as situações descritas podem ser evitadas caso a variância dos inputs e outputs de cada camada seja a mesma. Para que isso possa acontecer, o número de inputs (fan-in) e neurônios (fan-out) deve ser igual.\n",
    "        </li>\n",
    "        <li> \n",
    "            Além disso, a inicialização dos pesos deve ser extraída de uma Distribuição Normal de média 0 e variância ($\\sigma^{2}=\\frac{1}{fan_{avg}}$), ou de uma Uniforme entre -r e +r sendo $r=\\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "            <p style='margin-top:20px'>$$fan_{avg}=\\frac{fan_{in}+fan_{out}}{2}$$</p>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A inicialização LeCun utiliza as mesmas fórmulas que a Glorot, mas trocando $fan_{avg}$ por $fan_{in}$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c7049-ce9e-4e43-84d6-587f32d48641",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='initialization1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58fe18c-925a-4f22-96fb-a2af1ccf6682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f36edbea9d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# É possível definir a inicialização para a camada com 'kernel_initializer'.\n",
    "keras.layers.Dense(30, activation='relu' ,kernel_initializer='he_normal')\n",
    "\n",
    "# A customização pode se tornar um pouco mais específica com os objetos do módulo 'initializers'.\n",
    "dense_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(30, activation='relu', kernel_initializer=dense_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8d70f-86fa-4e68-9c41-f6c3341527bf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Non Saturating Activation Functions</h3>\n",
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> ReLU</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Foi uma das primeiras funções propostas para substituir a Sigmoid. Tem a vantagem de não saturar com valores positivos e é rápida para se computar.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por outro lado, a ReLU tem inclinação a cometer uma falha grave para learning rates elevadas: matar neurônios. Isso significa que uma boa parte dos TLU's da rede começa a lançar apenas 0 como output.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de18cb-3320-4654-882c-ffbab61a0dd0",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Leaky RELU</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A solução para esse infortúnio pode ser providenciada com a Leaky RELU. Com ela, podemos ajustar o parâmetro $\\alpha$, que indica a inclinação da função quando $x<0$.\n",
    "        </li>\n",
    "        <li> \n",
    "            Um paper sugeriu que valores em torno de $\\alpha=0.2$ induziam melhores performances.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1496f5-1295-4694-a9b2-6675ce1247fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando uma LeakyReLU com o módulo 'activations' do Keras.\n",
    "keras.activations.relu(alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f5470-f417-4eca-ad24-b687c3a1ab48",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Randomized Leaky RELU (RReLU)</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Variação da Leaky ReLU em que $\\alpha$ é definido aleatoriamente durante o treinamento e fixado como um número médio na fase de testes. Aparentemente, funciona como um redutor do risco de overfitting.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476261fd-2a9b-4be5-8dbd-165a5f9a472a",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Parametric Leaky RELU (PReLU)</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nessa outra variação do Leaky ReLU, $\\alpha$ passa a ter o seu valor ajustado pelo próprio treinamento do modelo. Pode ser excelente em grandes datasets de imagens, mas corre o risco de viciar o modelo em dados menores.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "454e631a-7196-4c00-af8e-08f70f388224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.activation.prelu.PReLU at 0x7f36e3012070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O PReLU deve ser aplicado com o módulo 'layers'.\n",
    "keras.layers.PReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e2e12-0e72-4edd-9f0d-59b2f1f70be6",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> ELU</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A ELU é uma função de ativação recentemente criada com o propósito de superar a ReLU e suas variantes. Podemos afirmar que esse objetivo foi alcançado com um grande sucesso, pois o tempo de treinamento foi reduzido e as redes ELU tiveram uma performance maior no set de teste.\n",
    "        </li>\n",
    "        <li> \n",
    "            O argumento $\\alpha$ indica o valor que ELU tendenciará para argumentos muito baixos.\n",
    "        </li>\n",
    "        <li> \n",
    "            Apenas observe que a fase de teste será mais lenta do que quando essa é feita com a ReLU.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='elu1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db4353c8-6eb8-429e-a688-3b8f5cdc5273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ELU com alpha=1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiVklEQVR4nO3de3hV9Z3v8fc3ITcgAYJJFMIdUaNyjQnao8x4G3S8jfVWIwWLqDNHW3VmrNapdR6nOk5t8Vg7Vg0UxeDlVBxqsXSkynipBAgCogG5ySWCCbeQEBKS7O/5IxtPpCCEbLL23vm8nmc/7Mvav/VJsvPhl7XXXsvcHRERiV0JQQcQEZH2UZGLiMQ4FbmISIxTkYuIxDgVuYhIjFORi4jEOBW5yDEyMzezoZFeVqStVOTSLmb2uZntM7PaVpenwo9NMrP3D/Gch8zsxUPcr7JrBzO7w8yWmFmDmc0IOo90nC5BB5C4cLm7zw86hPAF8G/A3wBpAWeRDqQZucQEM+tnZrPNrMrMdrSa9SeY2b+Y2UYzqzSzF8ysR/ixgeFZ/s1mttnMdpnZ7WZ2lpmtMLPdB8Y5zDoLzOzD8HJbzewpM0s+zLIzzOzXZvaWmdWY2f+Y2YCDFrvQzNaEx/uVmVn4uUPM7O3w17XdzErMrGdbv0fuPtvd/wvY0dbnSmxTkUvUM7NE4PfARmAg0Bd4OfzwpPDlr4HBQHfg4HIuBE4GrgeeAB4ALgROB64zs3GHWXUzcDdwAnA2cAHwD98QtQh4OLz8MqDkoMcvA84ChgPX0TJzBjDgUaAPcBrQD3jowJPM7Pfh8j/U5fffkEc6CRW5RMJ/HVQuUyI8fgEtJffP7r7X3evd/cC29yLgF+6+3t1rgfuBG8ys9WbDh8PP+W9gL/CSu1e6ewXwHjDqUCt19zJ3X+juTe7+OfAMcLjSB5jr7u+6ewMt/1mcbWb9Wj3+7+6+2903Ae8AI8PrWevub7l7g7tXAb9ovR53v8zdex7mctlRfQclrmkbuUTCVW3cRt4EJLW+w8wO3G48xPL9gI3u3nSIx/rQMlM/YCMtr+ucVvd92er6vkPc7n6okGY2jJZSzQe6hsctO9SyYZsPXHH3WjPbGc534P5trZatO7BeM8sB/g9wLpBOywRr1zesR+RrNCOXIGyiZRNJa4NoKfiKQyy/Geh/0Cz7gC+A1tui+4fH+fIQy7bV08Aq4GR3zwB+RMtmkMP5avZtZt2BzHC+I3kEcODM8Hpuar0eM/vDQXsFtb78oe1flsQbFbkcb2Zmqa0vwDzgVDObYGZJZpZJS5m9dphZ9yJgK/DvZtYtPM63wo+9BNxtZoPC5fkI8MphxmmrdGAPUGtmpwJ/f4TlLzWz/xV+Q/RhYKG7bz7Ccw6spxaoNrO+wD+3ftDdL3H37oe5XHJgOTPrEv7+JgKJ4e+T/uruBFTkEglvHDRLfL3VY+fQsvmi9WUncAlwG1AJrAR2c5iidPdm4HJgKC2z+S20vHEJMB2YCbwLbADqgTsj9HX9E3AjUAM8B7xyhOVnAT+h5esbQ8vM+mj8KzAaqAbmArOPJSzwL7R8f+8Lr3tf+D6Jc6YTS4i0X/gDOFvcXcUpHU4zchGRGKciFxGJcdq0IiIS4zQjFxGJcYHsmnTCCSf4wIEDg1i1iEjMKisr2+7uWQffH0iRDxw4kCVLlgSxahGRmGVmGw91vzatiIjEOBW5iEiMU5GLiMQ4FbmISIxTkYuIxLiIFbmZJZrZRzpjiYhIx4rkjPwHQHkExxMRkaMQkSI3s1zgb4HiSIwnIhJv6vY38dDvPqF636FOgtU+kZqRPwHcC4QOt4CZ3WpmS8xsSVVVVYRWKyIS/eobm7n1hTJe+PBzyjbujPj47S5yM7sMqHT3bzqXIe7+rLvnu3t+VtZffMJURCQuNTaHuGPWUt5fu53Hvj2c80/NOfKT2igSM/JvAVeY2efAy8D5ZvZiBMYVEYlpTc0h7np5GfPLK3n4ytO5Nr/fkZ90DNpd5O5+v7vnuvtA4AbgbXc/2lNciYjEpVDIufe1Fcz9eCs/uvRUJpw98LitS/uRi4hEmLvz4zkrmb20grsvHMat5w05ruuL6NEP3X0BsCCSY4qIxBJ356dzyykp3cRt4wbz/QuGHvd1akYuIhJBU9/6jOL3NzDx7AHcN/5UzOy4r1NFLiISIU8vWMeTb6/luvxcfnL56R1S4qAiFxGJiBkfbOCxeau4YkQfHr16OAkJHVPioCIXEWm3VxZv4qE3PuWivBx+ft0IEjuwxEFFLiLSLnOWVXDf7I85b1gWT904iqTEjq9VFbmIyDGat3Ib97y6nIKBmTxz0xhSuiQGkkNFLiJyDN5ZXcmdLy1leG4Ppk06i7TkYEocVOQiIm324bod3D6zjGE56cy4uYDuKRH9SE6bqchFRNqgbOMuJj+/mP6ZXZk5uZAeaUlBR1KRi4gcrZUV1Uyavojs9BRKbikks1ty0JEAFbmIyFFZva2GCdNKyUhLomTKWLIzUoOO9BUVuYjIEayvqqWouJSkxARmTSmkb8+0oCN9jYpcROQbbN5ZR1FxKSF3Sm4pZEDvbkFH+gsqchGRw9hWXU9RcSl7G5qYObmAk3PSg450SCpyEZFD2F7bQFHxQnbUNvD89wo4vU+PoCMdVrA7P4qIRKHddfu5qbiUit37eP7mAkb17xV0pG+kGbmISCs19Y1MnL6I9VV7eXZCPoWDewcd6YhU5CIiYXX7m5g8YwmffLGHXxWN5rxhWUFHOioqchERoL6xmVtfKGPJxp1MvX4kF+XlBB3pqGkbuYh0eo3NIe6YtZT3127nZ9cM5/IRfYKO1CaakYtIp9bUHOKul5cxv7ySh688nWvz+wUdqc1U5CLSaYVCzr2vrWDux1v50aWnMuHsgUFHOiYqchHplNydH89ZyeylFdx94TBuPW9I0JGOmYpcRDodd+enc8spKd3EbeMG8/0LhgYdqV1U5CLS6Ux96zOK39/AxLMHcN/4UzHr2JMlR5qKXEQ6lacXrOPJt9dyXX4uP7n89JgvcVCRi0gnMuODDTw2bxVXjOjDo1cPJyEh9kscVOQi0km8sngTD73xKRfl5fDz60aQGCclDipyEekE5iyr4L7ZH3PesCyeunEUSYnxVX3x9dWIiBxk3spt3PPqcgoGZvLMTWNI6ZIYdKSIU5GLSNx6Z3Uld760lOG5PZg26SzSkuOvxCECRW5mqWa2yMyWm9knZvavkQgmItIeH67bwe0zyxiWk86MmwvonhK/h5aKxFfWAJzv7rVmlgS8b2Z/cPeFERhbRKTNyjbuYvLzi+mf2ZWZkwvpkZYUdKTjqt1F7u4O1IZvJoUv3t5xRUSOxcqKaiZNX0R2egoltxSS2S056EjHXUS2kZtZopktAyqBt9y99BDL3GpmS8xsSVVVVSRWKyLyNau31TBhWikZaUmUTBlLdkZq0JE6RESK3N2b3X0kkAsUmNkZh1jmWXfPd/f8rKzYOOuGiMSO9VW1FBWXkpSYwKwphfTtmRZ0pA4T0b1W3H038A4wPpLjioh8k8076ygqLsXdmTWlkAG9uwUdqUNFYq+VLDPrGb6eBlwErGrvuCIiR2NbdT1FxaXsbWhi5uRChmanBx2pw0Vir5WTgOfNLJGW/xhedfffR2BcEZFvtL22gaLiheyobeDFWwrJ65MRdKRARGKvlRXAqAhkERE5arvr9nNTcSkVu/fx/M0FjOrfK+hIgdEnO0Uk5tTUNzJx+iLWV+3lue/mUzi4d9CRAqUiF5GYUre/ickzlvDJF3v4VdFozj1Ze8GpyEUkZtQ3NnPrC2Us2biTqdeP5KK8nKAjRYX4PfiAiMSVxuYQd8xayvtrt/P4tSO4fESfoCNFDc3IRSTqNTWHuOvlZcwvr+ThK0/nmjG5QUeKKipyEYlqoZBz72srmPvxVh649DQmnD0w6EhRR0UuIlHL3fnxnJXMXlrB3RcOY8p5g4OOFJVU5CISldydn84tp6R0E7eNG8z3LxgadKSopSIXkag09a3PKH5/AxPPHsB940/FLH5OlhxpKnIRiTpPL1jHk2+v5br8XH5y+ekq8SNQkYtIVJnxwQYem7eKK0b04dGrh5OQoBI/EhW5iESNVxZv4qE3PuXivBx+ft0IElXiR0VFLiJRYc6yCu6b/THjhmXxyxtHkZSoejpa+k6JSODmrdzGPa8up2BgJr++aQwpXRKDjhRTVOQiEqh3Vldy50tLGZ7bg2mTziItWSXeVipyEQnMh+t2cPvMMoblpDPj5gK6p+jwT8dCRS4igSjbuIvJzy+mf2ZXZk4upEdaUtCRYpaKXEQ63MqKaiZNX0R2egoltxSS2S056EgxTUUuIh1q9bYaJkwrJSMtiZIpY8nOSA06UsxTkYtIh9mwfS9FxaUkJSYwa0ohfXumBR0pLqjIRaRDbN5ZR9FzC3F3Zk0pZEDvbkFHihsqchE57rZV11NUXEptQxMzJxcyNDs96EhxRUUuIsfV9toGiooXsqO2gee/V0Ben4ygI8Ud7bQpIsfN7rr9TJi2iIrd+3j+5gJG9e8VdKS4pBm5iBwXNfWNTJy+iHWVtTz33XwKB/cOOlLcUpGLSMTV7W9i8owlfPLFHn5VNJpzT84KOlJcU5GLSETVNzZz28wylmzcydTrR3JRXk7QkeKetpGLSMQ0Noe4Y9ZS3luzncevHcHlI/oEHalT0IxcRCKiOeTc9coy5pdX8vBVZ3DNmNygI3UaKnIRabdQyLn3tyuYu2IrD1x6GhPGDgg6UqfS7iI3s35m9o6ZfWpmn5jZDyIRTERig7vz4zkreW3pFu6+cBhTzhscdKROJxLbyJuAf3T3pWaWDpSZ2Vvu/mkExhaRKObuPPJmOSWlm7h93BC+f8HQoCN1Su2ekbv7VndfGr5eA5QDfds7rohEv6nz1/DcexuYdM5Afjj+FMx0suQgRHQbuZkNBEYBpZEcV0Siz9ML1vHkn9ZwXX4uD16WpxIPUMSK3My6A68Bd7n7nkM8fquZLTGzJVVVVZFarYgEYMYHG3hs3iquGNGHR68eTkKCSjxIESlyM0uipcRL3H32oZZx92fdPd/d87Oy9CkvkVj1yuJNPPTGp1ycl8PPrxtBoko8cJHYa8WAaUC5u/+i/ZFEJFrNWVbBfbM/ZtywLH554yiSErUHczSIxE/hW8AE4HwzWxa+XBqBcUUkisxbuY17Xl1O4aBMfn3TGFK6JAYdScLavfuhu78P6G8rkTi2YHUld760lOG5PSieeBZpySrxaKK/i0TkG324bge3zSxjWE46M24uoHuKDtEUbVTkInJYZRt3Mfn5xfTP7MrMyYX0SEsKOpIcgopcRA5pZUU1k36ziOz0FEpuKSSzW3LQkeQwVOQi8hdWb6thwrRSMlKTKJkyluyM1KAjyTdQkYvI12zYvpei4lKSEhOYNaWQvj3Tgo4kR6AiF5GvbN5ZR9FzC3F3Zk0pZEDvbkFHkqOgIhcRALZV11NUXEptQxMzJxcyNDs96EhylLQfkYiwvbaBouKF7Ny7nxdvKSSvT0bQkaQNNCMX6eR21+1nwrRFVOzex/RJZzGyX8+gI0kbqchFOrGa+kYmTl/EuspanvtuPgWDMoOOJMdARS7SSdXtb2LyjCV88sUe/rNoNOeerKOSxioVuUgnVN/YzG0zy1iycSdP3DCSC/Nygo4k7aA3O0U6mcbmEHfMWsp7a7bz+LUjuGx4n6AjSTtpRi7SiTSHnLteWcb88koevuoMrhmTG3QkiQAVuUgnEQo59/52BXNXbOWBS09jwtgBQUeSCFGRi3QC7s6P56zktaVbuPvCYUw5b3DQkSSCVOQicc7deeTNckpKN3H7uCF8/4KhQUeSCFORi8S5qfPX8Nx7G5h0zkB+OP4UWk6zK/FERS4Sx55esI4n/7SG6/JzefCyPJV4nFKRi8SpGR9s4LF5q7hiRB8evXo4CQkq8XilIheJQ68s3sRDb3zKxXk5/Py6ESSqxOOailwkzsxZVsF9sz9m3LAsfnnjKJIS9Wse7/QTFokj81Zu455Xl1M4KJNf3zSGlC6JQUeSDqAiF4kTC1ZXcudLSxme24PiiWeRlqwS7yxU5CJx4MN1O7htZhnDctKZcXMB3VN0GKXOREUuEuPKNu5i8vOL6Z/ZlZmTC+mRlhR0JOlgKnKRGLayoppJv1lEdnoKJbcUktktOehIEgAVuUiMWr2thgnTSslITaJkyliyM1KDjiQBUZGLxKAN2/dSVFxKUmICs6YU0rdnWtCRJEAqcpEYs3lnHUXPLcTdmTWlkAG9uwUdSQKmIheJIduq6ykqLqW2oYmZkwsZmp0edCSJAhEpcjObbmaVZrYyEuOJyF/aXttAUfFCdu7dzwuTC8nrkxF0JIkSkZqRzwDGR2gsETnI7rr9TJi2iIrd+5g+6SxG9usZdCSJIhEpcnd/F9gZibFE5Otq6huZOH0R6ypree67+RQMygw6kkSZDttGbma3mtkSM1tSVVXVUasViWl1+5uYPGMJn3yxh/8sGs25J2cFHUmiUIcVubs/6+757p6flaUXo8iR1Dc2c9vMMpZs3MkTN4zkwrycoCNJlNIBGUSiUGNziDtmLeW9Ndt5/NoRXDa8T9CRJIpp90ORKNMccu56ZRnzyyt5+KozuGZMbtCRJMpFavfDl4APgVPMbIuZTY7EuCKdTSjk3PvbFcxdsZUHLj2NCWMHBB1JYkBENq24+3ciMY5IZ+bu/HjOSl5buoV7LhrGlPMGBx1JYoQ2rYhEAXfnkTfLKSndxO3jhnDn+UODjiQxREUuEgWmzl/Dc+9tYNI5A/nh+FMw08mS5eipyEUC9vSCdTz5pzVcn9+PBy/LU4lLm6nIRQI044MNPDZvFVeO7MMjV59JQoJKXNpORS4SkFcWb+KhNz7l4rwcHr92BIkqcTlGKnKRAMxZVsF9sz9m3LAsfnnjKJIS9asox06vHpEONm/lNu55dTmFgzJ5ZsIYUrokBh1JYpyKXKQDLVhdyZ0vLWV4bg+KJ55FapJKXNpPRS7SQT5ct4PbZpYxLCedGTcX0D1FhzqSyFCRi3SAso27mPz8Ygb07srMyYX0SEsKOpLEERW5yHG2sqKaSb9ZRHZ6Ci9OLiSzW3LQkSTOqMhFjqPV22qYMK2UjNQkSqaMJTsjNehIEodU5CLHyYbteykqLiW5SwKzphTSt2da0JEkTqnIRY6DzTvrKHpuIe5OyS2FDOjdLehIEsdU5CIRtq26nqLiUmobmpg5uZCh2elBR5I4p/2fRCJoe20DRcUL2bl3Py/eUkhen4ygI0knoBm5SITsrtvPhGmLqNi9j+mTzmJkv55BR5JOQkUuEgE19Y1M/M1i1lXW8tx38ykYlBl0JOlEVOQi7VS3v4nJM5bwSUU1/1k0mnNPzgo6knQyKnKRdqhvbOa2mWUs2biTJ24YyYV5OUFHkk5Ib3aKHKPG5hB3zFrKe2u28/i1I7hseJ+gI0knpRm5yDFoDjl3vbKM+eWVPHzVGVwzJjfoSNKJqchF2igUcu797QrmrtjKA5eexoSxA4KOJJ2cilykDdydB3+3kteWbuGei4Yx5bzBQUcSUZGLHC1355E3y3lx4SZuHzeEO88fGnQkEUBFLnLUps5fw3PvbWDSOQP54fhTMNPJkiU6qMhFjsLTC9bx5J/WcH1+Px68LE8lLlFFRS5yBDM+2MBj81Zx5cg+PHL1mSQkqMQluqjIRb7Bq4s389Abn/I3p+fw+LUjSFSJSxRSkYscxpxlFfxw9grGDcviye+MIilRvy4SnfTKFDmEeSu3cc+ryykclMkzE8aQ0iUx6EgihxWRIjez8Wa22szWmtl9kRhTJCgLVldy50tLGZHbg+KJZ5GapBKX6NbuIjezROBXwCVAHvAdM8tr77giQfhw3Q5um1nGKSem85ubC+ieosMRSfSLxIy8AFjr7uvdfT/wMnBlBMYV6VCl63fwvRmLGdC7Ky98r5AeaUlBRxI5KpEo8r7A5la3t4Tv+xozu9XMlpjZkqqqqgisViRyFn++k5tnLKZvrzRKbhlLZrfkoCOJHLUOe7PT3Z9193x3z8/K0oH3JXqUbdzFpOmLOLFHKrOmFJKVnhJ0JJE2iUSRVwD9Wt3ODd8nEvUWrt/BxOmLyM5I5aUpY8lOTw06kkibRaLIFwMnm9kgM0sGbgB+F4FxRY6r+Z9+ycTpi8jJSGHWlEJyMlTiEpva/Za8uzeZ2R3AH4FEYLq7f9LuZCLH0esfbeGf/u8KTu+TwYybC7RNXGJaRPatcvc3gTcjMZbI8eTuFL+3gZ++Wc45Q3rz7HfztYuhxDy9gqXTaGwO8eCcT3hp0SYuOeNEpl4/Uh/2kbigIpdOobqukb8vKePP63bwD381hH+6+BQdxVDihopc4t7ayhpunVnG5p11/PzaEXxbJ0qWOKMil7j2u+VfcN9rK+ianEjJLWMpGJQZdCSRiFORS1xqaGrmp3PLeeHDjeQP6MVTN47mxB7avVDik4pc4s66qlrufmUZK7ZUM+XcQdw7/lQdS1zimopc4oa7M3PhRh55s5zUpER+fdMYxp9xYtCxRI47FbnEhW3V9dz72gre/ayKccOy+Nk1w8nWJzWlk1CRS0wLhZyS0o38x7zVNIZCPHzVGdxU2F9nuZdORUUuMWvVtj3cP/tjPtq0m28N7c2/XXUmg07oFnQskQ6nIpeYU72vkafeXsNvPvicjLQkfnHdCP5uVF/NwqXTUpFLzGhqDjFr0SamvvUZu/c1cu2YXO6/5DR66YBX0smpyCXquTtvr6rk0T+sYm1lLWMHZ/Ivf5vHGX17BB1NJCqoyCVquTsLPqviibc+Y/mWagb27sozE8ZwcV6ONqOItKIil6jj7ry7ZjtT3/qMZZt307dnGo99+0yuHp2rD/aIHIKKXKLG/qYQbyz/guL3N1C+dQ99e6bxyN+dyTVjcknuogIXORwVuQSuuq6RWYs2MePPG/hyTwPDcrrzH98ezpWj+pDSRccLFzkSFbkEwt1Z/PkuXl60ibkfb6WhKcT/GnoCj317OOOGZWkbuEgbqMilQ+2obeD1jyp4adEm1lXtJT2lC9fl9+PGwv6cdlJG0PFEYpKKXI67PfWN/HHlNn63/Av+vG4HzSFndP+e/Mc1w7ls+El0TdbLUKQ99Bskx0X1vkb+57Mq5q74gndWV7G/KURurzRuO28wV47syyknpgcdUSRuqMglYjbu2Mv88kr+VP4lizbspCnkZKWncGNBf64Y2YdR/Xpq27fIcaAil2O2p76R0vU7+fO67by/ZjtrKmsBODm7O7ecO5iL8rIZ2a8XiTrJschxpSKXo1ZT38jyzdV8uH47H6zdwYotuwk5pHRJ4KyBmdxQ0J8LT8tmQG8dgVCkI6nI5ZDcnfXb97J04y6WbtrNR5t2sfrLGtwhMcEY2a8nd/z1UM4ecgKjB/TU/t4iAVKRC43NIdZX7eXTrdV8+sUeyrfWsPKLanbXNQKQntqFUf17Mf6MExndvxejB/Sie4peOiLRQr+NnUhTc4jNu/axrrKW9dtrWVtZS/nWGlZ/WcP+phAAyV0SOPXEdMaffiKj+vdkdP9eDMnqToK2c4tELRV5nGlsDrGtup7Nu+rYsnMfG3bsZX1VLeuq9rJxx14am/2rZXt3S+a0kzKYdM5A8k7KIK9PBoNP6EYXHZhKJKaoyGOIu7OrrpEv99RTWdPAl9X1bNm9jy276tiyax8Vu/axtXofof/f1XRJMAb07sqQrO5ceFoOQ7K6MTirO0OyutGzq07IIBIPVOQBa2wOsbuukV11+9m1d3/Lv3WN7Kht4Ms9DVTW1PPlngaqalqut55RA5jBiRmp5PZKo3BQJrm90sjt1fWrf0/qmapDv4rEuXYVuZldCzwEnAYUuPuSSISKJU3NIfY2NLOnvpHahiZq6puobWikpv7A9SZq65uoqW9kT33TV0V9oLRr6psOO3bPrklkp6eQnZ7K4KxuZKenkpPRcjs7I4Wc9FRO7JGqQ7yKdHLtnZGvBK4GnolAlnZxd5pCTmNziP1NIfY3h2hsdvY3hb5+X1P4/uZm9jeFqG8Msa+xmX37m9nX2Ex9q+t/eTtEfavHauub2NfYfMRsCQbpqUmkp3ahV9dkenVLZmDvri3XuyaT2S2Jnl2TyeyWTM+uSWR2a7k/NUm79InIkbWryN29HOiwj10/+ac1/NdHFeGSDoVL2r+67X7kMY4kwSAtKZG05ERSkxK/dr1HWhInZqSQltRyu3tKF9JTk+ie2oX01C6kp3QJX0+ie0oXMlJbbqclJeqj6SJy3MTUNvLs9BTy+mSQnJhAcpcEklr/m2hfXT9wf+vlksKPf/2+BNKSw2WdlEhqcsvjKl0RiSVHLHIzmw+ceIiHHnD3OUe7IjO7FbgVoH///kcdsLUbCvpzQ8GxPVdEJF4dscjd/cJIrMjdnwWeBcjPz4/ARhAREQHQ7g4iIjGuXUVuZn9nZluAs4G5ZvbHyMQSEZGj1d69Vl4HXo9QFhEROQbatCIiEuNU5CIiMU5FLiIS41TkIiIxzjwSn2tv60rNqoCNx/j0E4DtEYwTKcrVNsrVNsrVNtGaC9qXbYC7Zx18ZyBF3h5mtsTd84POcTDlahvlahvlaptozQXHJ5s2rYiIxDgVuYhIjIvFIn826ACHoVxto1xto1xtE6254Dhki7lt5CIi8nWxOCMXEZFWVOQiIjEupovczP7RzNzMTgg6C4CZPWxmK8xsmZn9t5n1CToTgJn9zMxWhbO9bmY9g84ELSfvNrNPzCxkZoHvKmZm481stZmtNbP7gs4DYGbTzazSzFYGnaU1M+tnZu+Y2afhn+EPgs4EYGapZrbIzJaHc/1r0JlaM7NEM/vIzH4fyXFjtsjNrB9wMbAp6Cyt/Mzdh7v7SOD3wIMB5zngLeAMdx8OfAbcH3CeAw6cvPvdoIOYWSLwK+ASIA/4jpnlBZsKgBnA+KBDHEIT8I/ungeMBf53lHy/GoDz3X0EMBIYb2Zjg430NT8AyiM9aMwWOTAVuBeImndr3X1Pq5vdiJJs7v7f7t4UvrkQyA0yzwHuXu7uq4POEVYArHX39e6+H3gZuDLgTLj7u8DOoHMczN23uvvS8PUaWsqpb7CpwFvUhm8mhS9R8XtoZrnA3wLFkR47JovczK4EKtx9edBZDmZmPzWzzUAR0TMjb+17wB+CDhGF+gKbW93eQhQUUywws4HAKKA04CjAV5svlgGVwFvuHhW5gCdomXyGIj1wu04scTx900mfgR/Rslmlwx3pZNTu/gDwgJndD9wB/CQacoWXeYCWP4lLOiLT0eaS2GVm3YHXgLsO+os0MO7eDIwMvxf0upmd4e6BvsdgZpcBle5eZmZ/Fenxo7bID3fSZzM7ExgELDczaNlMsNTMCtx9W1C5DqEEeJMOKvIj5TKzScBlwAXegR8eiNTJuztABdCv1e3c8H1yGGaWREuJl7j77KDzHMzdd5vZO7S8xxD0m8XfAq4ws0uBVCDDzF5095siMXjMbVpx94/dPdvdB7r7QFr+BB7dESV+JGZ2cqubVwKrgsrSmpmNp+VPuivcvS7oPFFqMXCymQ0ys2TgBuB3AWeKWtYyi5oGlLv7L4LOc4CZZR3YK8vM0oCLiILfQ3e/391zw511A/B2pEocYrDIo9y/m9lKM1tBy6afqNglC3gKSAfeCu8a+eugA0F0nbw7/GbwHcAfaXnj7lV3/ySoPAeY2UvAh8ApZrbFzCYHnSnsW8AE4Pzwa2pZeLYZtJOAd8K/g4tp2UYe0V39opE+oi8iEuM0IxcRiXEqchGRGKciFxGJcSpyEZEYpyIXEYlxKnIRkRinIhcRiXH/D1OoOGzWf6bjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a = 1 * (np.exp(np.linspace(-4,0, 1001, endpoint=False)) - 1)\n",
    "b = np.linspace(0, 4, 1001)\n",
    "a_b=np.concatenate((a,b))\n",
    "plt.plot(np.linspace(-4,4, 2002), a_b)\n",
    "plt.title('ELU com alpha=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11d636-8fbe-45f0-ba84-57fd09fbe418",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> SELU</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A SELU se propõe a ser a versão escalada da ELU. Tem o potencial de bater todas as outras funções sob as seguintes condições:\n",
    "            <ul style='list-style-type:lower-alpha'> \n",
    "                <li> \n",
    "                    A rede ser inteiramente feita com camadas densas (os neurônios recebem todos os outputs da camada anterior) e consistir em uma Sequential API.\n",
    "                </li>\n",
    "                <li> \n",
    "                    Os inputs estarem normalizados com média 0 e desvio-padrão 1.\n",
    "                </li>\n",
    "                <li> \n",
    "                    A inicialização dos pesos deve ser feita com LeCun normal.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> \n",
    "            Se esses requisitos forem cumpridos, a rede irá se auto-normalizar. Ou seja, os outputs das camadas terão média 0 e std 1.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f12c1ce-3542-4718-ae07-078417026906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f36d38dc520>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementando a SELU com a inicialização 'lecun_normal'\n",
    "keras.layers.Dense(45, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790d9f7-a8f4-4771-9c9d-1882c95cd57c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Batch Normalization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Batch Normalization consiste na normalização dos outputs de uma TLU (antes mesmo da aplicação da função de ativação). Funciona como um StandardScaler, do Scikit-Learn.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em treinamento, os batches são normalizados separadamente com as suas respectivas médias e variâncias, por isso, recomenda-se setar um tamanho razoável para eles (mais de 30 instâncias, se possível). Para teste, uma média móvel de todas as médias e desvios-padrões para a dada camada é feita para a normalização dos dados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='bn1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Apenas observe que, após a padronização dos dados à moda StandardScaler, os dados são multiplicados por um fator $\\gamma$ e somados por um $\\beta$. Esses são aprendidos durante o treinamento.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a80904-2af2-4e67-9ca2-d3f785e9a5fd",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Impactos da BN</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Batch Normalization é uma outra solução para a questão do desaparecimento/explosão de gradientes.\n",
    "        </li>\n",
    "        <li> \n",
    "            O treinamento é acelerado. Há também a regularização do modelo.\n",
    "        </li>\n",
    "        <li> \n",
    "            A BN torna possível que learning rates maiores alcancem a solução ótima.\n",
    "        </li>\n",
    "        <li> \n",
    "            As previsões podem ser mais devagares com a BN. Caso necessite de estimativas rápidas, recorra primeiro à função de ativação ELU juntamente com a inicialização He.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7867f7-2634-4607-b856-88d7850a18a7",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Implementing Batch Normalization with Keras</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06cd80f6-b7a1-4768-8b5b-b321df6cdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os criadores da BN recomendam que ela seja utilizada antes da função de ativação. Por isso, estamos usando TLU's lineares e uma\n",
    "# 'keras.layers.Activation()'.\n",
    "model = keras.models.Sequential([\n",
    "            keras.layers.Input(shape=[8]),\n",
    "            # Queremos que os inputs sejam normalizados logo de início.\n",
    "            keras.layers.BatchNormalization(),\n",
    "            \n",
    "            # Os dados passarão pelos TLU's sem função de ativação, tendo os seus outputs normalizados.\n",
    "            # Só após isso que a função de ativação será posta em ação.\n",
    "            keras.layers.Dense(90, kernel_initializer='he_normal',use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('elu'),\n",
    "    \n",
    "            keras.layers.Dense(70, kernel_initializer='he_normal',use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('elu'),\n",
    "    \n",
    "            # Normalizaremos as informações também antes da camada de output.\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e5515-feda-4f7f-8e80-31ff0d374dba",
   "metadata": {},
   "source": [
    "<p style='color:red'> Trecho grifado (p.337)</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
