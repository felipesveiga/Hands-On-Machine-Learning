{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e91510-7582-402a-99b6-255ef2f2b365",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Training Deep Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc4a71-2ec3-4d00-8da6-53407feb1b05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Vanishing/Exploding Gradients</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em NN muito profundas, as camadas inferiores tendem a ter os seus pesos pouco modificados em cada ciclo do otimizador. Esse fenômeno é conhecido como o desaparecimento de gradientes (Gradient Vanishing).\n",
    "        </li>\n",
    "        <li> \n",
    "            Por outro lado, a seção mais superficial do modelo também é capaz de sofrer incoveniências. É comum os seus pesos terem uma alteração muito grande em cada ciclo, situação nomeada como Gradient Explosion.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462f015-2d9d-404d-b8c2-64ef2ec66d05",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Glorot and He Initialization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Xavier Glorot argumenta que as situações descritas podem ser evitadas caso a variância dos inputs e outputs de cada camada seja a mesma. Para que isso possa acontecer, o número de inputs (fan-in) e neurônios (fan-out) deve ser igual.\n",
    "        </li>\n",
    "        <li> \n",
    "            Além disso, a inicialização dos pesos deve ser extraída de uma Distribuição Normal de média 0 e variância ($\\sigma^{2}=\\frac{1}{fan_{avg}}$), ou de uma Uniforme entre -r e +r sendo $r=\\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "            <p style='margin-top:20px'>$$fan_{avg}=\\frac{fan_{in}+fan_{out}}{2}$$</p>\n",
    "        </li>\n",
    "        <li> \n",
    "            Nota: A inicialização LeCun utiliza as mesmas fórmulas que a Glorot, mas trocando $fan_{avg}$ por $fan_{in}$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c7049-ce9e-4e43-84d6-587f32d48641",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='initialization1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58fe18c-925a-4f22-96fb-a2af1ccf6682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f36ffe18220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "keras.initializers.initializers_v2.VarianceScaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e5515-feda-4f7f-8e80-31ff0d374dba",
   "metadata": {},
   "source": [
    "<p style='color:red'> Montar os inicializadores com o Keras</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
