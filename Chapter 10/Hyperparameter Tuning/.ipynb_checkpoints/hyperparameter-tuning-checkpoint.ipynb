{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921204c-9b3d-45d5-a957-bf2bebe2c0a0",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Tunagem de Hiperparâmetros</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fa685-7a4b-4356-ac0d-ab004c1016d8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Otimização Bayesiana</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Otimização Bayesiana consiste na estimação da função-custo para um certo modelo. Seu objetivo é encontrar o conjunto de hiperparâmetros que retorne a melhor performance.\n",
    "        </li>\n",
    "        <li> \n",
    "            Inicialmente, combinações aleatórias de configurações são escolhidas para treinar e avaliar o modelo. Ao final, os ajustes que providenciaram os melhores desempenhos passam a ser considerados como promissores. Dessa forma, hiperparâmetros de valores próximos com os desses são feitos, sob a expectativa de que obtenhamos resultados ainda melhores!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff827b22-d1c1-4110-9fcb-934e32eb061b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> O tradeoff entre Exploration e Exploitation</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Exploration consiste na mera exploração aleatória do espaço de hiperparâmetros. Por outro lado, a Exploitation é a exploração feita pelo modelo dos espaços considerados promissores para uma performance satisfatória. \n",
    "        </li>\n",
    "        <li> \n",
    "            É muito importante equilibrarmos o foco que o modelo dará a ambas essas atividades. Um modelo com uma taxa de Exploration muito alta não aproveitará os espaços promissores, ocasionando em um underfitting. Por outro lado, uma Exploitation elevada fará com que o algoritmo não explore suficientemente o espaço de hiperparâmetros, correndo o risco de ter a sua performance presa no mínimo local.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b563f-2a05-479f-b7ae-10c18b4ec7e0",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Tunagem de Parâmetros com o KerasTuner</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O KerasTuner é uma biblioteca voltada à otimização de modelos Keras (apesar de também suportar os algoritmos do scikit-learn). Com ela, somos capazes de realizar Random Searches e Otimizações Bayesianas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86ecb7f9-d787-47ab-ae8a-63f903f864b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos construir um modelo simples no Keras para ilustrarmos o uso do KerasTuner.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# O espaço em que os hiperparâmetros poderão ser inseridos é definido pela classe Hyperparameter.\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Inicializando o modelo com uma camada de Input.\n",
    "    model.add(Input(shape=(8)))\n",
    "    \n",
    "    # A quantidade de camadas é aleatória, de um a quatro.\n",
    "    for i in range(hp.Int('n-hidden',min_value=1, max_value=4, step=1)):\n",
    "        # Para cada hidden layer, o número de TLU's varia de 15 a 50, com uma diferença de, no mínimo, 10 unidades à cada iteração.\n",
    "        # O modelo poderá ter uma activation function como 'relu' ou 'tanh'.\n",
    "        model.add(Dense(hp.Int(f'units{i}',min_value=15, max_value=50, step=10),\n",
    "                       activation=hp.Choice('activation', ['relu', 'tanh'])))\n",
    "                  \n",
    "    # Como faremos uma regressão, a camada de output deverá ter apenas um neurônio.\n",
    "    model.add(Dense(1))\n",
    "                  \n",
    "    # Ajustando a learning rate do nosso otimizador.\n",
    "    lr = hp.Float('learning_rate', min_value=.001, max_value=.3, sampling='log')\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "                  \n",
    "    # A função deve retornar o modelo compilado.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa1917-ba18-4c9f-96da-2fcf81fd3f3a",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Random Search com o KerasTuner</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Ao invés de recorrermos ao Scikit-Learn, podemos performar uma Random Search com o KerasTuner. Isso remove a necessidade de termos que envelopar o modelo com um wrapper.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f499b73-4a6c-4be0-9d52-6c17e2b62556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "n-hidden (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': None}\n",
      "units0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 15, 'max_value': 50, 'step': 10, 'sampling': None}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.3, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "# 'objective' corresponde à métrica a ser monitorada para a consideração do melhor modelo.\n",
    "\n",
    "# 'max_trials' é a quantidade de configurações distintas a serem criadas e testadas.\n",
    "\n",
    "# 'executions_per_trial' equivale a quantos modelos serão criados e avaliados na mesma iteração. Isso serve para a redução do risco\n",
    "# de uma inicialização ruim dos weights atrapalhar o desempenho do algoritmo.\n",
    "\n",
    "# 'directory' e 'project_name' são os diretórios onde os registros das iterações serão anotados.\n",
    "\n",
    "# 'overwrite' \n",
    "tuner = RandomSearch(build_model, objective='val_loss', max_trials=2, executions_per_trial=2, seed=42,\n",
    "            directory='random_search', project_name='keras_rdn_search', overwrite=True)\n",
    "\n",
    "# 'search_space_summary' disponibiliza um relatório sobre os espaços dos hiperparâmetros.\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fecad58-17cb-48ca-bf13-e2551218349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos, brevemente, preparar os dados para treinamento, validação e teste.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Carregando os dados e segregando os diferentes sets.\n",
    "X,y = fetch_california_housing(return_X_y=True)\n",
    "X_, X_test, y_, y_test = train_test_split(X,y, test_size=.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=.2, random_state=42)\n",
    "\n",
    "# Modificando as escalas das features.\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65f6ae6-808f-42fa-81fe-af1d47490298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 01m 44s]\n",
      "val_loss: 0.34737756848335266\n",
      "\n",
      "Best val_loss So Far: 0.34737756848335266\n",
      "Total elapsed time: 00h 03m 17s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Hora de realizarmos a nossa pesquisa de espaço. Para tornar o processo mais rápido, usaremos Early Stopping.\n",
    "# 'search' recebe os mesmos argumentos do que o 'fit' de um modelo avulso.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "tuner.search(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bbe0714-090d-4e41-9b53-1d8bd9db03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in random_search/keras_rdn_search\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7f0555f561f0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n-hidden: 4\n",
      "units0: 25\n",
      "activation: tanh\n",
      "learning_rate: 0.0014644674369313304\n",
      "units1: 35\n",
      "units2: 25\n",
      "units3: 15\n",
      "Score: 0.34737756848335266\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n-hidden: 3\n",
      "units0: 15\n",
      "activation: tanh\n",
      "learning_rate: 0.004714855288349009\n",
      "units1: 15\n",
      "units2: 15\n",
      "Score: 0.3563932031393051\n"
     ]
    }
   ],
   "source": [
    "# 'results_summary' dá um relatório sobre as 10 melhores iterações.\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b84a5c-d1a6-42b6-9cdd-04145f11b029",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Consultando os melhores resultados</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            'get_best_models' disponibiliza os melhores modelos construídos.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c49212f-29cb-4bf5-a944-8d38c6744c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 25)          225       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 35)          910       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 25)          900       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 15)          390       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 1)           16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,441\n",
      "Trainable params: 2,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Pegando o melhor algoritmo feito.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Montando o modelo.\n",
    "best_model.build()\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31396071-a69f-4824-85e9-d528a407c969",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Treinando o modelo novamente</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Com as melhores configurações em mãos, seria aconselhável recriarmos o modelo para treiná-lo em todo o set de treino.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3b75394-7e57-479e-b399-c36b0bd64ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 8), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (32, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 8), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (32, 8).\n",
      "516/516 [==============================] - 2s 2ms/step - loss: 0.7084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04bc3f8520>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo os melhores ajustes e criando o modelo.\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "model = build_model(best_hyper)\n",
    "\n",
    "# Concatenando os dados de treino e validação em um único.\n",
    "X_all_scaled = np.concatenate((X_train_scaled, X_val_scaled))\n",
    "y_all = np.concatenate((y_train, y_val))\n",
    "\n",
    "model.fit(X_all_scaled,  y_all, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ca5e2-9ff9-4d08-b468-577b67a7b9af",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Bayesian Optimization no Keras Tuner</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tendo completado o artigo introdutório do KerasTuner, vamos aplicar uma Otimização Bayesiana com a biblioteca.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aece61d0-6451-4ab7-85d5-d9569dd1170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 10s]\n",
      "val_loss: 1.4547815322875977\n",
      "\n",
      "Best val_loss So Far: 0.31540486216545105\n",
      "Total elapsed time: 00h 04m 47s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "# 'beta' regula o tradeoff entre exploitation e exploration; quanto mais elevado, maior a exploration!\n",
    "\n",
    "# 'num_initial_points' indica a quantidade de amostras de treino aleatórias que inicialmente treinarão o modelo.\n",
    "bayes_tuner = BayesianOptimization(build_model, objective='val_loss', max_trials=10, num_initial_points=20, beta=2.1, seed=42\n",
    "                                ,directory='bayes_opt', project_name='keras_bayes_opt', overwrite=True)\n",
    "\n",
    "# Performando a optimização.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "bayes_tuner.search(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b825632-bfb8-4ef4-a335-450ec223adc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 2ms/step - loss: 0.3159\n"
     ]
    }
   ],
   "source": [
    "# Avaliando o melhor modelo.\n",
    "best_model = bayes_tuner.get_best_models()[0]\n",
    "best_model.evaluate(X_test_scaled, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f9bf2-3d88-4f86-a03e-c47a0bcaa574",
   "metadata": {},
   "source": [
    "<p style='color:red'> Tópico \"Specify the tuning objective\"</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
