{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f921204c-9b3d-45d5-a957-bf2bebe2c0a0",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Tunagem de Hiperparâmetros</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fa685-7a4b-4356-ac0d-ab004c1016d8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Otimização Bayesiana</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Otimização Bayesiana consiste na estimação da função-custo para um certo modelo. Seu objetivo é encontrar o conjunto de hiperparâmetros que retorne a melhor performance.\n",
    "        </li>\n",
    "        <li> \n",
    "            Inicialmente, combinações aleatórias de configurações são escolhidas para treinar e avaliar o modelo. Ao final, os ajustes que providenciaram os melhores desempenhos passam a ser considerados como promissores. Dessa forma, hiperparâmetros de valores próximos com os desses são feitos, sob a expectativa de que obtenhamos resultados ainda melhores!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff827b22-d1c1-4110-9fcb-934e32eb061b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> O tradeoff entre Exploration e Exploitation</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Exploration consiste na mera exploração aleatória do espaço de hiperparâmetros. Por outro lado, a Exploitation é a exploração feita pelo modelo dos espaços considerados promissores para uma performance satisfatória. \n",
    "        </li>\n",
    "        <li> \n",
    "            É muito importante equilibrarmos o foco que o modelo dará a ambas essas atividades. Um modelo com uma taxa de Exploration muito alta não aproveitará os espaços promissores, ocasionando em um underfitting. Por outro lado, uma Exploitation elevada fará com que o algoritmo não explore suficientemente o espaço de hiperparâmetros, correndo o risco de ter a sua performance presa no mínimo local.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b563f-2a05-479f-b7ae-10c18b4ec7e0",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Tunagem de Parâmetros com o KerasTuner</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O KerasTuner é uma biblioteca voltada à otimização de modelos Keras (apesar de também suportar os algoritmos do scikit-learn). Com ela, somos capazes de realizar Random Searches e Otimizações Bayesianas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86ecb7f9-d787-47ab-ae8a-63f903f864b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos construir um modelo simples no Keras para ilustrarmos o uso do KerasTuner.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# O espaço em que os hiperparâmetros poderão ser inseridos é definido pela classe Hyperparameter.\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Inicializando o modelo com uma camada de Input.\n",
    "    model.add(Input(shape=(None, 8)))\n",
    "    \n",
    "    # A quantidade de camadas é aleatória, de um a quatro.\n",
    "    for i in range(hp.Int('n-hidden',min_value=1, max_value=4, step=1)):\n",
    "        # Para cada hidden layer, o número de TLU's varia de 15 a 50, com uma diferença de, no mínimo, 10 unidades à cada iteração.\n",
    "        # O modelo poderá ter uma activation function como 'relu' ou 'tanh'.\n",
    "        model.add(Dense(hp.Int(f'units{i}',min_value=15, max_value=50, step=10),\n",
    "                       activation=hp.Choice('activation', ['relu', 'tanh'])))\n",
    "                  \n",
    "    # Como faremos uma regressão, a camada de output deverá ter apenas um neurônio.\n",
    "    model.add(Dense(1))\n",
    "                  \n",
    "    # Ajustando a learning rate do nosso otimizador.\n",
    "    lr = hp.Float('learning_rate', min_value=.001, max_value=.3, sampling='log')\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "                  \n",
    "    # A função deve retornar o modelo compilado.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa1917-ba18-4c9f-96da-2fcf81fd3f3a",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Random Search com o KerasTuner</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Ao invés de recorrermos ao Scikit-Learn, podemos performar uma Random Search com o KerasTuner. Isso remove a necessidade de termos que envelopar o modelo com um wrapper.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f499b73-4a6c-4be0-9d52-6c17e2b62556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "n-hidden (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': None}\n",
      "units0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 15, 'max_value': 50, 'step': 10, 'sampling': None}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.3, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "# 'objective' corresponde à métrica a ser monitorada para a consideração do melhor modelo.\n",
    "\n",
    "# 'max_trials' é a quantidade de configurações distintas a serem criadas e testadas.\n",
    "\n",
    "# 'executions_per_trial' equivale a quantos modelos serão criados e avaliados na mesma iteração. Isso serve para a redução do risco\n",
    "# de uma inicialização ruim dos weights atrapalhar o desempenho do algoritmo.\n",
    "\n",
    "# 'directory' e 'project_name' são os diretórios onde os registros das iterações serão anotados.\n",
    "\n",
    "# 'overwrite' \n",
    "tuner = RandomSearch(build_model, objective='val_loss', max_trials=3, executions_per_trial=3, seed=42,\n",
    "            directory='random_search', project_name='keras_rdn_search', overwrite=True)\n",
    "\n",
    "# 'search_space_summary' disponibiliza um relatório sobre os espaços dos hiperparâmetros.\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fecad58-17cb-48ca-bf13-e2551218349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos, brevemente, preparar os dados para treinamento, validação e teste.\n",
    "from sklearn.\n",
    "import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Carregando os dados e segregando os diferentes sets.\n",
    "X,y = fetch_california_housing(return_X_y=True)\n",
    "X_, X_test, y_, y_test = train_test_split(X,y, test_size=.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=.2, random_state=42)\n",
    "\n",
    "# Modificando as escalas das features.\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d65f6ae6-808f-42fa-81fe-af1d47490298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 02m 13s]\n",
      "val_loss: 0.33190874258677167\n",
      "\n",
      "Best val_loss So Far: 0.33190874258677167\n",
      "Total elapsed time: 00h 06m 55s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Hora de realizarmos a nossa pesquisa de espaço. Para tornar o processo mais rápido, usaremos Early Stopping.\n",
    "# 'search' recebe os mesmos argumentos do que o 'fit' de um modelo avulso.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "tuner.search(X_train_scaled, y_train, epochs=50, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f9bf2-3d88-4f86-a03e-c47a0bcaa574",
   "metadata": {},
   "source": [
    "<p style='color:red'> Tópico \"Start the search\"</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
