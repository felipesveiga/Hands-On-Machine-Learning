{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286178-8a12-439a-a71f-da63dfc95e65",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Introduction to Artificial Neural Networks With Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b829b0f-8853-4031-a37d-009f01d2674d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf0b2d-3d7a-492a-9afb-9e694abe488d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> TLU</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma Threshold Logic Unit (TLU) é uma estrututa que simula um neurônio. Ela recebe uma série de features e computa a soma ponderada delas (assim como um algoritmo de regressão simples). Mas após o cálculo, a equação é utilizada como argumento de uma função conhecida como step function.\n",
    "        </li>\n",
    "        <li> \n",
    "            Pode ser utilizada para tarefas de classificação binária. Assim como na Regressão Logística, caso o resultado obtido esteja acima de um threshold, a instância é designada à classe positiva; senão, à classe negativa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Estrutura de uma TLU</h1>\n",
    "    <img src='tlu1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ee9ec-5ae4-4b66-800f-8e6df13fb189",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> As step functions mais comuns da TLU</h1>\n",
    "    <img src='tlu2.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12996bf-94ed-4310-9f88-c169373767f0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Perceptron</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Um Perceptron consiste em uma camada de várias TLU's. Caso os neurônios sejam conectados com todos os inputs provenientes do nível anterior, dizemos que a camada em questão é uma <em>fully connected layer.</em>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Um Perceptron de 3 TLU's</h1>\n",
    "    <img src='perceptron1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada46e6-5721-4164-a576-96b0419da0e7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Perceptron acima pode realizar 3 classificações binárias distintas, tornando-o um modelo multioutput.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, além das features, um termo bias (nesse caso, 1) também pode ser inserido como input.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='perceptron2.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Note como a função de previsão para uma fully conneted layer é muito similar à função afim. A letra $\\phi$ é a função de ativação - no contexto do TLU's, ela é chamada de step function.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3354dd-ce5a-4d04-8e58-09f0ead3aa31",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Durante a fase de treino, os neurônios de input que melhor contribuírem para previsões corretas recebem um maior peso ao alimentarem o TLU de output.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3867454-d312-4bd2-bae1-e6aba03f107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As fronteiras de decisão do objeto Perceptron são lineares. Portanto, não espere grandes resultados em datasets complexos!\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X, y = X[:, [2,3]], (y==0).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e18ecab0-4486-4b4b-a584-02035dd90017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O objeto 'Perceptron' admite Early Stopping!\n",
    "from sklearn.metrics import roc_auc_score\n",
    "per_clf = Perceptron(early_stopping=True, n_iter_no_change=8).fit(X_train, y_train)\n",
    "roc_auc_score(y_test, per_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2b6f7-c060-45e6-920f-aa08ce22ba13",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Multi-Layer Perceptron and Backpropagation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Considerando o desempenho limitado dos Perceptrons, foram criados os Multi-Layer Perceptrons. Eles possuem uma ou mais camadas de TLU's, conhecidas como hidden layers responsáveis por abastecerem a output layer - que possui o(s) TLU(s) que farão as previsões finais.\n",
    "        </li>\n",
    "        <li> \n",
    "            Todas as camadas dos MLP's são fully connected layers e, com exceção do nível de output, contam com um neurônio de bias.\n",
    "        </li>\n",
    "        <li> \n",
    "            Como os valores são transmitidos das camadas inferiores às superiores (e não vice-versa), a arquitetura de um MLP é classificada como uma Feedforward Neural Network (FNN).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3108bd9-d960-43ae-ba4f-0b09f70d94e8",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Treinando um MLP</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O fitting de um Multi-Layer Perceptron ocorre de maneira iterativa com mini-batches do set de treino.\n",
    "        </li>\n",
    "        <li> \n",
    "            Cada instância do subconjunto alimenta as hidden layers até que o TLU de output faça uma previsão. O valor lançado é comparado com o verdadeiro rótulo/número, sendo o nível de erro avaliado por uma loss function.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em seguida, é analisado o impacto de cada neurônio nas hidden layers para o erro. Com isso, os pesos de cada conexão são atualizados para a próxima instância.\n",
    "        </li>\n",
    "        <li> \n",
    "            É importante ressaltar que os pesos de cada conexão sejam inicializados de maneira aleatória, sem isso, o processo de treinamento pode não convergir a uma solução aleatória.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab392eed-3cb9-4319-b4a7-4b6e8ddedd29",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Regression MLP's</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor oferece aqui algumas dicas de montagem de MLP's em regressões.\n",
    "        </li>\n",
    "        <li>\n",
    "            Colocar uma função de ativação nos neurônios de output não é algo comum, mas caso queira que os valores previstos sejam sempre positivos, podemos recorrer ao uso da ReLU ou softplus. Se desejar que os outputs caiam dentro de um certo intervalo numérico, use a Logistic Function ou Hyperbolic Tangent Function (não se esqueça de atualizar os valores-alvo dentro do intervalo requerido [0-1] para logística e [-1,1] para hiperbólica).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com relação à loss function, busque usar o Mean Absolute Error caso o dataset tenha outliers. Além disso, a Hubber Loss pode ser uma opção intermediária entre o MAE e o MSE, tendo menor sensibilidade a outliers, uma alta taxa de precisão e de convergência.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324d644-b9dc-4433-9c32-77e0cc5e5842",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Classification MLP's</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Com relação a classificações, os neurônios de output recebem uma Regressão Logística, ou uma softmax para computar as probabilidades de classe.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em classificações binárias, um neurônio de output para cada condição deve existir; para as multi-classe, um neurônio terá que existir para cada categoria.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551aecd-9722-4094-90ad-b10f1c4e33f4",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Installing TensorFlow 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f422c10a-94bd-4ab3-ad17-85a8d378ad34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.9.1', '2.9.0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d717b1-93ce-4158-8515-bc9525ef1872",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Building an Image Classifier Using the Sequential API</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Iremos aqui utilizar um dataset para classificação de imagens mais sofisticado do que o MNIST. Esse, o Fashion MNIST, contém imagens sobre peças de vestuário.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d8d55-0bd7-4baf-9a8d-a9abd9245cf7",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Using Keras to Load the Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0402ecd5-868d-4db8-bd63-646d56c5539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "# Segregando os dados de treino e teste (é importante que os X's e y's estejam entre parênteses).\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Agora, iremos criar o set de validação. Vamos aproveitar e normalizar os dados.\n",
    "X_valid, X_train = X_train_full[:5000]/255, X_train_full[5000:]/255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Agora, daremos um nome inteligível às classes-alvo.\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8179f4d-17b9-4a31-a770-ad6cb13a6018",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Creating the Model Using the Sequential API</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nosso primeiro modelo no TensorFlow será um MLP de duas hidden layers.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2a87e23-3b2d-4fc6-b237-bc45cda7b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A API Sequential permitirá empilharmos as camadas de TLU's.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Agora, adionaremos as camadas de TLU's.\n",
    "\n",
    "# A camada 'Flatten' basicamente planifica o array dos dados de cada instância (de 28x28 para 784x1).\n",
    "# Alternativamente, poderíamos ter adicionado um 'keras.layers.InputLayer(input_shape=[28,28])'.\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "\n",
    "# Nossa rede neural terá duas hidden layers de tamanhos distintos; ambas com uma relu como activation function.\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Por se tratar de uma tarefa para classificação múltipla, teremos que criar 10 TLU's e definir 'softmax' como função de \n",
    "# ativação na camada para outputs.\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Nota: poderíamos ter definido activation=keras.activations.relu, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "852f8827-7d01-4566-aaf3-f2dd968a8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Também conseguiríamos construir a rede neural por meio de uma lista de camadas.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(100, activation = keras.activations.relu),\n",
    "    keras.layers.Dense(10, activation=keras.activations.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39fa9442-2578-4fea-b2d1-1d51614fc83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ba4e1-2282-4dcd-afd0-c4a16a25bf4b",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Note a tremenda quantidade de parâmetros que nosso modelo tem. Apesar de isso conferir bastante flexibilidade no seu treinamento, ele pode sofrer overfitting.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b98872e-b215-47c6-8f08-dee5d9989eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f919d1554c0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O atributo 'layers' menciona todas as camadas do modelo.\n",
    "print(model.layers[0].name)\n",
    "\n",
    "# Para obter uma camada em específico, use 'get_layer'.\n",
    "dense_21 = model.get_layer('dense_21')\n",
    "dense_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "51802bb9-ffe8-487f-bcef-e564394761e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# É possível visualizar a nossa rede com o seguinte método\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab14cf3-420f-409b-8e25-15fec524a137",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Weights e Biases</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como já espeficiamos o formato do input na primeira camada, o modelo já inicializou os pesos de conexões e os termos de bias.\n",
    "        </li>\n",
    "        <li> \n",
    "            Caso queira uma outra estratégia na inicialização, procure explorar os argumentos <em> kernel_initializer</em> e <em> bias_initializer</em>, que serão ensinados no próximo capítulo!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f42b2cf3-18d1-499c-a282-1aac054a4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0552734  -0.01370593  0.00293531 ...  0.05770957  0.01238564\n",
      "   0.02020808]\n",
      " [-0.05668053  0.07336296 -0.0106301  ... -0.06688693 -0.04526081\n",
      "  -0.02096421]\n",
      " [ 0.01877947  0.02175879 -0.01822219 ... -0.04045416  0.03780955\n",
      "  -0.07247542]\n",
      " ...\n",
      " [-0.0512436  -0.07093383 -0.06993344 ... -0.0512969  -0.00860279\n",
      "   0.00362902]\n",
      " [-0.02233877  0.04725705 -0.04704604 ...  0.05243443  0.04636137\n",
      "   0.0721141 ]\n",
      " [ 0.0703267   0.05418946  0.02972484 ... -0.04649205  0.02327072\n",
      "  -0.06621653]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como já espeficiamos o formato do input na primeira camada, o modelo já inicializou os pesos de conexões e os termos de bias.\n",
    "\n",
    "# Quais são os weights e biases da camada 'dense_21'?\n",
    "weights, bias = dense_21.get_weights()\n",
    "print(weights)\n",
    "\n",
    "# Veja que todos os biases foram criados como 0.\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad483b-99b4-4a2d-88ff-b32405571fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv /Users/felipeveiga/Desktop/Screen\\ Shot\\ 2022-07-26\\ at\\ 08.44.52.png ./perceptron2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb0468-948d-483d-98d2-281ff88808aa",
   "metadata": {},
   "source": [
    "<p style='color:red'> Compiling the Model.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
