{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd286178-8a12-439a-a71f-da63dfc95e65",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Introduction to Artificial Neural Networks With Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b829b0f-8853-4031-a37d-009f01d2674d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf0b2d-3d7a-492a-9afb-9e694abe488d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> TLU</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma Threshold Logic Unit (TLU) é uma estrututa que simula um neurônio. Ela recebe uma série de features e computa a soma ponderada delas (assim como um algoritmo de regressão simples). Mas após o cálculo, a equação é utilizada como argumento de uma função conhecida como step function.\n",
    "        </li>\n",
    "        <li> \n",
    "            Pode ser utilizada para tarefas de classificação binária. Assim como na Regressão Logística, caso o resultado obtido esteja acima de um threshold, a instância é designada à classe positiva; senão, à classe negativa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Estrutura de uma TLU</h1>\n",
    "    <img src='tlu1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ee9ec-5ae4-4b66-800f-8e6df13fb189",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> As step functions mais comuns da TLU</h1>\n",
    "    <img src='tlu2.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12996bf-94ed-4310-9f88-c169373767f0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Perceptron</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Um Perceptron consiste em uma camada de várias TLU's. Caso os neurônios sejam conectados com todos os inputs provenientes do nível anterior, dizemos que a camada em questão é uma <em>fully connected layer.</em>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Um Perceptron de 3 TLU's</h1>\n",
    "    <img src='perceptron1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada46e6-5721-4164-a576-96b0419da0e7",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Perceptron acima pode realizar 3 classificações binárias distintas, tornando-o um modelo multioutput.\n",
    "        </li>\n",
    "        <li> \n",
    "            Observe que, além das features, um termo bias (nesse caso, 1) também pode ser inserido como input.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='perceptron2.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Note como a função de previsão para uma fully conneted layer é muito similar à função afim. A letra $\\phi$ é a função de ativação - no contexto do TLU's, ela é chamada de step function.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3354dd-ce5a-4d04-8e58-09f0ead3aa31",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Durante a fase de treino, os neurônios de input que melhor contribuírem para previsões corretas recebem um maior peso ao alimentarem o TLU de output.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3867454-d312-4bd2-bae1-e6aba03f107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As fronteiras de decisão do objeto Perceptron são lineares. Portanto, não espere grandes resultados em datasets complexos!\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X, y = X[:, [2,3]], (y==0).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e18ecab0-4486-4b4b-a584-02035dd90017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O objeto 'Perceptron' admite Early Stopping!\n",
    "from sklearn.metrics import roc_auc_score\n",
    "per_clf = Perceptron(early_stopping=True, n_iter_no_change=8).fit(X_train, y_train)\n",
    "roc_auc_score(y_test, per_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2b6f7-c060-45e6-920f-aa08ce22ba13",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Multi-Layer Perceptron and Backpropagation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Considerando o desempenho limitado dos Perceptrons, foram criados os Multi-Layer Perceptrons. Eles possuem uma ou mais camadas de TLU's, conhecidas como hidden layers responsáveis por abastecerem a output layer - que possui o(s) TLU(s) que farão as previsões finais.\n",
    "        </li>\n",
    "        <li> \n",
    "            Todas as camadas dos MLP's são fully connected layers e, com exceção do nível de output, contam com um neurônio de bias.\n",
    "        </li>\n",
    "        <li> \n",
    "            Como os valores são transmitidos das camadas inferiores às superiores (e não vice-versa), a arquitetura de um MLP é classificada como uma Feedforward Neural Network (FNN).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3108bd9-d960-43ae-ba4f-0b09f70d94e8",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Treinando um MLP</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O fitting de um Multi-Layer Perceptron ocorre de maneira iterativa com mini-batches do set de treino.\n",
    "        </li>\n",
    "        <li> \n",
    "            Cada instância do subconjunto alimenta as hidden layers até que o TLU de output faça uma previsão. O valor lançado é comparado com o verdadeiro rótulo/número, sendo o nível de erro avaliado por uma loss function.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em seguida, é analisado o impacto de cada neurônio nas hidden layers para o erro. Com isso, os pesos de cada conexão são atualizados para a próxima instância.\n",
    "        </li>\n",
    "        <li> \n",
    "            É importante ressaltar que os pesos de cada conexão sejam inicializados de maneira aleatória, sem isso, o processo de treinamento pode não convergir a uma solução aleatória.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab392eed-3cb9-4319-b4a7-4b6e8ddedd29",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Regression MLP's</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O autor oferece aqui algumas dicas de montagem de MLP's em regressões.\n",
    "        </li>\n",
    "        <li>\n",
    "            Colocar uma função de ativação nos neurônios de output não é algo comum, mas caso queira que os valores previstos sejam sempre positivos, podemos recorrer ao uso da ReLU ou softplus. Se desejar que os outputs caiam dentro de um certo intervalo numérico, use a Logistic Function ou Hyperbolic Tangent Function (não se esqueça de atualizar os valores-alvo dentro do intervalo requerido [0-1] para logística e [-1,1] para hiperbólica).\n",
    "        </li>\n",
    "        <li> \n",
    "            Com relação à loss function, busque usar o Mean Absolute Error caso o dataset tenha outliers. Além disso, a Hubber Loss pode ser uma opção intermediária entre o MAE e o MSE, tendo menor sensibilidade a outliers, uma alta taxa de precisão e de convergência.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324d644-b9dc-4433-9c32-77e0cc5e5842",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Classification MLP's</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Com relação a classificações, os neurônios de output recebem uma Regressão Logística ou uma softmax para computar as probabilidades de classe.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em classificações binárias, um neurônio de output para cada condição deve existir; para as multi-classe, um neurônio terá que existir para cada classe.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16a30851-0795-4e5a-beaf-80aa66865d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-macosx_10_14_x86_64.whl (228.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 228.5 MB 878 bytes/s  0:00:016  |▊                               | 5.5 MB 6.2 MB/s eta 0:00:36     |█▏                              | 8.3 MB 3.2 MB/s eta 0:01:10     |███▌                            | 25.1 MB 7.3 MB/s eta 0:00:28     |██████▉                         | 49.1 MB 6.5 MB/s eta 0:00:28     |██████████▊                     | 76.7 MB 10.2 MB/s eta 0:00:15     |████████████████▍               | 117.0 MB 9.3 MB/s eta 0:00:12     |██████████████████▉             | 134.2 MB 9.3 MB/s eta 0:00:11     |████████████████████▍           | 145.6 MB 9.8 MB/s eta 0:00:09\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (20.9)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 231 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.48.0-cp38-cp38-macosx_10_10_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 3.2 MB/s eta 0:00:01    |███████████▏                    | 4.6 MB 3.8 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[K     |████████████████████████████████| 167 kB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.0)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=24566d0dd5d1c31317162562036897afa4255e1e953cc556510ca6da4678c81e\n",
      "  Stored in directory: /Users/felipeveiga/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.0 importlib-metadata-4.12.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.4.1 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "420c1f8a-cecc-47b4-a3f7-63fa22cc68e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-9090218d8889>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-9090218d8889>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Finalmente instalando o Tensor Flow!\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ad483b-99b4-4a2d-88ff-b32405571fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv /Users/felipeveiga/Desktop/Screen\\ Shot\\ 2022-07-26\\ at\\ 08.44.52.png ./perceptron2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb0468-948d-483d-98d2-281ff88808aa",
   "metadata": {},
   "source": [
    "<p style='color:red'> Escrever sobre o trecho grifado (p.290).</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
